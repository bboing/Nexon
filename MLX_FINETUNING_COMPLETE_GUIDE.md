# ğŸ MLX íŒŒì¸íŠœë‹ ì™„ì „ ê°€ì´ë“œ

**Apple Silicon (M1/M2/M3) Mac ì „ìš© LLM íŒŒì¸íŠœë‹ ì‹¤ì „ ê°€ì´ë“œ**

---

## ğŸ“‹ ëª©ì°¨

1. [ìš©ì–´ ì‚¬ì „ (ê¼­ ë¨¼ì € ì½ê¸°!)](#1-ìš©ì–´-ì‚¬ì „-ê¼­-ë¨¼ì €-ì½ê¸°)
2. [ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê°œìš”](#2-ì „ì²´-í”„ë¡œì„¸ìŠ¤-ê°œìš”)
3. [í•µì‹¬ ê°œë… ì´í•´](#3-í•µì‹¬-ê°œë…-ì´í•´)
4. [íŒŒì¼ êµ¬ì¡°](#4-íŒŒì¼-êµ¬ì¡°)
5. [ì‹¤í–‰ ê³¼ì • ë‹¨ê³„ë³„ ì„¤ëª…](#5-ì‹¤í–‰-ê³¼ì •-ë‹¨ê³„ë³„-ì„¤ëª…)
6. [ì¸ì(Arguments) ì‰½ê²Œ ì´í•´í•˜ê¸°](#6-ì¸ìarguments-ì‰½ê²Œ-ì´í•´í•˜ê¸°)
7. [ì‹¤ì œ ì‹¤í–‰ ë¡œê·¸ ë¶„ì„](#7-ì‹¤ì œ-ì‹¤í–‰-ë¡œê·¸-ë¶„ì„)
8. [ê²°ê³¼ë¬¼ ì´í•´í•˜ê¸°](#8-ê²°ê³¼ë¬¼-ì´í•´í•˜ê¸°)
9. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#9-íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## 1. ìš©ì–´ ì‚¬ì „ (ê¼­ ë¨¼ì € ì½ê¸°!)

### ğŸ“š **ê¸°ë³¸ ìš©ì–´**

#### **ëª¨ë¸ (Model)**
```
ë¹„ìœ : í•™ìƒì˜ ë‡Œ
ì„¤ëª…: AIê°€ í•™ìŠµí•œ ì§€ì‹ì´ ì €ì¥ëœ íŒŒì¼
ì˜ˆì‹œ: Llama-3.2-3B = 30ì–µ ê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ AI ëª¨ë¸
```

#### **íŒŒì¸íŠœë‹ (Fine-tuning)**
```
ë¹„ìœ : ì „ë¬¸ ì§ì—… êµìœ¡
ì„¤ëª…: ê¸°ë³¸ êµìœ¡ë°›ì€ ëª¨ë¸ì„ íŠ¹ì • ë¶„ì•¼(ê²Œì„ NPC)ì— íŠ¹í™”ì‹œí‚¤ëŠ” ê²ƒ
ì˜ˆì‹œ: ì¼ë°˜ ì˜ì‚¬ â†’ ì •í˜•ì™¸ê³¼ ì „ë¬¸ì˜
```

#### **LoRA (Low-Rank Adaptation)**
```
ë¹„ìœ : ì „ë¬¸ ë¶„ì•¼ ë…¸íŠ¸
ì„¤ëª…: ì›ë³¸ ì§€ì‹(2GB)ì€ ê·¸ëŒ€ë¡œ ë‘ê³ , ì „ë¬¸ ë¶„ì•¼ ì§€ì‹(16MB)ë§Œ ë”°ë¡œ ê¸°ë¡
ì˜ˆì‹œ: ì˜ëŒ€ êµê³¼ì„œ(ì›ë³¸) + ì •í˜•ì™¸ê³¼ ìˆ˜ì²©(LoRA)
```

#### **ì–´ëŒ‘í„° (Adapter)**
```
ë¹„ìœ : USB ì–´ëŒ‘í„°
ì„¤ëª…: ì›ë³¸ ëª¨ë¸ì— ê½‚ì•„ì„œ ì‚¬ìš©í•˜ëŠ” ì¶”ê°€ ë¶€í’ˆ (LoRA ê°€ì¤‘ì¹˜)
ì˜ˆì‹œ: ê¸°ë³¸ ì¹´ë©”ë¼ + ë§ì›ë Œì¦ˆ = ë§ì› ì¹´ë©”ë¼
```

---

### ğŸ“ **í•™ìŠµ ê´€ë ¨ ìš©ì–´**

#### **Loss (ì†ì‹¤ê°’)** â­â­â­
```
ë¹„ìœ : ì‹œí—˜ í‹€ë¦° ê°œìˆ˜
ì„¤ëª…: AIê°€ ì–¼ë§ˆë‚˜ í‹€ë¦¬ê³  ìˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì
- ë†’ìŒ (2.5): ë§ì´ í‹€ë¦¼ (í•™ìŠµ ì´ˆê¸°)
- ë‚®ìŒ (0.5): ì ê²Œ í‹€ë¦¼ (í•™ìŠµ ì™„ë£Œ)

ì¢‹ì€ í•™ìŠµ: 2.5 â†’ 1.5 â†’ 0.8 â†’ 0.5 (ì ì  ê°ì†Œ)
ë‚˜ìœ í•™ìŠµ: 2.5 â†’ 3.0 â†’ NaN (ì¦ê°€ ë˜ëŠ” ë°œì‚°)

ëª©í‘œ: Lossë¥¼ ìµœëŒ€í•œ ë‚®ì¶”ê¸°!
```

#### **Iteration (ë°˜ë³µ)**
```
ë¹„ìœ : ë¬¸ì œì§‘ í‘¸ëŠ” íšŸìˆ˜
ì„¤ëª…: í•™ìŠµ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë°˜ë³µí•´ì„œ í•™ìŠµí•˜ëŠ”ì§€
ì˜ˆì‹œ: 
- iters=100: ë¬¸ì œì§‘ì„ 100ë²ˆ í’€ê¸°
- iters=1000: ë¬¸ì œì§‘ì„ 1000ë²ˆ í’€ê¸°

ë§ì„ìˆ˜ë¡: ë” ì˜ ë°°ìš°ì§€ë§Œ, ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼
```

#### **Batch Size (ë°°ì¹˜ í¬ê¸°)**
```
ë¹„ìœ : í•œ ë²ˆì— í’€ ë¬¸ì œ ê°œìˆ˜
ì„¤ëª…: í•œ ë²ˆì— ëª‡ ê°œì˜ ë°ì´í„°ë¥¼ ë™ì‹œì— ì²˜ë¦¬í•˜ëŠ”ì§€
ì˜ˆì‹œ:
- batch_size=4: 4ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬
- batch_size=8: 8ê°œì”© ë¬¶ì–´ì„œ ì²˜ë¦¬

í¬ë©´: ë¹ ë¥´ì§€ë§Œ ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
ì‘ìœ¼ë©´: ëŠë¦¬ì§€ë§Œ ë©”ëª¨ë¦¬ ì ê²Œ ì‚¬ìš©

ì‹¤ìƒí™œ ë¹„ìœ : 
- batch_size=1: ë¬¸ì œ í•˜ë‚˜ í’€ê³  ë‹µ í™•ì¸ (ì •í™•, ëŠë¦¼)
- batch_size=10: ë¬¸ì œ 10ê°œ í’€ê³  ë‹µ í™•ì¸ (ë¹ ë¦„, ë©”ëª¨ë¦¬ ë§ì´)
```

#### **Learning Rate (í•™ìŠµë¥ )** â­â­â­
```
ë¹„ìœ : ê³µë¶€ ì†ë„ ì¡°ì ˆ
ì„¤ëª…: í•œ ë²ˆì— ì–¼ë§ˆë‚˜ ë§ì´ ìˆ˜ì •í• ì§€
ì˜ˆì‹œ: 1e-5 = 0.00001

ë„ˆë¬´ ë†’ìŒ (0.1): 
  - ë¹ ë¥´ì§€ë§Œ ë¶ˆì•ˆì •
  - ì •ë‹µ ê·¼ì²˜ì—ì„œ ì™”ë‹¤ê°”ë‹¤ (ìˆ˜ë ´ ì‹¤íŒ¨)
  
ì ë‹¹í•¨ (0.00001):
  - ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ
  - ì •ë‹µì— ì²œì²œíˆ ë„ë‹¬
  
ë„ˆë¬´ ë‚®ìŒ (0.000001):
  - ë§¤ìš° ëŠë¦¼
  - í•™ìŠµì´ ê±°ì˜ ì•ˆ ë¨

ì‹¤ìƒí™œ ë¹„ìœ :
- ë†’ìŒ: ë¹¨ë¦¬ ë‹¬ë¦¬ê¸° (ë„˜ì–´ì§ˆ ìœ„í—˜)
- ì ë‹¹: ë¹ ë¥´ê²Œ ê±·ê¸° (ì•ˆì •ì )
- ë‚®ìŒ: ì²œì²œíˆ ê±·ê¸° (ë„ˆë¬´ ëŠë¦¼)
```

#### **Epoch vs Iteration**
```
Epoch (ì—í¬í¬): ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆ ë‹¤ ë³¸ ê²ƒ
Iteration (ë°˜ë³µ): ë°°ì¹˜ í•˜ë‚˜ë¥¼ ì²˜ë¦¬í•œ ê²ƒ

ì˜ˆì‹œ:
- ë°ì´í„° 100ê°œ, batch_size=10
- 1 epoch = 10 iterations
- 100 iterations = 10 epochs

ìš°ë¦¬ ì„¤ì •:
- ë°ì´í„° 100ê°œ, batch_size=4, iters=100
- ì•½ 4 epochs í•™ìŠµ
```

---

### ğŸ”§ **LoRA ì „ë¬¸ ìš©ì–´**

#### **Rank (ë­í¬)**
```
ë¹„ìœ : ë…¸íŠ¸ ë‘ê»˜
ì„¤ëª…: LoRA ì–´ëŒ‘í„°ì˜ ìš©ëŸ‰ (í‘œí˜„ë ¥)
ì˜ˆì‹œ:
- rank=4: ì–‡ì€ ë…¸íŠ¸ (ë¹ ë¦„, ë‹¨ìˆœ)
- rank=8: ë³´í†µ ë…¸íŠ¸ (ê· í˜•)
- rank=16: ë‘êº¼ìš´ ë…¸íŠ¸ (ëŠë¦¼, ì •êµ)

ë†’ì„ìˆ˜ë¡: ë” ë³µì¡í•œ ê²ƒì„ ë°°ìš¸ ìˆ˜ ìˆì§€ë§Œ ëŠë¦¼
ë‚®ì„ìˆ˜ë¡: ê°„ë‹¨í•œ ê²ƒë§Œ ë°°ìš°ì§€ë§Œ ë¹ ë¦„

ì¶”ì²œ: 8~16
```

#### **Dropout (ë“œë¡­ì•„ì›ƒ)**
```
ë¹„ìœ : ì‹œí—˜ ë•Œ ë…¸íŠ¸ ì¼ë¶€ë¥¼ ì•ˆ ë³´ê¸°
ì„¤ëª…: í•™ìŠµ ì¤‘ì— ì¼ë¶€ëŸ¬ ì¼ë¶€ë¥¼ ë¬´ì‹œ (ê³¼ì í•© ë°©ì§€)
ì˜ˆì‹œ:
- dropout=0.0: ëª¨ë“  ê²ƒ ì‚¬ìš© (ì‘ì€ ë°ì´í„°ì— ì í•©)
- dropout=0.1: 10%ëŠ” ë¬´ì‹œ (í° ë°ì´í„°ì— ì í•©)

ê³¼ì í•©ì´ë€?
- ë¬¸ì œì§‘ë§Œ ë‹¬ë‹¬ ì™¸ì›Œì„œ ì‹¤ì „ì—ì„œ ëª» í‘¸ëŠ” ê²ƒ
- dropoutì€ ì´ë¥¼ ë°©ì§€
```

#### **Scale (ìŠ¤ì¼€ì¼)**
```
ë¹„ìœ : ë³¼ë¥¨ ì¡°ì ˆ
ì„¤ëª…: LoRAì˜ ì˜í–¥ë ¥ì„ ì–¼ë§ˆë‚˜ ê°•í•˜ê²Œ ì ìš©í• ì§€
ì˜ˆì‹œ:
- scale=10: ì‘ì€ ì˜í–¥
- scale=20: ë³´í†µ ì˜í–¥ (ê¸°ë³¸ê°’)
- scale=40: í° ì˜í–¥

ì›ë³¸ ëª¨ë¸ vs LoRA ë¹„ìœ¨ ì¡°ì •
```

#### **Num Layers (ë ˆì´ì–´ ìˆ˜)**
```
ë¹„ìœ : êµê³¼ì„œ ì¤‘ ì–´ëŠ ì¥ì„ ê³µë¶€í• ì§€
ì„¤ëª…: ëª¨ë¸ì˜ ì–´ëŠ ë¶€ë¶„ì„ í•™ìŠµì‹œí‚¬ì§€
ì˜ˆì‹œ:
- num_layers=16: ë§ˆì§€ë§‰ 16ê°œ ë ˆì´ì–´ë§Œ
- num_layers=-1: ì „ì²´ ë ˆì´ì–´ (ëª¨ë“  ê²ƒ)

ëª¨ë¸ êµ¬ì¡°:
- Llama-3BëŠ” 28ê°œ ë ˆì´ì–´
- ë§ˆì§€ë§‰ 16ê°œë§Œ í•™ìŠµ = ì• 12ê°œëŠ” ê·¸ëŒ€ë¡œ

ë§ì„ìˆ˜ë¡: ë” ë§ì´ ë°°ìš°ì§€ë§Œ ëŠë¦¼
ì ì„ìˆ˜ë¡: ì ê²Œ ë°°ìš°ì§€ë§Œ ë¹ ë¦„
```

---

### ğŸ“Š **ì„±ëŠ¥ ì§€í‘œ**

#### **Tokens/sec (í† í° ì†ë„)**
```
ë¹„ìœ : íƒ€ì ì†ë„
ì„¤ëª…: ì´ˆë‹¹ ëª‡ ê°œì˜ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•˜ëŠ”ì§€
ì˜ˆì‹œ: 1000 tokens/sec

í† í°ì´ë€?
- ë‹¨ì–´ì˜ ì¡°ê°
- "ì•ˆë…•í•˜ì„¸ìš”" â‰ˆ 3~4 tokens
- "Hello" â‰ˆ 1 token

ë†’ì„ìˆ˜ë¡: ë¹ ë¥¸ ì²˜ë¦¬
```

#### **It/sec (ë°˜ë³µ ì†ë„)**
```
ë¹„ìœ : ë¬¸ì œ í‘¸ëŠ” ì†ë„
ì„¤ëª…: ì´ˆë‹¹ ëª‡ ë²ˆ í•™ìŠµí•˜ëŠ”ì§€
ì˜ˆì‹œ: 4.5 it/sec = ì´ˆë‹¹ 4.5ë²ˆ í•™ìŠµ

M2 Mac ê¸°ì¤€:
- 3B ëª¨ë¸: 4~5 it/sec
- 1B ëª¨ë¸: 8~10 it/sec
```

#### **Trainable Parameters (í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°)**
```
ë¹„ìœ : ê³µë¶€í•  ë‚´ìš©ì˜ ì–‘
ì„¤ëª…: í•™ìŠµ ì¤‘ì— ë°”ê¿€ ìˆ˜ ìˆëŠ” ìˆ«ìì˜ ê°œìˆ˜
ì˜ˆì‹œ: 20,971,520ê°œ

ì „ì²´ ëª¨ë¸: 3,000,000,000ê°œ (30ì–µ)
LoRAë¡œ í•™ìŠµ: 20,971,520ê°œ (2ì²œë§Œ)
= ì „ì²´ì˜ ì•½ 0.7%ë§Œ í•™ìŠµ!

ì´ë˜ì„œ ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ë¥¼ ì ê²Œ ì”€!
```

---

### ğŸ’¾ **ë°ì´í„° ê´€ë ¨ ìš©ì–´**

#### **JSONL (JSON Lines)**
```
í˜•ì‹: í•œ ì¤„ì— í•˜ë‚˜ì˜ JSON
ì˜ˆì‹œ:
{"text": "ì²« ë²ˆì§¸ ë°ì´í„°"}
{"text": "ë‘ ë²ˆì§¸ ë°ì´í„°"}

JSONê³¼ì˜ ì°¨ì´:
- JSON: [{"text": "..."}, {"text": "..."}]
- JSONL: ê° ì¤„ì´ ë…ë¦½ì  (ìŠ¤íŠ¸ë¦¬ë° ê°€ëŠ¥)
```

#### **Validation (ê²€ì¦)**
```
ë¹„ìœ : ëª¨ì˜ê³ ì‚¬
ì„¤ëª…: í•™ìŠµ ì¤‘ì— ì‹¤ë ¥ì„ í…ŒìŠ¤íŠ¸
ì˜ˆì‹œ:
- train.jsonl: ì‹¤ì œ ê³µë¶€í•  ë°ì´í„°
- valid.jsonl: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°

ì™œ í•„ìš”?
- í•™ìŠµ ë°ì´í„°ë¡œë§Œ í‰ê°€í•˜ë©´ ì‹¤ë ¥ì„ ëª¨ë¦„
- ìƒˆë¡œìš´ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ = ì§„ì§œ ì‹¤ë ¥ í™•ì¸
```

#### **Max Sequence Length (ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´)**
```
ì„¤ëª…: í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜
ì˜ˆì‹œ: 2048 tokens â‰ˆ 1500ë‹¨ì–´

ê¸¸ë©´: 
  - ê¸´ ê¸€ ì²˜ë¦¬ ê°€ëŠ¥
  - ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©
  
ì§§ìœ¼ë©´:
  - ì§§ì€ ê¸€ë§Œ ê°€ëŠ¥
  - ë©”ëª¨ë¦¬ ì ê²Œ ì‚¬ìš©

ìš°ë¦¬ ì„¤ì •: 2048 (NPC ëŒ€í™”ì— ì¶©ë¶„)
```

---

### ğŸ¯ **ìµœì í™” ìš©ì–´**

#### **Optimizer (ì˜µí‹°ë§ˆì´ì €)**
```
ë¹„ìœ : ê³µë¶€ ë°©ë²•
ì„¤ëª…: ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ í•™ìŠµí• ì§€
ì˜ˆì‹œ:
- Adam: ê°€ì¥ ì¼ë°˜ì  (ì¶”ì²œ)
- SGD: ì˜¤ë˜ëœ ë°©ì‹ (ëŠë¦¼)
- AdamW: Adam ê°œì„  ë²„ì „

Adamì´ë€?
- Adaptive Moment Estimation
- í•™ìŠµë¥ ì„ ìë™ìœ¼ë¡œ ì¡°ì ˆ
- ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì˜ ì‘ë™
```

#### **Gradient (ê¸°ìš¸ê¸°)**
```
ë¹„ìœ : ì‚°ì„ ë‚´ë ¤ê°€ëŠ” ë°©í–¥
ì„¤ëª…: Lossë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ ê°€ì•¼ í•˜ëŠ”ì§€
ì˜ˆì‹œ:
- Gradient = "ì™¼ìª½ìœ¼ë¡œ ê°€ë©´ Loss ê°ì†Œ"
- ëª¨ë¸ì´ ì´ ë°©í–¥ìœ¼ë¡œ ìˆ˜ì •ë¨

Gradient Descent (ê²½ì‚¬ í•˜ê°•ë²•):
- ì‚° ì •ìƒ(ë†’ì€ Loss)ì—ì„œ ì¶œë°œ
- ê°€ì¥ ê¸‰í•œ ë‚´ë¦¬ë§‰ê¸¸ë¡œ ì´ë™
- ê³„ê³¡(ë‚®ì€ Loss)ì— ë„ë‹¬
```

#### **Checkpointing (ì²´í¬í¬ì¸íŠ¸)**
```
ì„¤ëª…: ì¤‘ê°„ ì €ì¥
ì˜ˆì‹œ: save_every=100

ì™œ í•„ìš”?
- í•™ìŠµ ì¤‘ ì»´í“¨í„° êº¼ì§€ë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ
- 100ë²ˆë§ˆë‹¤ ì €ì¥ = ì´ì–´ì„œ í•™ìŠµ ê°€ëŠ¥

ê²Œì„ ì„¸ì´ë¸Œì™€ ë™ì¼!
```

---

### ğŸ”¢ **ìˆ«ì í‘œê¸°ë²•**

```
1e-5 = 0.00001 (ì†Œìˆ˜ì  5ìë¦¬)
1e-4 = 0.0001  (ì†Œìˆ˜ì  4ìë¦¬)
1e-3 = 0.001   (ì†Œìˆ˜ì  3ìë¦¬)

1e5 = 100,000 (10ë§Œ)
1e6 = 1,000,000 (100ë§Œ)
1e9 = 1,000,000,000 (10ì–µ)

"e"ëŠ” "Ã—10ì˜ ê±°ë“­ì œê³±"ì„ ì˜ë¯¸
```

---

## 2. ì „ì²´ í”„ë¡œì„¸ìŠ¤ ê°œìš”

### ğŸ¯ **í•œ ì¤„ ìš”ì•½**

> Hugging Faceì—ì„œ ê¸°ë³¸ ëª¨ë¸ì„ ë‹¤ìš´ë°›ì•„ â†’ ë‚´ ë°ì´í„°ë¡œ LoRA í•™ìŠµ â†’ ì‘ì€ ì–´ëŒ‘í„°(16MB) ìƒì„±

### ğŸ“Š **ì „ì²´ íë¦„ë„**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì‹¤í–‰ ëª…ë ¹ì–´                                                 â”‚
â”‚  sh scripts/start-mlx-training.sh                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 1: í™˜ê²½ ì¤€ë¹„                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ training/ ë””ë ‰í† ë¦¬ë¡œ ì´ë™                                â”‚
â”‚  â€¢ mlx-env/ ê°€ìƒí™˜ê²½ í™•ì¸/ìƒì„±                              â”‚
â”‚  â€¢ MLX íŒ¨í‚¤ì§€ ì„¤ì¹˜ (mlx, mlx-lm, transformers, datasets)   â”‚
â”‚  â€¢ ì†Œìš” ì‹œê°„: ì²« ì‹¤í–‰ 3ë¶„, ì´í›„ ì¦‰ì‹œ                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 2: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ ëª¨ë¸: mlx-community/Llama-3.2-3B-Instruct-4bit          â”‚
â”‚  â€¢ ë‹¤ìš´ë¡œë“œ ìœ„ì¹˜: ~/.cache/huggingface/hub/                â”‚
â”‚  â€¢ í¬ê¸°: ì•½ 2GB                                             â”‚
â”‚  â€¢ ì†Œìš” ì‹œê°„: ì²« ì‹¤í–‰ 5~10ë¶„, ì´í›„ ìºì‹œ ì‚¬ìš©               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 3: ë°ì´í„° ì¤€ë¹„                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ training/data/train.jsonl ìƒì„± (100ê°œ ìƒ˜í”Œ)            â”‚
â”‚  â€¢ training/data/valid.jsonl ìƒì„± (10ê°œ ìƒ˜í”Œ)             â”‚
â”‚  â€¢ í˜•ì‹: {"text": "instruction + input + output"}          â”‚
â”‚  â€¢ ìë™ ìƒì„± (ìŠ¤í¬ë¦½íŠ¸ ë‚´ì¥)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 4: LoRA íŒŒì¸íŠœë‹ ì‹¤í–‰                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ ëª…ë ¹ì–´: python -m mlx_lm.lora                            â”‚
â”‚  â€¢ 100 iterations í•™ìŠµ                                       â”‚
â”‚  â€¢ Metal GPU ê°€ì† (M1/M2/M3)                               â”‚
â”‚  â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©: ì•½ 6~8GB                                    â”‚
â”‚  â€¢ ì†Œìš” ì‹œê°„: 10~15ë¶„ (3B ëª¨ë¸ ê¸°ì¤€)                       â”‚
â”‚  â€¢ ì¶œë ¥: Loss ê°’ ê°ì†Œ í™•ì¸ (2.5 â†’ 0.5)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 5: ê²°ê³¼ ì €ì¥                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  training/models/llama-game-npc-mlx/                        â”‚
â”‚    â”œâ”€ adapters.safetensors (ì•½ 16MB) â† í•™ìŠµëœ ê°€ì¤‘ì¹˜!     â”‚
â”‚    â””â”€ adapter_config.json (ì„¤ì •)                            â”‚
â”‚                                                              â”‚
â”‚  ì›ë³¸ ëª¨ë¸(2GB)ì€ ë³€ê²½ ì•ˆ ë¨!                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Step 6: ì¶”ë¡  í…ŒìŠ¤íŠ¸                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  from mlx_lm import load, generate                           â”‚
â”‚                                                              â”‚
â”‚  model, tokenizer = load(                                    â”‚
â”‚      "mlx-community/Llama-3.2-3B-Instruct-4bit",            â”‚
â”‚      adapter_path="models/llama-game-npc-mlx"               â”‚
â”‚  )                                                           â”‚
â”‚                                                              â”‚
â”‚  response = generate(model, tokenizer, prompt)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. í•µì‹¬ ê°œë… ì´í•´

### ğŸ§  **LoRA (Low-Rank Adaptation)**

**ì „í†µì  íŒŒì¸íŠœë‹**:
```
ì›ë³¸ ëª¨ë¸ (2GB) â†’ ì „ì²´ ìˆ˜ì • â†’ ìƒˆ ëª¨ë¸ (2GB) ì €ì¥
- ë©”ëª¨ë¦¬: 16GB+ í•„ìš”
- ì‹œê°„: ìˆ˜ ì‹œê°„
- ì €ì¥: ë§¤ë²ˆ 2GB
```

**LoRA íŒŒì¸íŠœë‹**:
```
ì›ë³¸ ëª¨ë¸ (2GB) + ì–´ëŒ‘í„° (16MB) = í•™ìŠµëœ ëª¨ë¸
- ë©”ëª¨ë¦¬: 6~8GB
- ì‹œê°„: 10~15ë¶„
- ì €ì¥: 16MBë§Œ ì¶”ê°€
```

**ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„**:
```
W_new = W_original + Î”W

ì „í†µì : Î”W = 2GB (ëª¨ë“  ê°€ì¤‘ì¹˜ ë³€ê²½)
LoRA:   Î”W = BÂ·A (16MB, ì‘ì€ í–‰ë ¬ 2ê°œ)
       B: (hidden_dim Ã— rank)
       A: (rank Ã— hidden_dim)
       rank = 8 (ì„¤ì •ê°’)
```

### ğŸ¯ **Ollama vs MLX ê´€ê³„**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama (ë¡œì»¬)          â”‚       â”‚  MLX í•™ìŠµ í™˜ê²½          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ llama3.1 (8B)        â”‚       â”‚  â€¢ Llama-3.2-3B         â”‚
â”‚  â€¢ gemma-3 (12B)        â”‚       â”‚  â€¢ í•™ìŠµ ê°€ëŠ¥            â”‚
â”‚  â€¢ ì¶”ë¡  ì „ìš© (ì½ê¸° ì „ìš©)â”‚       â”‚  â€¢ ì–´ëŒ‘í„° ìƒì„±          â”‚
â”‚  â€¢ GGUF í˜•ì‹            â”‚       â”‚  â€¢ SafeTensors í˜•ì‹     â”‚
â”‚  â€¢ í•™ìŠµ ë¶ˆê°€ âŒ         â”‚       â”‚  â€¢ í•™ìŠµ ê°€ëŠ¥ âœ…         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì¤‘ìš”**: ë‘˜ì€ **ì™„ì „íˆ ë³„ê°œ**ì…ë‹ˆë‹¤!
- Ollamaì— ìˆëŠ” ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê²Œ ì•„ë‹™ë‹ˆë‹¤
- Hugging Faceì—ì„œ ìƒˆ ëª¨ë¸ì„ ë°›ì•„ì„œ í•™ìŠµí•©ë‹ˆë‹¤
- í•™ìŠµ í›„ (ì„ íƒì ìœ¼ë¡œ) Ollamaì— ë°°í¬ ê°€ëŠ¥

---

## 3. íŒŒì¼ êµ¬ì¡°

### ğŸ“ **ì „ì²´ ë””ë ‰í† ë¦¬ êµ¬ì¡°**

```
my-ai-platform/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ start-mlx-training.sh      â­ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”‚
â””â”€â”€ training/
    â”œâ”€â”€ mlx-env/                    â­ Python ê°€ìƒí™˜ê²½
    â”‚   â”œâ”€â”€ bin/python
    â”‚   â””â”€â”€ lib/...
    â”‚
    â”œâ”€â”€ data/                       â­ í•™ìŠµ ë°ì´í„°
    â”‚   â”œâ”€â”€ train.jsonl             (100ê°œ ìƒ˜í”Œ)
    â”‚   â””â”€â”€ valid.jsonl             (10ê°œ ìƒ˜í”Œ)
    â”‚
    â”œâ”€â”€ scripts/
    â”‚   â””â”€â”€ finetune_mlx.py         â­ íŒŒì¸íŠœë‹ ë©”ì¸ ìŠ¤í¬ë¦½íŠ¸
    â”‚
    â”œâ”€â”€ models/                     â­ ê²°ê³¼ë¬¼
    â”‚   â””â”€â”€ llama-game-npc-mlx/
    â”‚       â”œâ”€â”€ adapters.safetensors  (16MB, í•™ìŠµ ê²°ê³¼!)
    â”‚       â””â”€â”€ adapter_config.json   (ì„¤ì •)
    â”‚
    â”œâ”€â”€ MLX_GUIDE.md                ğŸ“– ì‚¬ìš© ê°€ì´ë“œ
    â””â”€â”€ HOW_IT_WORKS.md             ğŸ“– ë™ì‘ ì›ë¦¬
```

### ğŸ” **í•µì‹¬ íŒŒì¼ ì—­í• **

| íŒŒì¼ | ì—­í•  | ìˆ˜ì • í•„ìš”? |
|------|------|-----------|
| `start-mlx-training.sh` | ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìë™í™” | âŒ ì—†ìŒ |
| `finetune_mlx.py` | í•™ìŠµ ë¡œì§ (CONFIG ìˆ˜ì •) | âœ… ì„¤ì • ë³€ê²½ ì‹œ |
| `train.jsonl` | í•™ìŠµ ë°ì´í„° | âœ… ì»¤ìŠ¤í…€ ë°ì´í„° ì‚¬ìš© ì‹œ |
| `adapters.safetensors` | í•™ìŠµ ê²°ê³¼ (LoRA ê°€ì¤‘ì¹˜) | âŒ ìë™ ìƒì„± |
| `adapter_config.json` | í•™ìŠµ ì„¤ì • ê¸°ë¡ | âŒ ìë™ ìƒì„± |

---

## 4. ì‹¤í–‰ ê³¼ì • ë‹¨ê³„ë³„ ì„¤ëª…

### ğŸš€ **ì‹¤í–‰ ëª…ë ¹ì–´**

```bash
cd /Users/taegyunkim/bboing/ollama_model/my-ai-platform
sh scripts/start-mlx-training.sh
```

### ğŸ“ **ìŠ¤í¬ë¦½íŠ¸ ë‚´ë¶€ ë™ì‘**

#### **1ë‹¨ê³„: ê²½ë¡œ ì´ë™**
```bash
cd "$(dirname "$0")/../training"
```
- `$(dirname "$0")` = `scripts/` ë””ë ‰í† ë¦¬
- `../` = ìƒìœ„ ë””ë ‰í† ë¦¬ (`my-ai-platform/`)
- `training` = `training/` ë””ë ‰í† ë¦¬
- **ìµœì¢…**: `/my-ai-platform/training/`

#### **2ë‹¨ê³„: ì‹œìŠ¤í…œ í™•ì¸**
```bash
if [[ "$OSTYPE" != "darwin"* ]]; then
    echo "âŒ ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” Macì—ì„œë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤"
    exit 1
fi
```
- macOS ì—¬ë¶€ í™•ì¸
- Apple Silicon (arm64) ê¶Œì¥

#### **3ë‹¨ê³„: ê°€ìƒí™˜ê²½ ì„¤ì •**
```bash
if [ ! -d "mlx-env" ]; then
    python3 -m venv mlx-env
    source mlx-env/bin/activate
    pip install mlx mlx-lm transformers datasets huggingface_hub
fi
```
- ì²« ì‹¤í–‰: í™˜ê²½ ìƒì„± + íŒ¨í‚¤ì§€ ì„¤ì¹˜ (3ë¶„)
- ì´í›„: ê¸°ì¡´ í™˜ê²½ í™œì„±í™” (ì¦‰ì‹œ)

#### **4ë‹¨ê³„: íŒŒì¸íŠœë‹ ì‹¤í–‰**
```bash
python scripts/finetune_mlx.py
```
- `finetune_mlx.py` ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
- ë‚´ë¶€ì—ì„œ `mlx_lm.lora` ëª…ë ¹ì–´ í˜¸ì¶œ

---

### ğŸ”§ **`finetune_mlx.py` ë‚´ë¶€ ë™ì‘**

#### **CONFIG ì„¤ì •**
```python
CONFIG = {
    "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",
    "batch_size": 4,
    "iters": 100,
    "learning_rate": 1e-5,
    "lora_layers": 16,
    "lora_rank": 8,
    "data": "../data",
}
```

#### **ë°ì´í„° ìƒì„±**
```python
# train.jsonl ìƒì„±
sample_dataset = [
    {
        "instruction": "ë‹¹ì‹ ì€ NPCì…ë‹ˆë‹¤.",
        "input": "ì•ˆë…•í•˜ì„¸ìš”?",
        "output": "í™˜ì˜í•©ë‹ˆë‹¤!"
    },
    # ... 100ê°œ
] * 20  # ì´ 100ê°œ

# JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
{"text": "ë‹¹ì‹ ì€ NPCì…ë‹ˆë‹¤. ì•ˆë…•í•˜ì„¸ìš”? í™˜ì˜í•©ë‹ˆë‹¤!"}
```

#### **ëª¨ë¸ ë¡œë“œ**
```python
from mlx_lm import load

model, tokenizer = load("mlx-community/Llama-3.2-3B-Instruct-4bit")
```
- ì²« ì‹¤í–‰: Hugging Faceì—ì„œ ë‹¤ìš´ë¡œë“œ (5~10ë¶„)
- ì´í›„: ìºì‹œì—ì„œ ë¡œë“œ (1~2ë¶„)

#### **í•™ìŠµ ëª…ë ¹ì–´ êµ¬ì„±**
```python
cmd = [
    "python", "-m", "mlx_lm.lora",
    "--model", "mlx-community/Llama-3.2-3B-Instruct-4bit",
    "--train",
    "--data", "/path/to/data",
    "--batch-size", "4",
    "--iters", "100",
    "--learning-rate", "1e-05",
    "--num-layers", "16",  # LoRA ì ìš© ë ˆì´ì–´ ìˆ˜
    "--adapter-path", "/path/to/output",
]

subprocess.run(cmd)
```

---

## 5. ì¸ì(Arguments) ì‰½ê²Œ ì´í•´í•˜ê¸° â­

### ğŸ¯ **ì „ì²´ ì„¤ì • í•œëˆˆì— ë³´ê¸°**

```python
CONFIG = {
    # ğŸ§  ì–´ë–¤ AI ëª¨ë¸ì„ ì“¸ê¹Œ?
    "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",
    
    # ğŸ“š ì–´ë–»ê²Œ ê³µë¶€í• ê¹Œ?
    "batch_size": 4,              # í•œ ë²ˆì— 4ê°œì”© ì²˜ë¦¬
    "iters": 100,                 # 100ë²ˆ ë°˜ë³µ í•™ìŠµ
    "learning_rate": 1e-5,        # í•™ìŠµ ì†ë„ 0.00001
    
    # ğŸ›ï¸ LoRA ì„¤ì • (ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ)
    "lora_layers": 16,            # 16ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
    "lora_rank": 8,               # ë…¸íŠ¸ ë‘ê»˜ = 8
    
    # ğŸ“‚ ë°ì´í„° ìœ„ì¹˜
    "data": "../data",
}
```

---

### ğŸ“– **ê° ì¸ìë³„ ì‰¬ìš´ ì„¤ëª…**

#### **1ï¸âƒ£ ëª¨ë¸ ì„ íƒ (`--model`)**

```python
--model "mlx-community/Llama-3.2-3B-Instruct-4bit"
```

**ë¹„ìœ **: ì–´ë–¤ í•™ìƒì„ ê°€ë¥´ì¹ ê¹Œ?

| ëª¨ë¸ | í¬ê¸° | ë©”ëª¨ë¦¬ | ì†ë„ | í’ˆì§ˆ | ë¹„ìœ  |
|------|------|--------|------|------|------|
| 1B | 1GB | 4GB | ë¹ ë¦„ âš¡ | ë³´í†µ | ì´ˆë“±í•™ìƒ |
| 3B | 2GB | 8GB | ì¤‘ê°„ âœ… | ì¢‹ìŒ | ì¤‘í•™ìƒ (ì¶”ì²œ!) |
| 8B | 5GB | 16GB | ëŠë¦¼ ğŸŒ | ìµœê³  | ëŒ€í•™ìƒ |

**4bitì´ë€?**
```
ì›ë³¸ ëª¨ë¸: [â– â– â– â– â– â– â– â– ] 2GB
4bit ì••ì¶•: [â– â– ] 500MB (75% ê°ì†Œ!)

í’ˆì§ˆ: 99% ìœ ì§€ âœ…
ì†ë„: 2ë°° ë¹ ë¦„ âš¡
ë©”ëª¨ë¦¬: 4ë°° ì ˆì•½ ğŸ’¾
```

---

#### **2ï¸âƒ£ í•™ìŠµ ë°˜ë³µ íšŸìˆ˜ (`--iters`)**

```python
--iters 100  # ë¬¸ì œì§‘ì„ 100ë²ˆ í’€ê¸°
```

**ë¹„ìœ **: ë¬¸ì œì§‘ ë°˜ë³µ íšŸìˆ˜

**ì‹¤ì œ ì†Œìš” ì‹œê°„** (M2 Mac ê¸°ì¤€):
| iters | ì‹œê°„ | Loss ì˜ˆìƒ | ìš©ë„ |
|-------|------|-----------|------|
| 50 | 10ë¶„ | 2.5 â†’ 1.8 | ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ğŸ”¬ |
| 100 | 20ë¶„ | 2.5 â†’ 1.2 | ê°„ë‹¨í•œ í•™ìŠµ âœ… |
| 500 | 2ì‹œê°„ | 2.5 â†’ 0.6 | ì‹¤ì „ í•™ìŠµ ğŸ“ |
| 1000 | 4ì‹œê°„ | 2.5 â†’ 0.3 | ê³ í’ˆì§ˆ í•™ìŠµ ğŸ’ |

**ì–¸ì œ ëŠ˜ë ¤ì•¼ í• ê¹Œ?**
```
âœ… Lossê°€ ê³„ì† ì¤„ì–´ë“¤ê³  ìˆìœ¼ë©´
   Iter 100: Loss 1.5
   Iter 200: Loss 1.2
   Iter 300: Loss 0.9  â† ë” í•™ìŠµí•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŒ!
   
   â†’ itersë¥¼ 500~1000ìœ¼ë¡œ ëŠ˜ë¦¬ê¸°

âŒ Lossê°€ ì•ˆ ì¤„ë©´
   Iter 100: Loss 2.5
   Iter 200: Loss 2.48
   Iter 300: Loss 2.47 â† ê±°ì˜ ë³€í™” ì—†ìŒ
   
   â†’ ë°ì´í„° ë¬¸ì œ, iters ëŠ˜ë ¤ë„ ì†Œìš©ì—†ìŒ
```

---

#### **3ï¸âƒ£ ë°°ì¹˜ í¬ê¸° (`--batch-size`)**

```python
--batch-size 4  # í•œ ë²ˆì— 4ê°œì”© ì²˜ë¦¬
```

**ë¹„ìœ **: í•œ ë²ˆì— í’€ ë¬¸ì œ ê°œìˆ˜

```
batch_size=1:  [ë¬¸ì œ1] â†’ ì±„ì  â†’ [ë¬¸ì œ2] â†’ ì±„ì  (ëŠë¦¼, ì •í™•)
batch_size=4:  [ë¬¸ì œ1,2,3,4] â†’ ì±„ì  (ë¹ ë¦„, ê· í˜•) âœ…
batch_size=8:  [ë¬¸ì œ1,2,3,4,5,6,7,8] â†’ ì±„ì  (ë¹ ë¦„, ë©”ëª¨ë¦¬â†‘)
```

**Mac ë©”ëª¨ë¦¬ë³„ ì¶”ì²œ**:
| RAM | ì¶”ì²œ batch_size | ì˜ˆìƒ ë©”ëª¨ë¦¬ ì‚¬ìš© |
|-----|----------------|-----------------|
| 8GB | 2 | ~6GB |
| 16GB | 4 | ~10GB â­ |
| 32GB+ | 8 | ~18GB |

**ë©”ëª¨ë¦¬ ë¶€ì¡± ì—ëŸ¬?**
```bash
RuntimeError: out of memory

í•´ê²°ë²•:
1ï¸âƒ£ batch_size ë°˜ìœ¼ë¡œ ì¤„ì´ê¸°
   8 â†’ 4 â†’ 2 â†’ 1
   
2ï¸âƒ£ ì‘ì€ ëª¨ë¸ ì‚¬ìš©
   3B â†’ 1B
```

---

#### **4ï¸âƒ£ í•™ìŠµë¥  (`--learning-rate`)** â­â­â­

```python
--learning-rate 1e-05  # 0.00001
```

**ë¹„ìœ **: ê±·ëŠ” ì†ë„

```
ğŸƒğŸ’¨ ë„ˆë¬´ ë¹ ë¦„ (1e-3 = 0.001):
     Loss: 2.0 â†’ 3.5 â†’ 1.8 â†’ 4.2 â†’ nan
     ëª©í‘œ ì§€ì ì„ ì§€ë‚˜ì³ì„œ í—¤ë§´, ê²°êµ­ í­ë°œ!
     
ğŸš¶ ì ë‹¹í•¨ (1e-5 = 0.00001): âœ…
     Loss: 2.0 â†’ 1.5 â†’ 1.0 â†’ 0.5
     ì•ˆì •ì ìœ¼ë¡œ ëª©í‘œ ë„ë‹¬!
     
ğŸŒ ë„ˆë¬´ ëŠë¦¼ (1e-7 = 0.0000001):
     Loss: 2.0 â†’ 1.99 â†’ 1.98 â†’ 1.97
     ë„ˆë¬´ ëŠë ¤ì„œ í•™ìŠµ ì•ˆ ë¨
```

**ì¶”ì²œ ê°’**:
| learning_rate | ìƒí™© |
|--------------|------|
| 1e-3 | âŒ ë„ˆë¬´ ë¹ ë¦„ (ë°œì‚° ìœ„í—˜) |
| 1e-4 | âš ï¸ ì¡°ê¸ˆ ë¹ ë¦„ (ëª¨ë‹ˆí„°ë§ í•„ìš”) |
| 1e-5 | âœ… ê¸°ë³¸ê°’ (ì•ˆì •ì ) |
| 1e-6 | ğŸŒ ë„ˆë¬´ ëŠë¦¼ (ì‹œê°„ ë‚­ë¹„) |

**ì‹¤ì „ íŒ**:
```
ì‹œì‘: 1e-5ë¡œ í•™ìŠµ
â†“
Loss í™•ì¸:
  - Lossê°€ ë°œì‚° (ì¦ê°€) â†’ 1e-6ìœ¼ë¡œ ë‚®ì¶”ê¸°
  - Lossê°€ ë„ˆë¬´ ëŠë¦¬ê²Œ ê°ì†Œ â†’ 5e-5ë¡œ ë†’ì´ê¸°
  - Lossê°€ ì•ˆì •ì ìœ¼ë¡œ ê°ì†Œ â†’ ê·¸ëŒ€ë¡œ ìœ ì§€ âœ…
```

---

#### **5ï¸âƒ£ LoRA ë ˆì´ì–´ ìˆ˜ (`--num-layers`)**

```python
--num-layers 16  # ë§ˆì§€ë§‰ 16ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
```

**ëª¨ë¸ êµ¬ì¡° ì´í•´í•˜ê¸°**:
```
Llama-3B = ì´ 28ê°œ ë ˆì´ì–´

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 1-12   â”‚ â† ê¸°ë³¸ ì–¸ì–´ ì§€ì‹ (ê³ ì • â„ï¸)
â”‚ (Frozen)     â”‚   "ê³ ì–‘ì´", "ì•ˆë…•", "is", "the" ê°™ì€ ê¸°ë³¸
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 13-28  â”‚ â† ì „ë¬¸ ì§€ì‹ (í•™ìŠµ ğŸ”¥)
â”‚ (LoRA)       â”‚   "NPC ë§íˆ¬", "íŒíƒ€ì§€ ìš©ì–´"
â”‚  16ê°œ ë ˆì´ì–´  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ì„ íƒ ê°€ì´ë“œ**:
| num_layers | í•™ìŠµ ë²”ìœ„ | ì†ë„ | í’ˆì§ˆ | íŒŒì¼ í¬ê¸° |
|-----------|----------|------|------|----------|
| 8 | ë§ˆì§€ë§‰ 1/4 | âš¡âš¡âš¡ | â­â­ | 8MB |
| 16 | ë§ˆì§€ë§‰ 1/2 | âš¡âš¡ | â­â­â­ | 16MB âœ… |
| 28 | ì „ì²´ | âš¡ | â­â­â­â­ | 32MB |
| -1 | ì „ì²´ | âš¡ | â­â­â­â­ | 32MB |

**ì–¸ì œ ëŠ˜ë ¤ì•¼ í• ê¹Œ?**
- ë³µì¡í•œ ìºë¦­í„° (ì—¬ëŸ¬ ì„±ê²©, ë§íˆ¬)
- ì „ë¬¸ ì§€ì‹ (ì˜í•™, ë²•ë¥ , ê²Œì„ ë£°)
- ê¸´ ëŒ€í™” ë¬¸ë§¥ ì´í•´

---

#### **6ï¸âƒ£ LoRA ë­í¬ (`lora_rank`)**

```python
"lora_rank": 8  # ë…¸íŠ¸ ë‘ê»˜ = 8
```

**ë¹„ìœ **: ì „ë¬¸ ë¶„ì•¼ ë…¸íŠ¸ì˜ ë‘ê»˜

```
Rank 4:  [â– â– â– â– ]
  íŒŒì¼ í¬ê¸°: 8MB
  í•™ìŠµ ì‹œê°„: 10ë¶„
  ìš©ë„: ê°„ë‹¨í•œ ìŠ¤íƒ€ì¼ ë³€í™” (ë§íˆ¬ë§Œ)
  
Rank 8:  [â– â– â– â– â– â– â– â– ] â­
  íŒŒì¼ í¬ê¸°: 16MB
  í•™ìŠµ ì‹œê°„: 20ë¶„
  ìš©ë„: ëŒ€ë¶€ë¶„ì˜ ì‘ì—… (ì¶”ì²œ!)
  
Rank 16: [â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– ]
  íŒŒì¼ í¬ê¸°: 32MB
  í•™ìŠµ ì‹œê°„: 40ë¶„
  ìš©ë„: ë³µì¡í•œ íƒœìŠ¤í¬ (ì„±ê²©+ì§€ì‹+ë¬¸ë§¥)
  
Rank 32: [32ê°œ...]
  íŒŒì¼ í¬ê¸°: 64MB
  í•™ìŠµ ì‹œê°„: 80ë¶„
  ìš©ë„: ë§¤ìš° ë³µì¡í•œ ì „ë¬¸ ë¶„ì•¼
```

**ìˆ˜í•™ì  ì˜ë¯¸**:
```
ì›ë³¸ ê°€ì¤‘ì¹˜: W (4096 Ã— 4096 = 16,777,216ê°œ)
LoRA ê°€ì¤‘ì¹˜: BÂ·A
  - B: (4096 Ã— rank)
  - A: (rank Ã— 4096)
  - ì´ ê°œìˆ˜: 2 Ã— 4096 Ã— rank

rank=8:  65,536ê°œ (ì›ë³¸ì˜ 0.4%)
rank=16: 131,072ê°œ (ì›ë³¸ì˜ 0.8%)
rank=32: 262,144ê°œ (ì›ë³¸ì˜ 1.6%)

â†’ ì ì€ íŒŒë¼ë¯¸í„°ë¡œ í° íš¨ê³¼!
```

**Rank ì„ íƒ ê°€ì´ë“œ**:
| íƒœìŠ¤í¬ ë³µì¡ë„ | ì¶”ì²œ Rank |
|-------------|----------|
| ë§íˆ¬ ë³€ê²½ (ì¡´ëŒ“ë§ â†’ ë°˜ë§) | 4 |
| NPC ìºë¦­í„° (ì„±ê²© 1ê°œ) | 8 â­ |
| ë³µí•© ìºë¦­í„° (ì—¬ëŸ¬ ì„±ê²©) | 16 |
| ì „ë¬¸ ì§€ì‹ (ì˜í•™, ë²•ë¥ ) | 32 |

---

#### **7ï¸âƒ£ LoRA ìŠ¤ì¼€ì¼ (`lora_scale`)**

```python
"lora_scale": 20.0  # ì›ë³¸ vs LoRA ë¹„ìœ¨
```

**ë¹„ìœ **: ìŠ¤í”¼ì»¤ ë³¼ë¥¨ ğŸ”Š

```
Scale 5:  ì›ë³¸ 90% + LoRA 10%
  ê²°ê³¼: ì›ë³¸ ëª¨ë¸ ëŠë‚Œ ê°•í•¨
  
Scale 10: ì›ë³¸ 80% + LoRA 20%
  ê²°ê³¼: ì¡°ê¸ˆ ë³€í™”
  
Scale 20: ì›ë³¸ 60% + LoRA 40% â­
  ê²°ê³¼: ëšœë ·í•œ ë³€í™” (ê¸°ë³¸ê°’)
  
Scale 40: ì›ë³¸ 40% + LoRA 60%
  ê²°ê³¼: ë§¤ìš° ê°•í•œ ë³€í™” (ê³¼í•˜ë©´ ì´ìƒí•´ì§)
```

**ì¡°ì • ì‹œê¸°**:
```
ë¬¸ì œ: í•™ìŠµí–ˆëŠ”ë° ì›ë˜ ëª¨ë¸ê³¼ ë¹„ìŠ·í•¨
í•´ê²°: scaleì„ 30~40ìœ¼ë¡œ ë†’ì´ê¸°

ë¬¸ì œ: ì´ìƒí•œ ë§ì„ í•¨ (ë„ˆë¬´ ê³¼ê²©)
í•´ê²°: scaleì„ 10~15ë¡œ ë‚®ì¶”ê¸°
```

---

#### **8ï¸âƒ£ ë°ì´í„° (`--data`)**

```python
--data "/path/to/data"
```

**í•„ìš”í•œ íŒŒì¼**:
```
data/
â”œâ”€â”€ train.jsonl  â† í•™ìŠµ ë°ì´í„° (í•„ìˆ˜)
â””â”€â”€ valid.jsonl  â† ê²€ì¦ ë°ì´í„° (ì„ íƒ)
```

**ë°ì´í„° í˜•ì‹ (JSONL)**:
```jsonl
{"text": "### Instruction:\në‹¹ì‹ ì€ ëŒ€ì¥ì¥ì´ NPCì…ë‹ˆë‹¤.\n\n### Input:\nê²€ì„ ì£¼ì„¸ìš”\n\n### Response:\n500ê³¨ë“œì…ë‹ˆë‹¤!"}
{"text": "### Instruction:\në‹¹ì‹ ì€ ì—˜í”„ NPCì…ë‹ˆë‹¤.\n\n### Input:\nê¸¸ì„ ìƒì—ˆì–´ìš”\n\n### Response:\në¹›ë‚˜ëŠ” ì´ë¼ë¥¼ ë”°ë¼ê°€ì„¸ìš”."}
```

**ì¢‹ì€ ë°ì´í„° vs ë‚˜ìœ ë°ì´í„°**:
```
âœ… ì¢‹ì€ ë°ì´í„°:
- ì¼ê´€ëœ í˜•ì‹
- ë‹¤ì–‘í•œ ì˜ˆì‹œ (ìµœì†Œ 100ê°œ)
- ëª…í™•í•œ ì…ë ¥/ì¶œë ¥
- ì ì ˆí•œ ê¸¸ì´ (ë„ˆë¬´ ì§§ì§€ë„, ê¸¸ì§€ë„ ì•Šê²Œ)

âŒ ë‚˜ìœ ë°ì´í„°:
- í˜•ì‹ì´ ì œê°ê°
- ë„ˆë¬´ ì ìŒ (10ê°œ ì´í•˜)
- ëª¨í˜¸í•œ ë‚´ìš©
- ë„ˆë¬´ ê¸´ ë¬¸ì¥ (2048 í† í° ì´ˆê³¼)
```

---

#### **9ï¸âƒ£ ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (`--max-seq-length`)**

```python
--max-seq-length 2048  # ì•½ 1500ë‹¨ì–´
```

**í† í° ê³„ì‚°**:
```
ì˜ì–´: 1 ë‹¨ì–´ â‰ˆ 1 í† í°
í•œêµ­ì–´: 1 ë‹¨ì–´ â‰ˆ 2~3 í† í°

2048 í† í° â‰ˆ 
  - ì˜ì–´: 2000ë‹¨ì–´
  - í•œêµ­ì–´: 700~1000ë‹¨ì–´
```

**ìš©ë„ë³„ ì¶”ì²œ**:
| ìš©ë„ | ì¶”ì²œ ê¸¸ì´ | ë©”ëª¨ë¦¬ |
|------|----------|--------|
| ì§§ì€ ëŒ€í™” (NPC) | 512 | 4GB |
| ì¼ë°˜ ëŒ€í™” | 2048 | 8GB â­ |
| ê¸´ ë¬¸ì„œ ìš”ì•½ | 4096 | 16GB |
| ì±… ë¶„ì„ | 8192 | 32GB |

---

#### **ğŸ”Ÿ ë¡œê¹… (`--steps-per-report`)**

```python
--steps-per-report 10  # 10ë²ˆë§ˆë‹¤ ì§„í–‰ ìƒí™©
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
Iter 10: Train loss 2.345, Tokens/sec 1000, It/sec 4.5
        â†“          â†“               â†“              â†“
     í˜„ì¬ ë°˜ë³µ   í‹€ë¦° ì •ë„      ì²˜ë¦¬ ì†ë„      í•™ìŠµ ì†ë„
```

**ì¡°ì ˆ**:
- `steps_per_report=5`: ìì£¼ ë³´ê¸° (ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§)
- `steps_per_report=10`: ë³´í†µ â­
- `steps_per_report=50`: ê°€ë” ë³´ê¸° (ë¡œê·¸ ì ìŒ)

---

#### **1ï¸âƒ£1ï¸âƒ£ ê²€ì¦ (`--val-batches`, `--steps-per-eval`)**

```python
--val-batches 25         # ëª¨ì˜ê³ ì‚¬ 25ë¬¸ì œ
--steps-per-eval 50      # 50ë²ˆë§ˆë‹¤ ëª¨ì˜ê³ ì‚¬
```

**ë¹„ìœ **: ëª¨ì˜ê³ ì‚¬ ì‹œìŠ¤í…œ

```
í•™ìŠµ 50ë²ˆ â†’ ëª¨ì˜ê³ ì‚¬ 25ë¬¸ì œ í’€ê¸° â†’ ì ìˆ˜ í™•ì¸
í•™ìŠµ 50ë²ˆ â†’ ëª¨ì˜ê³ ì‚¬ 25ë¬¸ì œ í’€ê¸° â†’ ì ìˆ˜ í™•ì¸
...
```

**ì¶œë ¥ ì˜ˆì‹œ**:
```
Iter 50: Val loss 1.234
Iter 100: Val loss 0.987
Iter 150: Val loss 0.856
```

**ì¢‹ì€ í•™ìŠµ vs ë‚˜ìœ í•™ìŠµ**:
```
âœ… ì¢‹ì€ í•™ìŠµ:
Train loss: 2.0 â†’ 1.5 â†’ 1.0 â†’ 0.5
Valid loss: 2.1 â†’ 1.6 â†’ 1.2 â†’ 0.7
(ë¹„ìŠ·í•˜ê²Œ ê°ì†Œ = ì˜ ë°°ì›€!)

âŒ ê³¼ì í•©:
Train loss: 2.0 â†’ 1.0 â†’ 0.3 â†’ 0.1
Valid loss: 2.1 â†’ 1.5 â†’ 2.0 â†’ 3.5
(Trainì€ ì¢‹ì€ë° ValidëŠ” ë‚˜ë¹ ì§ = ì•”ê¸°ë§Œ í•¨!)
```

---

#### **1ï¸âƒ£2ï¸âƒ£ ì €ì¥ (`--save-every`, `--adapter-path`)**

```python
--save-every 100            # 100ë²ˆë§ˆë‹¤ ì €ì¥
--adapter-path "/path/to/"  # ì €ì¥ ìœ„ì¹˜
```

**ì €ì¥ë˜ëŠ” íŒŒì¼**:
```
models/llama-game-npc-mlx/
â”œâ”€â”€ adapters.safetensors       (16MB, í•™ìŠµ ê²°ê³¼!)
â”œâ”€â”€ adapter_config.json        (1KB, ì„¤ì •)
â””â”€â”€ checkpoint-100/            (ì¤‘ê°„ ì €ì¥)
    â””â”€â”€ adapters.safetensors
```

**ì²´í¬í¬ì¸íŠ¸ í™œìš©**:
```bash
# í•™ìŠµ ì¤‘ ì»´í“¨í„° êº¼ì§ ğŸ’”
# í•˜ì§€ë§Œ checkpoint-100ì´ ìˆìŒ!

# ì´ì–´ì„œ í•™ìŠµ:
python -m mlx_lm.lora \
  --model ... \
  --resume-adapter-file checkpoint-100/adapters.safetensors
  
# 50ë²ˆë¶€í„° ì‹œì‘ (100ë²ˆ ê±´ë„ˆëœ€) âœ…
```

---

#### **1ï¸âƒ£3ï¸âƒ£ ì˜µí‹°ë§ˆì´ì € (`--optimizer`)**

```python
--optimizer adam  # í•™ìŠµ ë°©ë²•
```

**ì˜µí‹°ë§ˆì´ì € ë¹„êµ**:
| ì´ë¦„ | ì†ë„ | ì•ˆì •ì„± | ë©”ëª¨ë¦¬ | ì¶”ì²œ |
|------|------|--------|--------|------|
| SGD | ë¹ ë¦„ | ë‚®ìŒ | ë‚®ìŒ | âŒ ì˜¤ë˜ë¨ |
| Adam | ë³´í†µ | ë†’ìŒ | ë³´í†µ | âœ… ê¸°ë³¸ê°’ |
| AdamW | ë³´í†µ | ë§¤ìš° ë†’ìŒ | ë†’ìŒ | â­ ê³ ê¸‰ |
| Adafactor | ë¹ ë¦„ | ë†’ìŒ | ë‚®ìŒ | ğŸ¯ ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ |

**ë¹„ìœ **:
```
SGD:      ìì „ê±° (ë¹ ë¥´ì§€ë§Œ ë¶ˆì•ˆì •)
Adam:     ìë™ì°¨ (ì•ˆì •ì , ì¼ë°˜ì )
AdamW:    ê³ ê¸‰ ì„¸ë‹¨ (ë§¤ìš° ì•ˆì •ì )
Adafactor: ê²½ì°¨ (ì—°ë¹„ ì¢‹ìŒ, ë©”ëª¨ë¦¬ ì ˆì•½)
```

---

### ğŸ¯ **ìƒí™©ë³„ ì¶”ì²œ ì„¤ì •**

#### **âš¡ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10ë¶„)**
```python
CONFIG = {
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",  # ì‘ì€ ëª¨ë¸
    "iters": 50,
    "batch_size": 8,
    "lora_rank": 4,
    "lora_layers": 8,
    "learning_rate": 1e-5,
}
```
**ìš©ë„**: ì½”ë“œ ì‘ë™ í™•ì¸, ë°ì´í„° í˜•ì‹ í…ŒìŠ¤íŠ¸
**ê²°ê³¼**: Loss 2.5 â†’ 1.8 (ê°„ë‹¨í•œ í™•ì¸ìš©)

---

#### **ğŸ“ ê¸°ë³¸ í•™ìŠµ (30ë¶„)**
```python
CONFIG = {
    "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",  # ì¤‘ê°„ ëª¨ë¸ â­
    "iters": 100,
    "batch_size": 4,
    "lora_rank": 8,
    "lora_layers": 16,
    "learning_rate": 1e-5,
}
```
**ìš©ë„**: ì¼ë°˜ì ì¸ NPC ëŒ€í™”, ê°„ë‹¨í•œ ìŠ¤íƒ€ì¼ ë³€í™”
**ê²°ê³¼**: Loss 2.5 â†’ 1.2 (ì‹¤ìš© ê°€ëŠ¥)

---

#### **ğŸ† ì‹¤ì „ í•™ìŠµ (2ì‹œê°„)**
```python
CONFIG = {
    "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",
    "iters": 500,
    "batch_size": 4,
    "lora_rank": 16,
    "lora_layers": -1,  # ì „ì²´ ë ˆì´ì–´
    "learning_rate": 5e-6,  # ë” ë‚®ì€ í•™ìŠµë¥ 
    "optimizer": "adamw",  # ë” ì•ˆì •ì 
}
```
**ìš©ë„**: í”„ë¡œë•ì…˜ ë°°í¬, ë³µì¡í•œ ìºë¦­í„°
**ê²°ê³¼**: Loss 2.5 â†’ 0.6 (ê³ í’ˆì§ˆ)

---

#### **ğŸ’ ìµœê³  í’ˆì§ˆ (5ì‹œê°„+)**
```python
CONFIG = {
    "model": "mlx-community/Meta-Llama-3.1-8B-Instruct-4bit",  # í° ëª¨ë¸
    "iters": 1000,
    "batch_size": 2,
    "lora_rank": 32,
    "lora_layers": -1,
    "learning_rate": 1e-6,
    "optimizer": "adamw",
}
```
**ìš©ë„**: ê³ ê¸‰ AI, ì „ë¬¸ ì§€ì‹, ìƒì—…ìš©
**ê²°ê³¼**: Loss 2.5 â†’ 0.3 (ìµœê³  í’ˆì§ˆ)

---

### ğŸš¨ **íŠ¸ëŸ¬ë¸”ìŠˆíŒ… - ì¸ì ì¡°ì •**

#### **ë¬¸ì œ 1: ë©”ëª¨ë¦¬ ë¶€ì¡± ğŸ’¾**
```bash
RuntimeError: out of memory
```

**í•´ê²° ìˆœì„œ**:
1. `batch_size` ì¤„ì´ê¸°: 8 â†’ 4 â†’ 2 â†’ 1
2. `lora_rank` ì¤„ì´ê¸°: 16 â†’ 8 â†’ 4
3. ì‘ì€ ëª¨ë¸ ì‚¬ìš©: 3B â†’ 1B
4. `max_seq_length` ì¤„ì´ê¸°: 2048 â†’ 1024

---

#### **ë¬¸ì œ 2: Lossê°€ ì•ˆ ì¤„ì–´ë“¦ ğŸ“‰**
```
Iter 100: Train loss 2.5
Iter 200: Train loss 2.48
Iter 300: Train loss 2.47 (ê±°ì˜ ë³€í™” ì—†ìŒ)
```

**í•´ê²°**:
1. `learning_rate` ë†’ì´ê¸°: 1e-5 â†’ 5e-5 â†’ 1e-4
2. `iters` ëŠ˜ë¦¬ê¸°: 100 â†’ 500 (ë” ì˜¤ë˜ í•™ìŠµ)
3. **ë°ì´í„° í’ˆì§ˆ í™•ì¸** â­:
   - í˜•ì‹ì´ ì¼ê´€ì ì¸ì§€?
   - ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€? (ìµœì†Œ 100ê°œ)
   - ì˜ë¯¸ ìˆëŠ” ë‚´ìš©ì¸ì§€?

---

#### **ë¬¸ì œ 3: Lossê°€ ë°œì‚° (í­ë°œ) ğŸ’¥**
```
Iter 10: Train loss 2.5
Iter 20: Train loss 3.8
Iter 30: Train loss nan (í­ë°œ!)
```

**í•´ê²°**:
1. `learning_rate` ë‚®ì¶”ê¸°: 1e-4 â†’ 1e-5 â†’ 1e-6
2. `batch_size` ì¤„ì´ê¸°: 8 â†’ 4
3. ë°ì´í„° í™•ì¸:
   - ì´ìƒí•œ ë¬¸ì (íŠ¹ìˆ˜ë¬¸ì, ì´ëª¨ì§€)
   - ë„ˆë¬´ ê¸´ ë¬¸ì¥ (2048 ì´ˆê³¼)
   - ë¹ˆ ë°ì´í„°

---

#### **ë¬¸ì œ 4: í•™ìŠµì€ ë˜ëŠ”ë° ì‹¤ì „ì—ì„œ ëª» ì”€ ğŸ­**
```
Train loss: 0.3 (ì™„ë²½!)
Valid loss: 3.5 (ì‹¤íŒ¨...)
```

**ì›ì¸**: **ê³¼ì í•©** (Overfitting)
- í•™ìŠµ ë°ì´í„°ë§Œ ë‹¬ë‹¬ ì™¸ì›€
- ìƒˆë¡œìš´ ë°ì´í„°ëŠ” ëª» ë´„

**í•´ê²°** (ê³¼ì í•© ë°©ì§€):
1. `lora_rank` ì¤„ì´ê¸°: 16 â†’ 8 â†’ 4
2. ë°ì´í„° ë” ì¶”ê°€í•˜ê¸° (ë‹¤ì–‘í•œ ì˜ˆì‹œ)
3. `lora_layers` ì¤„ì´ê¸°: -1 â†’ 16
4. Dropout ì¶”ê°€: `lora_dropout=0.1`
5. Early Stopping (validation loss ì¦ê°€ ì‹œ ì¤‘ë‹¨)

---

### ğŸ›ï¸ **ì„¤ì • ë³€ê²½ ë°©ë²•**

**`training/scripts/finetune_mlx.py` ìˆ˜ì •**:

```python
CONFIG = {
    # ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ìˆ˜ì •
    "iters": 500,         # 100 â†’ 500
    "lora_rank": 16,      # 8 â†’ 16
    "lora_layers": -1,    # 16 â†’ -1 (ì „ì²´)
    
    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
    "iters": 50,
    "batch_size": 8,
    "lora_rank": 4,
}
```

---

## 6. ì‹¤ì œ ì‹¤í–‰ ë¡œê·¸ ë¶„ì„

### ğŸ“Š **ì •ìƒ ì‹¤í–‰ ë¡œê·¸**

```bash
ğŸ Apple MLX íŒŒì¸íŠœë‹ í™˜ê²½ ì‹œì‘
================================
âœ… ì‹œìŠ¤í…œ: macOS (arm64)

âœ… MLX ê°€ìƒí™˜ê²½ ì¡´ì¬

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸš€ MLX íŒŒì¸íŠœë‹ ì‹¤í–‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ Apple MLX íŒŒì¸íŠœë‹ ì‹œì‘!
   MLX Device: Metal GPU
   Memory: Unified Memory

âœ… ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ!
   Train: /path/to/train.jsonl (100ê°œ)
   Valid: /path/to/valid.jsonl (10ê°œ)

ğŸ“¦ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: mlx-community/Llama-3.2-3B-Instruct-4bit
   (ì²« ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤...)
âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!

âœ… LoRA ì–´ëŒ‘í„° ì¶”ê°€ ì™„ë£Œ!
   - LoRA Rank: 16
   - Trainable params: 20,971,520

ğŸš€ MLX LoRA íŒŒì¸íŠœë‹ ì‹œì‘!
   Iterations: 100
   LoRA Rank: 8
   Learning Rate: 1e-05

Loading pretrained model
Loading datasets
Training
Iter 1: Train loss 2.456, It/sec 4.231, Tokens/sec 1024.5
Iter 10: Train loss 1.823, It/sec 4.567, Tokens/sec 1123.4
Iter 20: Train loss 1.456, It/sec 4.432, Tokens/sec 1089.7
Iter 30: Train loss 1.234, It/sec 4.398, Tokens/sec 1076.2
...
Iter 90: Train loss 0.567, It/sec 4.421, Tokens/sec 1082.3
Iter 100: Train loss 0.523, It/sec 4.411, Tokens/sec 1079.8
Saved final adapter weights to models/llama-game-npc-mlx/adapters.safetensors

âœ… íŒŒì¸íŠœë‹ ì™„ë£Œ!
   - í•™ìŠµ ì‹œê°„: 634.21ì´ˆ
   - ìƒ˜í”Œ/ì´ˆ: 2.34

ğŸ’¾ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜:
   /path/to/models/llama-game-npc-mlx
   â”œâ”€ adapters.safetensors (LoRA ê°€ì¤‘ì¹˜)
   â””â”€ adapter_config.json (ì„¤ì •)

ğŸ® ì¶”ë¡  í…ŒìŠ¤íŠ¸:
ì…ë ¥: ë‹¹ì‹ ì€ íŒíƒ€ì§€ ê²Œì„ì˜ ì—¬ê´€ ì£¼ì¸ì…ë‹ˆë‹¤. í”Œë ˆì´ì–´: ë°©ì„ ë¹Œë¦´ ìˆ˜ ìˆë‚˜ìš”? NPC:
ì¶œë ¥: *ë”°ëœ»í•œ ë¯¸ì†Œë¥¼ ì§€ìœ¼ë©°* í™˜ì˜í•©ë‹ˆë‹¤! 2ì¸µì— ê¹¨ë—í•œ ë°©ì´ ìˆìŠµë‹ˆë‹¤. 1ë°•ì— 10ì‹¤ë²„ì…ë‹ˆë‹¤. ì‹ì‚¬ë„ í¬í•¨ì´ì—ìš”.

âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ! ğŸ‰
```

### ğŸ” **ë¡œê·¸ í•´ì„**

#### **Loss ê°’ ë¶„ì„**
```
Iter 1:   Train loss 2.456  â† ì‹œì‘ (ë†’ìŒ)
Iter 10:  Train loss 1.823  â† ë¹ ë¥´ê²Œ ê°ì†Œ
Iter 50:  Train loss 0.823  â† ê³„ì† ê°ì†Œ
Iter 100: Train loss 0.523  â† ìˆ˜ë ´ (ë‚®ìŒ)
```

**ì¢‹ì€ í•™ìŠµ**:
- Lossê°€ ì§€ì†ì ìœ¼ë¡œ ê°ì†Œ
- 0.5~1.0 ì‚¬ì´ë¡œ ìˆ˜ë ´

**ë‚˜ìœ í•™ìŠµ**:
- Lossê°€ ë°œì‚° (ì¦ê°€)
- Lossê°€ ë³€í•˜ì§€ ì•ŠìŒ (í•™ìŠµë¥  ë„ˆë¬´ ë‚®ìŒ)

#### **ì†ë„ ì§€í‘œ**
```
It/sec: 4.5          # Iteration per second (ë†’ì„ìˆ˜ë¡ ë¹ ë¦„)
Tokens/sec: 1080     # ì´ˆë‹¹ ì²˜ë¦¬ í† í° ìˆ˜
```

**M2 Mac ê¸°ì¤€**:
- 3B ëª¨ë¸: 4~5 it/sec
- 1B ëª¨ë¸: 8~10 it/sec

---

## 7. ê²°ê³¼ë¬¼ ì´í•´í•˜ê¸°

### ğŸ“¦ **ìƒì„±ëœ íŒŒì¼**

```
models/llama-game-npc-mlx/
â”œâ”€â”€ adapters.safetensors    (16MB)
â””â”€â”€ adapter_config.json     (1KB)
```

#### **`adapters.safetensors`**
- **ë‚´ìš©**: LoRA í•™ìŠµëœ ê°€ì¤‘ì¹˜ (16MB)
- **í˜•ì‹**: SafeTensors (PyTorch í˜¸í™˜)
- **í¬ê¸°**: rank=8 ê¸°ì¤€ ì•½ 16MB
- **ìš©ë„**: ì›ë³¸ ëª¨ë¸ê³¼ í•©ì³ì„œ ì¶”ë¡ 

#### **`adapter_config.json`**
```json
{
    "model": "mlx-community/Llama-3.2-3B-Instruct-4bit",
    "iters": 100,
    "lora_parameters": {
        "rank": 8,
        "dropout": 0.0,
        "scale": 20.0
    },
    "num_layers": 16,
    "learning_rate": 1e-05,
    ...
}
```
- **ë‚´ìš©**: í•™ìŠµ ì„¤ì • ê¸°ë¡
- **ìš©ë„**: ì¬í˜„, ë¶„ì„, ë¬¸ì„œí™”

---

### ğŸ¯ **í•™ìŠµ ê²°ê³¼ ì‚¬ìš©í•˜ê¸°**

#### **Pythonì—ì„œ ì§ì ‘ ì‚¬ìš©**
```python
from mlx_lm import load, generate

# ì›ë³¸ + ì–´ëŒ‘í„° ìë™ ë³‘í•©
model, tokenizer = load(
    "mlx-community/Llama-3.2-3B-Instruct-4bit",
    adapter_path="models/llama-game-npc-mlx"
)

# ì¶”ë¡ 
prompt = "ë‹¹ì‹ ì€ ê²Œì„ NPCì…ë‹ˆë‹¤. í”Œë ˆì´ì–´: ì•ˆë…•í•˜ì„¸ìš”. NPC:"
response = generate(
    model,
    tokenizer,
    prompt=prompt,
    max_tokens=100,
    temp=0.7
)

print(response)
# â†’ "í™˜ì˜í•©ë‹ˆë‹¤! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
```

#### **API ì„œë²„ë¡œ ë°°í¬ (FastAPI)**
```python
from fastapi import FastAPI
from mlx_lm import load, generate

app = FastAPI()

# ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ
model, tokenizer = load(
    "mlx-community/Llama-3.2-3B-Instruct-4bit",
    adapter_path="models/llama-game-npc-mlx"
)

@app.post("/generate")
async def generate_text(prompt: str):
    response = generate(model, tokenizer, prompt=prompt)
    return {"response": response}

# uvicorn main:app --reload
```

#### **Ollamaì— ë°°í¬ (ì„ íƒ)**
```bash
# 1. ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ê³¼ ë³‘í•©
python -m mlx_lm.convert \
  --model mlx-community/Llama-3.2-3B-Instruct-4bit \
  --adapter models/llama-game-npc-mlx \
  --output models/game-npc-merged

# 2. GGUF ë³€í™˜
# (ë³„ë„ llama.cpp ë„êµ¬ í•„ìš”)

# 3. Ollama ì¶”ê°€
ollama create game-npc -f Modelfile
ollama run game-npc
```

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### âŒ **ë¬¸ì œ 1: `ModuleNotFoundError: No module named 'mlx'`**

**ì›ì¸**: ê°€ìƒí™˜ê²½ í™œì„±í™” ì•ˆ ë¨

**í•´ê²°**:
```bash
cd training
source mlx-env/bin/activate
python scripts/finetune_mlx.py
```

---

### âŒ **ë¬¸ì œ 2: `lora.py: error: unrecognized arguments: --lora-layers`**

**ì›ì¸**: ì˜ëª»ëœ ì¸ì ì´ë¦„

**í•´ê²°**: `finetune_mlx.py` ìˆ˜ì •
```python
# ì˜ëª»ë¨
"--lora-layers", str(CONFIG['lora_layers']),

# ì˜¬ë°”ë¦„
"--num-layers", str(CONFIG['lora_layers']),
```

---

### âŒ **ë¬¸ì œ 3: `OutOfMemoryError`**

**ì›ì¸**: ë©”ëª¨ë¦¬ ë¶€ì¡±

**í•´ê²°**: `CONFIG` ìˆ˜ì •
```python
CONFIG = {
    "batch_size": 2,  # 4 â†’ 2
    "lora_rank": 4,   # 8 â†’ 4
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",  # 3B â†’ 1B
}
```

---

### âŒ **ë¬¸ì œ 4: Lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ**

**ì›ì¸**: í•™ìŠµë¥ ì´ ë„ˆë¬´ ë‚®ìŒ

**í•´ê²°**:
```python
CONFIG = {
    "learning_rate": 1e-4,  # 1e-5 â†’ 1e-4
}
```

---

### âŒ **ë¬¸ì œ 5: Lossê°€ ë°œì‚° (NaN)**

**ì›ì¸**: í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ìŒ

**í•´ê²°**:
```python
CONFIG = {
    "learning_rate": 1e-6,  # 1e-5 â†’ 1e-6
}
```

---

## ğŸ¯ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… **ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­**

- [ ] Mac (Apple Silicon ê¶Œì¥)
- [ ] Python 3.8 ì´ìƒ ì„¤ì¹˜
- [ ] ìµœì†Œ 8GB RAM
- [ ] 10GB ì´ìƒ ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„
- [ ] ì¸í„°ë„· ì—°ê²° (ì²« ì‹¤í–‰ ì‹œ)

### âœ… **ì‹¤í–‰ í›„ í™•ì¸ì‚¬í•­**

- [ ] `adapters.safetensors` íŒŒì¼ ìƒì„±ë¨
- [ ] `adapter_config.json` íŒŒì¼ ìƒì„±ë¨
- [ ] Loss ê°’ì´ ê°ì†Œí–ˆëŠ”ì§€ í™•ì¸
- [ ] ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì •ìƒ ë™ì‘

---

## ğŸ“š ìš”ì•½

### **í•µì‹¬ í¬ì¸íŠ¸**

1. **MLX = Apple Silicon ì „ìš© íŒŒì¸íŠœë‹ ë„êµ¬**
   - Metal GPU ê°€ì†
   - ë¹ ë¥´ê³  ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

2. **LoRA = ì‘ì€ ì–´ëŒ‘í„°ë§Œ í•™ìŠµ**
   - ì›ë³¸ 2GBëŠ” ê·¸ëŒ€ë¡œ
   - ì–´ëŒ‘í„° 16MBë§Œ ìƒì„±

3. **í•œ ì¤„ë¡œ ì‹¤í–‰**
   ```bash
   sh scripts/start-mlx-training.sh
   ```

4. **ê²°ê³¼ë¬¼**
   - `adapters.safetensors` (16MB)
   - Pythonì—ì„œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥

5. **ì»¤ìŠ¤í„°ë§ˆì´ì§•**
   - `finetune_mlx.py`ì˜ `CONFIG` ìˆ˜ì •
   - `train.jsonl`ì— ë°ì´í„° ì¶”ê°€

---

**ì´ì œ ì™„ë²½í•˜ê²Œ ì´í•´ë˜ì…¨ë‚˜ìš”? ğŸš€**

ë°”ë¡œ ì‹¤í–‰í•´ë³´ì„¸ìš”:
```bash
cd /Users/taegyunkim/bboing/ollama_model/my-ai-platform
sh scripts/start-mlx-training.sh
```
