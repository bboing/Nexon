"""
ê²€ìƒ‰ ì‹œìŠ¤í…œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

MRR, nDCG@K, Precision@K, Recall@K ê³„ì‚°
"""
import sys
import json
import asyncio
import time
import numpy as np
import importlib
from pathlib import Path
from typing import List, Dict, Any


# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR / "langchain_app"))

from database.session import get_async_db

def calculate_mrr(results: List[Dict], ground_truth: List[str]) -> float:
    """
    MRR (Mean Reciprocal Rank) ê³„ì‚°
    
    ì²« ë²ˆì§¸ ì •ë‹µì˜ ìˆœìœ„ ì—­ìˆ˜
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        ground_truth: ì •ë‹µ ì—”í‹°í‹° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        MRR ì ìˆ˜ (0~1)
    """
    for rank, result in enumerate(results, start=1):
        name = result.get("data", {}).get("canonical_name", "")
        if name in ground_truth:
            return 1.0 / rank
    return 0.0


def calculate_ndcg(
    results: List[Dict],
    relevance: Dict[str, int],
    k: int = 10
) -> float:
    """
    nDCG@K (Normalized Discounted Cumulative Gain) ê³„ì‚°
    
    ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì •í™•ë„
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        relevance: ì—”í‹°í‹°ë³„ ê´€ë ¨ë„ {ì´ë¦„: ì ìˆ˜(0-3)}
        k: ìƒìœ„ Kê°œê¹Œì§€ í‰ê°€
        
    Returns:
        nDCG ì ìˆ˜ (0~1)
    """
    # DCG ê³„ì‚°
    dcg = 0.0
    for rank, result in enumerate(results[:k], start=1):
        name = result.get("data", {}).get("canonical_name", "")
        rel = relevance.get(name, 0)
        if rel > 0:
            dcg += rel / np.log2(rank + 1)
    
    # IDCG ê³„ì‚° (ì´ìƒì  ìˆœì„œ)
    ideal_rels = sorted(relevance.values(), reverse=True)
    idcg = sum(rel / np.log2(rank + 1) for rank, rel in enumerate(ideal_rels[:k], start=1) if rel > 0)
    
    return dcg / idcg if idcg > 0 else 0.0


def calculate_precision_at_k(
    results: List[Dict],
    ground_truth: List[str],
    k: int = 5
) -> float:
    """
    Precision@K ê³„ì‚°
    
    ìƒìœ„ Kê°œ ì¤‘ ì •ë‹µ ë¹„ìœ¨
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        ground_truth: ì •ë‹µ ì—”í‹°í‹° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ Kê°œ
        
    Returns:
        Precision ì ìˆ˜ (0~1)
    """
    top_k = [r.get("data", {}).get("canonical_name", "") for r in results[:k]]
    hits = len(set(top_k) & set(ground_truth))
    return hits / k if k > 0 else 0.0


def calculate_recall_at_k(
    results: List[Dict],
    ground_truth: List[str],
    k: int = 10
) -> float:
    """
    Recall@K ê³„ì‚°
    
    ì „ì²´ ì •ë‹µ ì¤‘ ìƒìœ„ Kê°œì— í¬í•¨ëœ ë¹„ìœ¨
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        ground_truth: ì •ë‹µ ì—”í‹°í‹° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ Kê°œ
        
    Returns:
        Recall ì ìˆ˜ (0~1)
    """
    top_k = [r.get("data", {}).get("canonical_name", "") for r in results[:k]]
    hits = len(set(top_k) & set(ground_truth))
    return hits / len(ground_truth) if ground_truth else 0.0


async def evaluate_single_query(
    searcher: "HybridSearcher",
    test_case: Dict[str, Any],
    verbose: bool = False
) -> Dict[str, float]:
    """
    ë‹¨ì¼ ì§ˆë¬¸ í‰ê°€
    
    Args:
        searcher: HybridSearcher ì¸ìŠ¤í„´ìŠ¤
        test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        
    Returns:
        í‰ê°€ ë©”íŠ¸ë¦­
    """
    query = test_case["query"]
    ground_truth = test_case["ground_truth"]
    relevance = test_case["relevance"]
    print(f"relevance:{relevance}")
    print(f"ground_truth:{ground_truth}")
    # ê²€ìƒ‰ ì‹¤í–‰ + ì‘ë‹µì‹œê°„ ì¸¡ì •
    try:
        print("evaluate_search.evaluate_single_query í˜¸ì¶œ ë¨")
        t_start = time.perf_counter()
        results = await searcher.search(query, limit=10)
        latency_ms = (time.perf_counter() - t_start) * 1000
        print(f"results: {results}")
    except Exception:
        import traceback
        traceback.print_exc()
        raise

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    # try:
    #     mrr = calculate_mrr(results, ground_truth)
    #     ndcg_10 = calculate_ndcg(results, relevance, k=10)
    #     ndcg_5 = calculate_ndcg(results, relevance, k=5)
    #     precision_5 = calculate_precision_at_k(results, ground_truth, k=5)
    #     recall_10 = calculate_recall_at_k(results, ground_truth, k=10)
    # except Exception as e:
    #     import traceback
    #     traceback.print_exc()
    #     raise
    mrr = calculate_mrr(results, ground_truth)
    ndcg_10 = calculate_ndcg(results, relevance, k=10)
    ndcg_5 = calculate_ndcg(results, relevance, k=5)
    precision_5 = calculate_precision_at_k(results, ground_truth, k=5)
    recall_10 = calculate_recall_at_k(results, ground_truth, k=10)
    
    metrics = {
        "mrr": mrr,
        "ndcg@10": ndcg_10,
        "ndcg@5": ndcg_5,
        "precision@5": precision_5,
        "recall@10": recall_10,
        "latency_ms": latency_ms
    }
    
    if verbose:
        print(f"\nì§ˆë¬¸: {query}")
        print(f"ì •ë‹µ: {ground_truth}")
        print(f"ê²°ê³¼ ({len(results)}ê°œ):")
        for i, r in enumerate(results[:5], 1):
            name = r.get("data", {}).get("canonical_name", "Unknown")
            score = r.get("score", 0)
            is_correct = "âœ…" if name in ground_truth else "  "
            print(f"  {i}. {name} ({score:.2f}) {is_correct}")
        print(f"ë©”íŠ¸ë¦­: MRR={mrr:.3f}, nDCG@10={ndcg_10:.3f}, P@5={precision_5:.3f}, ì‘ë‹µì‹œê°„={latency_ms:.0f}ms")
    
    return metrics


async def evaluate_search_system(
    test_file: str = "training/data/test/search_test_queries.json",
    verbose: bool = True,
    option: int = 0
) -> Dict[str, Any]:
    """
    ê²€ìƒ‰ ì‹œìŠ¤í…œ ì „ì²´ í‰ê°€
    
    Args:
        test_file: í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        option: 0=í˜„ì¬(Plan), 2=ì„ê³„ê°’, 3=Intent, 4=ì™„ì „ë³‘ë ¬
        
    Returns:
        í‰ê°€ ê²°ê³¼
    """
    import importlib

    module_configs = {
        0: {"path": "src.retrievers.hybrid_searcher", "name": "í˜„ì¬ (Plan ê¸°ë°˜)"},
        2: {"path": "src.retrievers.hybrid_searcher_option2", "name": "Option 2 (ì„ê³„ê°’ ê¸°ë°˜)"},
        3: {"path": "src.retrievers.hybrid_searcher_option3", "name": "Option 3 (Intent ê¸°ë°˜)"},
        4: {"path": "src.retrievers.hybrid_searcher_option4", "name": "Option 4 (ì™„ì „ ë³‘ë ¬ + í‚¤ì›Œë“œ)"},
        5: {"path": "src.retrievers.hybrid_searcher_sep", "name": "sep (Plan + í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¥˜ + ë™ì˜ì–´ ì„œì¹˜)"},
        6: {"path": "src.retrievers.hybrid_searcher_hop", "name": "hop (ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜ ì ìš©)"},
        7: {"path": "src.retrievers.hybrid_searcher_fin", "name": "fin (HOP êµ¬ì¡° + PG canonical_name â†’ Neo4j ë³´ê°•)"}
    }
    
    config = module_configs.get(option, module_configs[0])
    option_name = config["name"]

    # 2. ë™ì  ì„í¬íŠ¸ ì‹¤í–‰ (ë¹¨ê°„ ì¤„/ìºì‹± ë°©ì§€ í•µì‹¬)
    # import_moduleì€ "src.retrievers.hybrid_searcher" ê°™ì€ ë¬¸ìì—´ì„ ë°›ìŒ
    module = importlib.import_module(config["path"])
    importlib.reload(module)  # í˜¹ì‹œ ëª¨ë¥¼ ìºì‹œ ë°©ì§€
    
    # í•´ë‹¹ íŒŒì¼ ì•ˆì—ì„œ 'HybridSearcher'ë¼ëŠ” í´ë˜ìŠ¤ë¥¼ êº¼ë‚´ì˜´
    TargetSearcherClass = getattr(module, "HybridSearcher")

    # 3. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ
    test_path = ROOT_DIR / test_file
    with open(test_path, "r", encoding="utf-8") as f:
        test_cases = json.load(f)
    
    print(f"{'='*60}")
    print(f"ê²€ìƒ‰ ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘: {option_name}")
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
    print(f"{'='*60}")
    
    all_metrics = []
    
    # 4. DB ì„¸ì…˜ ì•ˆì—ì„œ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¨ í´ë˜ìŠ¤ë¡œ ê°ì²´ ìƒì„±
    async for session in get_async_db():
        searcher = TargetSearcherClass(
            db=session,
            use_milvus=True,
            use_neo4j=True,
            use_router=True,
            verbose=False
        )
        print(f"DEBUG: [{option_name}] í´ë˜ìŠ¤ ìœ„ì¹˜ -> {searcher.__class__.__module__}")
        
        for i, test_case in enumerate(test_cases, 1):
            if verbose: print(f"\n[{i}/{len(test_cases)}] ", end="")
            try:
                metrics = await evaluate_single_query(searcher, test_case, verbose=verbose)
                all_metrics.append(metrics)
            except Exception as e:
                print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
                all_metrics.append({"mrr": 0.0, "ndcg@10": 0.0, "ndcg@5": 0.0, "precision@5": 0.0, "recall@10": 0.0, "latency_ms": 0.0})
        break
    
    if not all_metrics:
        print("âŒ í‰ê°€ëœ ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
        return {}

    # í‰ê·  ê³„ì‚° (latencyëŠ” ë³„ë„ í†µê³„)
    avg_metrics = {
        metric: np.mean([m[metric] for m in all_metrics])
        for metric in all_metrics[0].keys()
    }

    latencies = [m["latency_ms"] for m in all_metrics]
    latency_stats = {
        "mean_ms": float(np.mean(latencies)),
        "median_ms": float(np.median(latencies)),
        "p95_ms": float(np.percentile(latencies, 95)),
        "min_ms": float(np.min(latencies)),
        "max_ms": float(np.max(latencies)),
    }

    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"MRR (Mean Reciprocal Rank):      {avg_metrics['mrr']:.4f}")
    print(f"nDCG@10:                          {avg_metrics['ndcg@10']:.4f}")
    print(f"nDCG@5:                           {avg_metrics['ndcg@5']:.4f}")
    print(f"Precision@5:                      {avg_metrics['precision@5']:.4f}")
    print(f"Recall@10:                        {avg_metrics['recall@10']:.4f}")
    print(f"--- ì‘ë‹µì‹œê°„ ---")
    print(f"í‰ê· :  {latency_stats['mean_ms']:.0f}ms")
    print(f"ì¤‘ê°„ê°’: {latency_stats['median_ms']:.0f}ms")
    print(f"P95:   {latency_stats['p95_ms']:.0f}ms")
    print(f"ìµœì†Œ:  {latency_stats['min_ms']:.0f}ms  /  ìµœëŒ€: {latency_stats['max_ms']:.0f}ms")
    print(f"{'='*60}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    category_metrics = {}
    for test_case, metrics in zip(test_cases, all_metrics):
        category = test_case["category"]
        if category not in category_metrics:
            category_metrics[category] = []
        category_metrics[category].append(metrics["mrr"])
    
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ MRR:")
    for category, mrrs in sorted(category_metrics.items()):
        avg_mrr = np.mean(mrrs)
        print(f"  {category:20s}: {avg_mrr:.4f}")
    
    return {
        "average": avg_metrics,
        "latency": latency_stats,
        "individual": all_metrics,
        "by_category": category_metrics
    }


async def compare_systems():
    """
    ì—¬ëŸ¬ ì‹œìŠ¤í…œ ë¹„êµ (í˜„ì¬ vs Option 2 vs Option 3 vs Option 4 vs sep vs hop vs fin)
    """
    print("\n" + "="*85)
    print("ê²€ìƒ‰ ì‹œìŠ¤í…œ ë¹„êµ: í˜„ì¬(Plan) vs Option 2(ì„ê³„ê°’) vs Option 3(Intent) vs Option 4(ì™„ì „ë³‘ë ¬) vs sep(Plan + í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¥˜ + ë™ì˜ì–´ ì„œì¹˜) vs hop(ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜ ì ìš©) vs fin(HOP êµ¬ì¡° + PG canonical_name â†’ Neo4j ë³´ê°•)")
    print("="*85)
    
    # í˜„ì¬ (Plan) í‰ê°€
    print("\n[1/7] í˜„ì¬ ì‹œìŠ¤í…œ (Plan ê¸°ë°˜) í‰ê°€ ì¤‘...")
    current_results = await evaluate_search_system(verbose=False, option=0)
    
    # Option 2 í‰ê°€
    print("\n[2/7] Option 2 (ì„ê³„ê°’ ê¸°ë°˜) í‰ê°€ ì¤‘...")
    option2_results = await evaluate_search_system(verbose=False, option=2)
    
    # Option 3 í‰ê°€
    print("\n[3/7] Option 3 (Intent ê¸°ë°˜) í‰ê°€ ì¤‘...")
    option3_results = await evaluate_search_system(verbose=False, option=3)
    
    # Option 4 í‰ê°€
    print("\n[4/7] Option 4 (ì™„ì „ ë³‘ë ¬ + í‚¤ì›Œë“œ) í‰ê°€ ì¤‘...")
    option4_results = await evaluate_search_system(verbose=False, option=4)

    # sep í‰ê°€
    print("\n[5/7] Option 5 (Plan + í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¥˜ + ë™ì˜ì–´ ì„œì¹˜) í‰ê°€ ì¤‘...")
    sep_results = await evaluate_search_system(verbose=False, option=5)

    # hop í‰ê°€
    print("\n[6/7] Option 6 (ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜ ì ìš©) í‰ê°€ ì¤‘...")
    hop_results = await evaluate_search_system(verbose=False, option=6)

    # fin í‰ê°€
    print("\n[7/7] Option 7 (HOP êµ¬ì¡° + PG canonical_name â†’ Neo4j ë³´ê°•) í‰ê°€ ì¤‘...")
    fin_results = await evaluate_search_system(verbose=False, option=7)

    
    # ë¹„êµí‘œ ì¶œë ¥
    print(f"\n{'='*125}")
    print(f"ë¹„êµ ê²°ê³¼")
    print(f"{'='*125}")
    print(f"{'ë©”íŠ¸ë¦­':<15s} {'í˜„ì¬(Plan)':<15s} {'Option 2':<15s} {'Option 3':<15s} {'Option 4':<15s} {'sep':<15s} {'hop':<15s} {'fin':<15s}")
    print(f"{'-'*125}")

    metrics = ["mrr", "ndcg@10", "ndcg@5", "precision@5", "recall@10"]
    for metric in metrics:
        current_val = current_results["average"][metric]
        opt2_val = option2_results["average"][metric]
        opt3_val = option3_results["average"][metric]
        opt4_val = option4_results["average"][metric]
        sep_val = sep_results["average"][metric]
        hop_val = hop_results["average"][metric]
        fin_val = fin_results["average"][metric]
        print(f"{metric:<15s} {current_val:<15.4f} {opt2_val:<15.4f} {opt3_val:<15.4f} {opt4_val:<15.4f} {sep_val:<15.4f} {hop_val:<15.4f} {fin_val:<15.4f}")

    # ì‘ë‹µì‹œê°„ ë¹„êµ (mean / p95)
    print(f"{'-'*125}")
    all_results = [current_results, option2_results, option3_results, option4_results, sep_results, hop_results, fin_results]
    means = "  ".join(f"{r['latency']['mean_ms']:<13.0f}" for r in all_results)
    p95s  = "  ".join(f"{r['latency']['p95_ms']:<13.0f}" for r in all_results)
    print(f"{'latency(mean)':<15s} {means}")
    print(f"{'latency(p95)':<15s} {p95s}")
    print(f"{'='*125}")

    # â”€â”€ ì‘ë‹µì‹œê°„ í•µì‹¬ ìš”ì•½ (Plan 1 / ìµœì € / ìµœê³  / Plan 7) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    all_named = [
        ("Plan 1 (í˜„ì¬)",    current_results["latency"]["mean_ms"],  current_results["latency"]["p95_ms"]),
        ("Option 2",         option2_results["latency"]["mean_ms"],  option2_results["latency"]["p95_ms"]),
        ("Option 3",         option3_results["latency"]["mean_ms"],  option3_results["latency"]["p95_ms"]),
        ("Option 4",         option4_results["latency"]["mean_ms"],  option4_results["latency"]["p95_ms"]),
        ("sep",              sep_results["latency"]["mean_ms"],      sep_results["latency"]["p95_ms"]),
        ("hop",              hop_results["latency"]["mean_ms"],      hop_results["latency"]["p95_ms"]),
        ("Plan 7 / fin",     fin_results["latency"]["mean_ms"],      fin_results["latency"]["p95_ms"]),
    ]
    fastest = min(all_named, key=lambda x: x[1])
    slowest = max(all_named, key=lambda x: x[1])
    plan1   = all_named[0]
    plan7   = all_named[-1]

    print(f"\n{'â”€'*55}")
    print(f"  ì‘ë‹µì‹œê°„ ìš”ì•½ (mean / P95)")
    print(f"{'â”€'*55}")
    print(f"  Plan 1 (í˜„ì¬)   : {plan1[1]:>7.0f}ms  /  P95 {plan1[2]:>7.0f}ms")
    print(f"  ìµœì € ì‹œê°„ Plan  : {fastest[1]:>7.0f}ms  /  P95 {fastest[2]:>7.0f}ms  â† {fastest[0]}")
    print(f"  ìµœê³  ì‹œê°„ Plan  : {slowest[1]:>7.0f}ms  /  P95 {slowest[2]:>7.0f}ms  â† {slowest[0]}")
    print(f"  Plan 7 (fin)    : {plan7[1]:>7.0f}ms  /  P95 {plan7[2]:>7.0f}ms")
    diff = plan7[1] - plan1[1]
    sign = "+" if diff >= 0 else ""
    print(f"{'â”€'*55}")
    print(f"  Plan 7 - Plan 1 : {sign}{diff:.0f}ms  ({'ëŠë¦¼' if diff > 0 else 'ë¹ ë¦„'})")
    
    # ìµœê³  ì„±ëŠ¥ í‘œì‹œ
    systems = {
        "í˜„ì¬ (Plan ê¸°ë°˜)": current_results["average"],
        "Option 2 (ì„ê³„ê°’ ê¸°ë°˜)": option2_results["average"],
        "Option 3 (Intent ê¸°ë°˜)": option3_results["average"],
        "Option 4 (ì™„ì „ ë³‘ë ¬ + í‚¤ì›Œë“œ)": option4_results["average"],
        "sep (Plan + í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¥˜ + ë™ì˜ì–´ ì„œì¹˜)": sep_results["average"],
        "hop (ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜ ì ìš©)": hop_results["average"],
        "fin (HOP êµ¬ì¡° + PG canonical_name â†’ Neo4j ë³´ê°•)": fin_results["average"],
    }

    best_name = max(systems, key=lambda name: (
        systems[name]["mrr"],
        systems[name]["ndcg@10"],
        systems[name]["ndcg@5"],
        systems[name]["precision@5"],
        systems[name]["recall@10"]
    ))

    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_name}")

    # ë¹„êµ ê²°ê³¼ ì €ì¥
    output_dir = ROOT_DIR / "training/data/output_data"
    output_file_path = output_dir / "evaluation_report.json"

    output_dir.mkdir(parents=True, exist_ok=True)

    comparison_data = {
        "metrics": metrics,
        "current": {**current_results["average"], "latency": current_results["latency"]},
        "option2": {**option2_results["average"], "latency": option2_results["latency"]},
        "option3": {**option3_results["average"], "latency": option3_results["latency"]},
        "option4": {**option4_results["average"], "latency": option4_results["latency"]},
        "sep":     {**sep_results["average"],     "latency": sep_results["latency"]},
        "hop":     {**hop_results["average"],     "latency": hop_results["latency"]},
        "fin":     {**fin_results["average"],     "latency": fin_results["latency"]},
    }
    
    with open(output_file_path, "w", encoding="utf-8") as f:
        json.dump(comparison_data, f, indent=4, ensure_ascii=False)
    print("\nâœ… í‰ê°€ ë³´ê³ ì„œê°€ 'evaluation_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ê²€ìƒ‰ ì‹œìŠ¤í…œ í‰ê°€")
    parser.add_argument("--mode", choices=["single", "compare"], default="single",
                        help="í‰ê°€ ëª¨ë“œ (single: ë‹¨ì¼ í‰ê°€, compare: ì‹œìŠ¤í…œ ë¹„êµ)")
    parser.add_argument("--option", type=int, choices=[0, 2, 3, 4, 5, 6, 7], default=0,
                        help="ê²€ìƒ‰ ì˜µì…˜ (0: í˜„ì¬(Plan), 2: ì„ê³„ê°’, 3: Intent, 4: ì™„ì „ë³‘ë ¬, 5: í‚¤ì›Œë“œ ë¬¸ì¥ ë¶„ë¦¬, 6: ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜, 7: HOP êµ¬ì¡° + PG canonical_name â†’ Neo4j ë³´ê°•)")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    if args.mode == "compare":
        asyncio.run(compare_systems())
    else:
        asyncio.run(evaluate_search_system(verbose=args.verbose, option=args.option))
