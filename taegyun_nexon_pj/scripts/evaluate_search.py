"""
ê²€ìƒ‰ ì‹œìŠ¤í…œ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

MRR, nDCG@K, Precision@K, Recall@K ê³„ì‚°
"""
import sys
import json
import asyncio
import numpy as np
import importlib
from pathlib import Path
from typing import List, Dict, Any


# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
ROOT_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(ROOT_DIR / "langchain_app"))

from database.session import get_async_db

def calculate_mrr(results: List[Dict], ground_truth: List[str]) -> float:
    """
    MRR (Mean Reciprocal Rank) ê³„ì‚°
    
    ì²« ë²ˆì§¸ ì •ë‹µì˜ ìˆœìœ„ ì—­ìˆ˜
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        ground_truth: ì •ë‹µ ì—”í‹°í‹° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        
    Returns:
        MRR ì ìˆ˜ (0~1)
    """
    for rank, result in enumerate(results, start=1):
        name = result.get("data", {}).get("canonical_name", "")
        if name in ground_truth:
            return 1.0 / rank
    return 0.0


def calculate_ndcg(
    results: List[Dict],
    relevance: Dict[str, int],
    k: int = 10
) -> float:
    """
    nDCG@K (Normalized Discounted Cumulative Gain) ê³„ì‚°
    
    ìˆœìœ„ë¥¼ ê³ ë ¤í•œ ì •í™•ë„
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        relevance: ì—”í‹°í‹°ë³„ ê´€ë ¨ë„ {ì´ë¦„: ì ìˆ˜(0-3)}
        k: ìƒìœ„ Kê°œê¹Œì§€ í‰ê°€
        
    Returns:
        nDCG ì ìˆ˜ (0~1)
    """
    # DCG ê³„ì‚°
    dcg = 0.0
    for rank, result in enumerate(results[:k], start=1):
        name = result.get("data", {}).get("canonical_name", "")
        rel = relevance.get(name, 0)
        if rel > 0:
            dcg += rel / np.log2(rank + 1)
    
    # IDCG ê³„ì‚° (ì´ìƒì  ìˆœì„œ)
    ideal_rels = sorted(relevance.values(), reverse=True)
    idcg = sum(rel / np.log2(rank + 1) for rank, rel in enumerate(ideal_rels[:k], start=1) if rel > 0)
    
    return dcg / idcg if idcg > 0 else 0.0


def calculate_precision_at_k(
    results: List[Dict],
    ground_truth: List[str],
    k: int = 5
) -> float:
    """
    Precision@K ê³„ì‚°
    
    ìƒìœ„ Kê°œ ì¤‘ ì •ë‹µ ë¹„ìœ¨
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        ground_truth: ì •ë‹µ ì—”í‹°í‹° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ Kê°œ
        
    Returns:
        Precision ì ìˆ˜ (0~1)
    """
    top_k = [r.get("data", {}).get("canonical_name", "") for r in results[:k]]
    hits = len(set(top_k) & set(ground_truth))
    return hits / k if k > 0 else 0.0


def calculate_recall_at_k(
    results: List[Dict],
    ground_truth: List[str],
    k: int = 10
) -> float:
    """
    Recall@K ê³„ì‚°
    
    ì „ì²´ ì •ë‹µ ì¤‘ ìƒìœ„ Kê°œì— í¬í•¨ëœ ë¹„ìœ¨
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        ground_truth: ì •ë‹µ ì—”í‹°í‹° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        k: ìƒìœ„ Kê°œ
        
    Returns:
        Recall ì ìˆ˜ (0~1)
    """
    top_k = [r.get("data", {}).get("canonical_name", "") for r in results[:k]]
    hits = len(set(top_k) & set(ground_truth))
    return hits / len(ground_truth) if ground_truth else 0.0


async def evaluate_single_query(
    searcher: "HybridSearcher",
    test_case: Dict[str, Any],
    verbose: bool = False
) -> Dict[str, float]:
    """
    ë‹¨ì¼ ì§ˆë¬¸ í‰ê°€
    
    Args:
        searcher: HybridSearcher ì¸ìŠ¤í„´ìŠ¤
        test_case: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        
    Returns:
        í‰ê°€ ë©”íŠ¸ë¦­
    """
    query = test_case["query"]
    ground_truth = test_case["ground_truth"]
    relevance = test_case["relevance"]
    print(f"relevance:{relevance}")
    print(f"ground_truth:{ground_truth}")
    # ê²€ìƒ‰ ì‹¤í–‰
    try:
        print("evaluate_search.evaluate_single_query í˜¸ì¶œ ë¨")
        results = await searcher.search(query, limit=10)
        print(f"results: {results}")
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise
    # results = await searcher.search(query, limit=10)
    # print(f"results: {results}")

    # ë©”íŠ¸ë¦­ ê³„ì‚°
    # try:
    #     mrr = calculate_mrr(results, ground_truth)
    #     ndcg_10 = calculate_ndcg(results, relevance, k=10)
    #     ndcg_5 = calculate_ndcg(results, relevance, k=5)
    #     precision_5 = calculate_precision_at_k(results, ground_truth, k=5)
    #     recall_10 = calculate_recall_at_k(results, ground_truth, k=10)
    # except Exception as e:
    #     import traceback
    #     traceback.print_exc()
    #     raise
    mrr = calculate_mrr(results, ground_truth)
    ndcg_10 = calculate_ndcg(results, relevance, k=10)
    ndcg_5 = calculate_ndcg(results, relevance, k=5)
    precision_5 = calculate_precision_at_k(results, ground_truth, k=5)
    recall_10 = calculate_recall_at_k(results, ground_truth, k=10)
    
    metrics = {
        "mrr": mrr,
        "ndcg@10": ndcg_10,
        "ndcg@5": ndcg_5,
        "precision@5": precision_5,
        "recall@10": recall_10
    }
    
    if verbose:
        print(f"\nì§ˆë¬¸: {query}")
        print(f"ì •ë‹µ: {ground_truth}")
        print(f"ê²°ê³¼ ({len(results)}ê°œ):")
        for i, r in enumerate(results[:5], 1):
            name = r.get("data", {}).get("canonical_name", "Unknown")
            score = r.get("score", 0)
            is_correct = "âœ…" if name in ground_truth else "  "
            print(f"  {i}. {name} ({score:.2f}) {is_correct}")
        print(f"ë©”íŠ¸ë¦­: MRR={mrr:.3f}, nDCG@10={ndcg_10:.3f}, P@5={precision_5:.3f}")
    
    return metrics


async def evaluate_search_system(
    test_file: str = "training/data/test/search_test_queries.json",
    verbose: bool = True,
    option: int = 0
) -> Dict[str, Any]:
    """
    ê²€ìƒ‰ ì‹œìŠ¤í…œ ì „ì²´ í‰ê°€
    
    Args:
        test_file: í…ŒìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        option: 0=í˜„ì¬(Plan), 2=ì„ê³„ê°’, 3=Intent, 4=ì™„ì „ë³‘ë ¬
        
    Returns:
        í‰ê°€ ê²°ê³¼
    """
    import importlib

    module_configs = {
        0: {"path": "src.retrievers.hybrid_searcher", "name": "í˜„ì¬ (Plan ê¸°ë°˜)"},
        2: {"path": "src.retrievers.hybrid_searcher_option2", "name": "Option 2 (ì„ê³„ê°’ ê¸°ë°˜)"},
        3: {"path": "src.retrievers.hybrid_searcher_option3", "name": "Option 3 (Intent ê¸°ë°˜)"},
        4: {"path": "src.retrievers.hybrid_searcher_option4", "name": "Option 4 (ì™„ì „ ë³‘ë ¬ + í‚¤ì›Œë“œ)"},
        5: {"path": "src.retrievers.hybrid_searcher_sep", "name": "sep (Plan + í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¥˜ + ë™ì˜ì–´ ì„œì¹˜)"},
        6: {"path": "src.retrievers.hybrid_searcher_hop", "name": "hop (ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜ ì ìš©)"}
    }
    
    config = module_configs.get(option, module_configs[0])
    option_name = config["name"]

    # 2. ë™ì  ì„í¬íŠ¸ ì‹¤í–‰ (ë¹¨ê°„ ì¤„/ìºì‹± ë°©ì§€ í•µì‹¬)
    # import_moduleì€ "src.retrievers.hybrid_searcher" ê°™ì€ ë¬¸ìì—´ì„ ë°›ìŒ
    module = importlib.import_module(config["path"])
    importlib.reload(module)  # í˜¹ì‹œ ëª¨ë¥¼ ìºì‹œ ë°©ì§€
    
    # í•´ë‹¹ íŒŒì¼ ì•ˆì—ì„œ 'HybridSearcher'ë¼ëŠ” í´ë˜ìŠ¤ë¥¼ êº¼ë‚´ì˜´
    TargetSearcherClass = getattr(module, "HybridSearcher")

    # 3. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ
    test_path = ROOT_DIR / test_file
    with open(test_path, "r", encoding="utf-8") as f:
        test_cases = json.load(f)
    
    print(f"{'='*60}")
    print(f"ê²€ìƒ‰ ì‹œìŠ¤í…œ í‰ê°€ ì‹œì‘: {option_name}")
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
    print(f"{'='*60}")
    
    all_metrics = []
    
    # 4. DB ì„¸ì…˜ ì•ˆì—ì„œ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜¨ í´ë˜ìŠ¤ë¡œ ê°ì²´ ìƒì„±
    async for session in get_async_db():
        searcher = TargetSearcherClass(
            db=session,
            use_milvus=True,
            use_neo4j=True,
            use_router=True,
            verbose=False
        )
        print(f"DEBUG: [{option_name}] í´ë˜ìŠ¤ ìœ„ì¹˜ -> {searcher.__class__.__module__}")
        
        for i, test_case in enumerate(test_cases, 1):
            if verbose: print(f"\n[{i}/{len(test_cases)}] ", end="")
            try:
                metrics = await evaluate_single_query(searcher, test_case, verbose=verbose)
                all_metrics.append(metrics)
            except Exception as e:
                print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}")
                all_metrics.append({"mrr": 0.0, "ndcg@10": 0.0, "ndcg@5": 0.0, "precision@5": 0.0, "recall@10": 0.0})
        break
    
    if not all_metrics:
        print("âŒ í‰ê°€ëœ ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
        return {}

    # í‰ê·  ê³„ì‚°
    avg_metrics = {
        metric: np.mean([m[metric] for m in all_metrics])
        for metric in all_metrics[0].keys()
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"MRR (Mean Reciprocal Rank):      {avg_metrics['mrr']:.4f}")
    print(f"nDCG@10:                          {avg_metrics['ndcg@10']:.4f}")
    print(f"nDCG@5:                           {avg_metrics['ndcg@5']:.4f}")
    print(f"Precision@5:                      {avg_metrics['precision@5']:.4f}")
    print(f"Recall@10:                        {avg_metrics['recall@10']:.4f}")
    print(f"{'='*60}")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
    category_metrics = {}
    for test_case, metrics in zip(test_cases, all_metrics):
        category = test_case["category"]
        if category not in category_metrics:
            category_metrics[category] = []
        category_metrics[category].append(metrics["mrr"])
    
    print(f"\nì¹´í…Œê³ ë¦¬ë³„ MRR:")
    for category, mrrs in sorted(category_metrics.items()):
        avg_mrr = np.mean(mrrs)
        print(f"  {category:20s}: {avg_mrr:.4f}")
    
    return {
        "average": avg_metrics,
        "individual": all_metrics,
        "by_category": category_metrics
    }


async def compare_systems():
    """
    ì—¬ëŸ¬ ì‹œìŠ¤í…œ ë¹„êµ (í˜„ì¬ vs Option 2 vs Option 3 vs Option 4 vs sep vs hop)
    """
    print("\n" + "="*85)
    print("ê²€ìƒ‰ ì‹œìŠ¤í…œ ë¹„êµ: í˜„ì¬(Plan) vs Option 2(ì„ê³„ê°’) vs Option 3(Intent) vs Option 4(ì™„ì „ë³‘ë ¬) vs sep(Plan + í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¥˜ + ë™ì˜ì–´ ì„œì¹˜) vs hop(ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜ ì ìš©)")
    print("="*85)
    
    # í˜„ì¬ (Plan) í‰ê°€
    print("\n[1/4] í˜„ì¬ ì‹œìŠ¤í…œ (Plan ê¸°ë°˜) í‰ê°€ ì¤‘...")
    current_results = await evaluate_search_system(verbose=False, option=0)
    
    # Option 2 í‰ê°€
    print("\n[2/4] Option 2 (ì„ê³„ê°’ ê¸°ë°˜) í‰ê°€ ì¤‘...")
    option2_results = await evaluate_search_system(verbose=False, option=2)
    
    # Option 3 í‰ê°€
    print("\n[3/4] Option 3 (Intent ê¸°ë°˜) í‰ê°€ ì¤‘...")
    option3_results = await evaluate_search_system(verbose=False, option=3)
    
    # Option 4 í‰ê°€
    print("\n[4/4] Option 4 (ì™„ì „ ë³‘ë ¬ + í‚¤ì›Œë“œ) í‰ê°€ ì¤‘...")
    option4_results = await evaluate_search_system(verbose=False, option=4)

    # sep í‰ê°€
    print("\n[5/6] Option 5 (Plan + í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¥˜ + ë™ì˜ì–´ ì„œì¹˜) í‰ê°€ ì¤‘...")
    sep_results = await evaluate_search_system(verbose=False, option=5)

    # hop í‰ê°€
    print("\n[6/6] Option 6 (ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜ ì ìš©) í‰ê°€ ì¤‘...")
    hop_results = await evaluate_search_system(verbose=False, option=6)


    
    # ë¹„êµí‘œ ì¶œë ¥
    print(f"\n{'='*85}")
    print(f"ë¹„êµ ê²°ê³¼")
    print(f"{'='*85}")
    print(f"{'ë©”íŠ¸ë¦­':<15s} {'í˜„ì¬(Plan)':<15s} {'Option 2':<15s} {'Option 3':<15s} {'Option 4':<15s} {'sep':<15s} {'hop':<15s}")
    print(f"{'-'*85}")
    
    metrics = ["mrr", "ndcg@10", "ndcg@5", "precision@5", "recall@10"]
    for metric in metrics:
        current_val = current_results["average"][metric]
        opt2_val = option2_results["average"][metric]
        opt3_val = option3_results["average"][metric]
        opt4_val = option4_results["average"][metric]
        sep_val = sep_results["average"][metric]
        hop_val = hop_results["average"][metric]
        print(f"{metric:<15s} {current_val:<15.4f} {opt2_val:<15.4f} {opt3_val:<15.4f} {opt4_val:<15.4f} {sep_val:<15.4f} {hop_val:<15.4f}")
    
    print(f"{'='*85}")
    
    # ìµœê³  ì„±ëŠ¥ í‘œì‹œ
    best_mrr = max(current_results["average"]["mrr"], 
                   option2_results["average"]["mrr"],
                   option3_results["average"]["mrr"],
                   option4_results["average"]["mrr"],
                   sep_results["average"]["mrr"],
                   hop_results["average"]["mrr"])
    
    if best_mrr == current_results["average"]["mrr"]:
        print("\nğŸ† ìµœê³  ì„±ëŠ¥: í˜„ì¬ (Plan ê¸°ë°˜)")
    elif best_mrr == option2_results["average"]["mrr"]:
        print("\nğŸ† ìµœê³  ì„±ëŠ¥: Option 2 (ì„ê³„ê°’ ê¸°ë°˜)")
    elif best_mrr == option3_results["average"]["mrr"]:
        print("\nğŸ† ìµœê³  ì„±ëŠ¥: Option 3 (Intent ê¸°ë°˜)")
    elif best_mrr == option4_results["average"]["mrr"]:
        print("\nğŸ† ìµœê³  ì„±ëŠ¥: Option 4 (ì™„ì „ ë³‘ë ¬ + í‚¤ì›Œë“œ)")
    elif best_mrr == sep_results["average"]["mrr"]:
        print("\nğŸ† ìµœê³  ì„±ëŠ¥: sep (Plan + í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¥˜ + ë™ì˜ì–´ ì„œì¹˜)")
    else:
        print("\nğŸ† ìµœê³  ì„±ëŠ¥: hop (ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜ ì ìš©)")

    # ë¹„êµ ê²°ê³¼ ì €ì¥
    output_dir = ROOT_DIR / "training/data/output_data"
    output_file_path = output_dir / "evaluation_report.json"

    output_dir.mkdir(parents=True, exist_ok=True)

    comparison_data = {
        "metrics": metrics,
        "current": current_results["average"],
        "option2": option2_results["average"],
        "option3": option3_results["average"],
        "option4": option4_results["average"],
        "sep": sep_results["average"],
        "hop": hop_results["average"]
    }
    
    with open(output_file_path, "w", encoding="utf-8") as f:
        json.dump(comparison_data, f, indent=4, ensure_ascii=False)
    print("\nâœ… í‰ê°€ ë³´ê³ ì„œê°€ 'evaluation_report.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ê²€ìƒ‰ ì‹œìŠ¤í…œ í‰ê°€")
    parser.add_argument("--mode", choices=["single", "compare"], default="single",
                        help="í‰ê°€ ëª¨ë“œ (single: ë‹¨ì¼ í‰ê°€, compare: ì‹œìŠ¤í…œ ë¹„êµ)")
    parser.add_argument("--option", type=int, choices=[0, 2, 3, 4, 5, 6], default=0,
                        help="ê²€ìƒ‰ ì˜µì…˜ (0: í˜„ì¬(Plan), 2: ì„ê³„ê°’, 3: Intent, 4: ì™„ì „ë³‘ë ¬, 5: í‚¤ì›Œë“œ, ë¬¸ì¥ ë¶„ë¦¬, 6: ì¿¼ë¦¬ ê¹Šì´ ë¶„ë¥˜)")
    parser.add_argument("--verbose", action="store_true", help="ìƒì„¸ ì¶œë ¥")
    
    args = parser.parse_args()
    
    if args.mode == "compare":
        asyncio.run(compare_systems())
    else:
        asyncio.run(evaluate_search_system(verbose=args.verbose, option=args.option))
