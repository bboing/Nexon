"""
Hybrid Search with HOP-based Routing (Async) - HOP STRATEGY
Router Agent â†’ HOP ê¹Šì´ íŒë‹¨ â†’ ì„ íƒì  DB ì‚¬ìš©
+ HOP-1: Postgres + Milvus (ì§ì ‘ ê´€ê³„)
+ HOP-2+: Postgres + Milvus + Neo4j (ì²´ì¸ ê´€ê³„)
+ Entity/Sentence ë¶„ë¦¬ â†’ DBë³„ ìµœì  ì¿¼ë¦¬
+ Synonym Resolution â†’ PostgreSQL ê°„ì ‘ ë§¤ì¹­
+ Jina Reranker â†’ RRF í›„ ë…¸ì´ì¦ˆ ì œê±°
"""
from typing import List, Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
import asyncio
import logging
import re
import requests
import os

from database.models.maple_dictionary import MapleDictionary
from src.retrievers.db_searcher import MapleDBSearcher
from src.retrievers.milvus_retriever import MilvusRetriever
from src.retrievers.neo4j_searcher import Neo4jSearcher
from src.agents.router_agent_hop import RouterAgent
from src.utils.keyword_extractor import MapleKeywordExtractor
from config.settings import settings

logger = logging.getLogger(__name__)


class HybridSearcher:
    """
    âœ… HOP-based Hybrid Search (ê´€ê³„ ê¹Šì´ ê¸°ë°˜ ê²€ìƒ‰)
    
    í•µì‹¬ ì•„ì´ë””ì–´:
    1. ê´€ê³„ ê¹Šì´(Hop)ì— ë”°ë¼ DB ì„ íƒ
       - hop=1: Postgres + Milvus (ì§ì ‘ ê´€ê³„)
       - hop=2+: Postgres + Milvus + Neo4j (ì²´ì¸ ê´€ê³„)
    
    2. Entity/Sentence ë¶„ë¦¬
       - Entity(ëª…ì‚¬) â†’ Postgres (canonical_name + synonym)
       - Sentence(ë™ì‚¬êµ¬) â†’ Milvus (ì˜ë¯¸ ê²€ìƒ‰)
    
    3. ë¬´ì¡°ê±´ ë³‘ë ¬ ì‹¤í–‰
       - Postgres + Milvus í•­ìƒ ë³‘ë ¬
       - hop >= 2ë©´ Neo4j ì¶”ê°€
    
    ê²€ìƒ‰ íë¦„:
    1. Router â†’ hop, entities, sentences ì¶”ì¶œ
    2. Postgres(entities) + Milvus(sentences) ë³‘ë ¬ ì‹¤í–‰
    3. hop >= 2 â†’ Neo4j ê´€ê³„ ê²€ìƒ‰ ì¶”ê°€
    4. RRF ë³‘í•©
    5. Reranker ì¬ì •ë ¬
    6. ìƒìœ„ Nê°œ ë°˜í™˜
    
    ì˜ˆì‹œ (hop=1):
    - ì§ˆë¬¸: "ë¬¼ì•½ íŒŒëŠ” ì‚¬ëŒ ëˆ„êµ¬ì•¼?"
    - hop: 1 (ITEM-NPC ì§ì ‘ ê´€ê³„)
    - entities: []
    - sentences: ["ë¬¼ì•½ íŒŒëŠ” ì‚¬ëŒ"]
    - ì‹¤í–‰: Milvusë§Œ â†’ "ë¯¸ë‚˜" ë°œê²¬
    
    ì˜ˆì‹œ (hop=2):
    - ì§ˆë¬¸: "ì•„ì´ìŠ¤ì§„ ì–»ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ?"
    - hop: 2 (ITEM-MONSTER-MAP ì²´ì¸)
    - entities: ["ì•„ì´ìŠ¤ì§„"]
    - sentences: []
    - ì‹¤í–‰: Postgres + Neo4j â†’ "ìŠ¤í¬ì•„" â†’ "íê´‘" ë°œê²¬
    """
    
    def __init__(
        self, 
        db: AsyncSession,
        use_milvus: bool = True,
        use_neo4j: bool = True,
        use_router: bool = True,
        verbose: bool = False
    ):
        self.db = db
        self.use_milvus = use_milvus
        self.use_neo4j = use_neo4j
        self.use_router = use_router
        self.verbose = verbose
        
        # PostgreSQL Searcher
        self.pg_searcher = MapleDBSearcher(db)
        
        # Milvus Searcher (ì˜µì…˜)
        self.milvus_searcher = None
        if use_milvus:
            try:
                self.milvus_searcher = MilvusRetriever()
                logger.info("âœ… Milvus ê²€ìƒ‰ í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Milvus ì—°ê²° ì‹¤íŒ¨, PostgreSQLë§Œ ì‚¬ìš©: {e}")
                self.use_milvus = False
        
        # Neo4j Searcher (ì˜µì…˜)
        self.neo4j_searcher = None
        if use_neo4j:
            try:
                self.neo4j_searcher = Neo4jSearcher()
                logger.info("âœ… Neo4j ê²€ìƒ‰ í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Neo4j ì—°ê²° ì‹¤íŒ¨: {e}")
                self.use_neo4j = False
        
        # Router Agent (ì˜µì…˜)
        self.router = None
        if use_router:
            try:
                self.router = RouterAgent(verbose=False)
                logger.info("âœ… Router Agent í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Router Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_router = False
        
        # Keyword Extractor (Kiwi í˜•íƒœì†Œ ë¶„ì„ + ë™ì˜ì–´ ì¹˜í™˜)
        try:
            self.keyword_extractor = MapleKeywordExtractor(db)
            logger.info("âœ… Kiwi Keyword Extractor í™œì„±í™”")
        except Exception as e:
            logger.warning(f"âš ï¸ Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {e}")
            self.keyword_extractor = None
    
    async def search(
        self,
        query: str,
        category: Optional[str] = None,
        limit: int = 10,
        pg_threshold: int = 3,
        use_plan_execution: bool = True
    ) -> List[Dict[str, Any]]:
        """
        âœ… HOP ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
        
        ì „ëµ:
        1. Router â†’ hop, entities, sentences ì¶”ì¶œ
        2. Postgres (entities) + Milvus (sentences) ë³‘ë ¬ ì‹¤í–‰
        3. hop >= 2ë©´ Neo4j ì¶”ê°€
        4. RRF ë³‘í•©
        5. Reranker ì¬ì •ë ¬
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ìˆœ ì •ë ¬)
        """
        print("hybrid_searcher_hop.search í˜¸ì¶œ ë¨")
        if self.verbose:
            print(f"\nğŸ” Hybrid Search (HOP): '{query}'")
        
        # Step 1: Routerë¡œ hop, entities, sentences ì¶”ì¶œ
        router_result = None
        hop = 1  # ê¸°ë³¸ê°’
        entities = []
        sentences = []
        
        if self.use_router and self.router:
            try:
                router_result = await self.router.route(query)
                hop = router_result.get("hop", 1)
                entities = router_result.get("entities", [])
                print(f"entities: {entities}")
                sentences = router_result.get("sentences", [])
                print(f"sentences: {sentences}")

                if self.verbose:
                    print(f"   ğŸ§­ Hop: {hop}")
                    print(f"   ğŸ“Œ Entities: {entities}")
                    print(f"   ğŸ“ Sentences: {sentences}")
                    
            except Exception as e:
                logger.warning(f"Router ì‹¤íŒ¨, ìì²´ í‚¤ì›Œë“œ ì¶”ì¶œ: {e}")
                # Fallback: ìì²´ í‚¤ì›Œë“œ ì¶”ì¶œ
                raw_keywords = await self._extract_keywords(query)
                structured = self._reconstruct_ngrams(raw_keywords, query)
                entities = structured["entities"]
                sentences = structured["sentences"]
        
        # Step 2: Postgres + Milvus ë³‘ë ¬ ì‹¤í–‰ (ë¬´ì¡°ê±´)
        results_by_source = {
            "PostgreSQL": [],
            "Milvus": [],
            "Neo4j": []
        }
        
        # ë³‘ë ¬ ì‹¤í–‰
        async def empty(): return []            # sentence ê°’ ì—†ìœ¼ë©´ [] return / ì½”ë£¨í‹´ ê°ì²´ ë°˜í™˜í•´ì•¼ í•¨. ë¦¬ìŠ¤íŠ¸ ê°ì²´ë©´ awaitableì´ ì•„ë‹˜. asyncti.gatherëŠ” ì•½ì†ëœ

        pg_task = self._search_postgres_with_synonym(entities, limit_per_entity=5) if entities else empty()
        milvus_task = self._search_milvus_sentences(sentences) if sentences and self.use_milvus else empty()

        pg_results, milvus_results = await asyncio.gather(pg_task, milvus_task)
        
        # sources í•„ë“œ ì¶”ê°€
        if isinstance(pg_results, list):
            for result in pg_results:
                if "sources" not in result:
                    result["sources"] = ["PostgreSQL"]
            results_by_source["PostgreSQL"] = pg_results
        
        if isinstance(milvus_results, list):
            for result in milvus_results:
                if "sources" not in result:
                    result["sources"] = ["Milvus"]
            results_by_source["Milvus"] = milvus_results
        
        if self.verbose:
            print(f"   PostgreSQL: {len(pg_results) if isinstance(pg_results, list) else 0}ê°œ")
            print(f"   Milvus: {len(milvus_results) if isinstance(milvus_results, list) else 0}ê°œ")
        
        # Step 3: hop >= 2ë©´ Neo4j ì¶”ê°€
        if hop >= 2 and self.use_neo4j and self.neo4j_searcher:
            if self.verbose:
                print(f"   ğŸ”— Hop={hop} â†’ Neo4j ê´€ê³„ ê²€ìƒ‰")
            
            neo4j_results = await self._search_neo4j_relations(query, entities, router_result)
            results_by_source["Neo4j"] = neo4j_results
            
            if self.verbose:
                print(f"   Neo4j: {len(neo4j_results)}ê°œ")
        
        # Step 4: RRF ë³‘í•©
        rrf_results = self._apply_rrf(results_by_source)
        
        if self.verbose:
            print(f"   âœ… RRF ì™„ë£Œ: {len(rrf_results)}ê°œ")
        
        # Step 5: Reranker (ê²°ê³¼ > limitì¼ ë•Œ)
        if len(rrf_results) > limit:
            rrf_results = await self._rerank_with_jina(query, rrf_results, top_n=limit)
            
            if self.verbose:
                print(f"   âœ… Reranker ì™„ë£Œ: {len(rrf_results)}ê°œ")
        
        if self.verbose:
            print(f"   ğŸ“Š ìµœì¢…: {len(rrf_results[:limit])}ê°œ\n")
        
        return rrf_results[:limit]
    
    
    def _format_graph_results(
        self,
        graph_results: List[Dict[str, Any]],
        match_type: str
    ) -> List[Dict[str, Any]]:
        """
        Neo4j ê²°ê³¼ë¥¼ í†µí•© í¬ë§·ìœ¼ë¡œ ë³€í™˜
        """
        formatted_results = []
        
        for result in graph_results:
            # Neo4j ê²°ê³¼ì—ì„œ ì´ë¦„ ì¶”ì¶œ (ë‹¤ì–‘í•œ í•„ë“œëª… ì§€ì›)
            name = (
                result.get("npc_name") or 
                result.get("map_name") or 
                result.get("monster_name") or 
                result.get("item_name") or 
                result.get("name", "Unknown")
            )
            
            # ID ì¶”ì¶œ
            entity_id = (
                result.get("npc_id") or 
                result.get("map_id") or 
                result.get("monster_id") or 
                result.get("item_id") or 
                result.get("id", "")
            )
            
            formatted_results.append({
                "score": 85.0,  # Graph ê´€ê³„ëŠ” ë†’ì€ ì‹ ë¢°ë„
                "match_type": match_type,
                "sources": ["Neo4j"],
                "data": {
                    "id": entity_id,
                    "canonical_name": name,
                    "category": "RELATION",  # ê´€ê³„ ê²€ìƒ‰ ê²°ê³¼
                    "description": f"{result.get('relation_type', '')} ê´€ê³„",
                    "relation_info": result  # ì›ë³¸ ê´€ê³„ ì •ë³´ ë³´ì¡´
                }
            })
        
        return formatted_results
    
    async def _enrich_graph_results(self, graph_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Neo4j ê²€ìƒ‰ ê²°ê³¼ë¥¼ PostgreSQL detail_dataë¡œ ë³´ì¶© (async)
        
        Neo4jëŠ” ê´€ê³„ë§Œ ì•Œë ¤ì£¼ê³ , ì‹¤ì œ ìƒì„¸ ì •ë³´ëŠ” PostgreSQLì— ìˆìŒ
        ì˜ˆ: Neo4j â†’ "ìŠ¤í¬ì•„"ë¼ëŠ” ì´ë¦„ë§Œ
            PostgreSQL â†’ ìŠ¤í¬ì•„ì˜ level, spawn_maps, drops ë“± ìƒì„¸ ì •ë³´
        
        Args:
            graph_results: Neo4j ê²€ìƒ‰ ê²°ê³¼
            
        Returns:
            detail_dataê°€ ì¶”ê°€ëœ ê²°ê³¼
        """
        enriched = []
        
        for result in graph_results:
            data = result.get("data", {})
            canonical_name = data.get("canonical_name")
            
            if not canonical_name or canonical_name == "Unknown":
                enriched.append(result)
                continue
            
            try:
                # PostgreSQLì—ì„œ ì „ì²´ ì •ë³´ ì¡°íšŒ (async)
                stmt = select(MapleDictionary).where(
                    MapleDictionary.canonical_name == canonical_name
                )
                db_result = await self.db.execute(stmt)
                pg_entity = db_result.scalar_one_or_none()
                
                if pg_entity:
                    # PostgreSQL ë°ì´í„°ë¡œ êµì²´ (ê´€ê³„ ì •ë³´ëŠ” ìœ ì§€)
                    relation_info = data.get("relation_info")
                    result["data"] = pg_entity.to_dict()
                    
                    # ê´€ê³„ ì •ë³´ ì¶”ê°€
                    if relation_info:
                        result["data"]["relation_info"] = relation_info
                    
                    # Category ì—…ë°ì´íŠ¸
                    result["data"]["category"] = str(pg_entity.category).split('.')[-1]
                    
                enriched.append(result)
                
            except Exception as e:
                logger.warning(f"Enrichment ì‹¤íŒ¨ ({canonical_name}): {e}")
                enriched.append(result)
        
        return enriched
    
    async def _extract_keywords(self, query: str) -> List[str]:
        """
        ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (Async)
        
        ì „ëµ:
        1. Kiwi í˜•íƒœì†Œ ë¶„ì„ (ìš°ì„ , ì •í™•ë„ ë†’ìŒ)
        2. Fallback: ì •ê·œì‹ + ë¶ˆìš©ì–´ (Kiwi ì‹¤íŒ¨ ì‹œ)
        """
        # Kiwi ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ìš°ì„  ì‚¬ìš©
        if self.keyword_extractor:
            try:
                return await self.keyword_extractor.extract(query)
            except Exception as e:
                logger.warning(f"Kiwi í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, Fallback ì‚¬ìš©: {e}")
        
        # Fallback: ê¸°ì¡´ ì •ê·œì‹ ë°©ì‹
        # ì¡°ì‚¬ (í† í° ëì—ì„œ ì œê±°)
        particles = ["ì´", "ê°€", "ì„", "ë¥¼", "ì€", "ëŠ”", "ë„", "ë§Œ", "ì˜", "ì—", "ì„œ", "ë¡œ", "ì™€", "ê³¼"]
        
        # ì ‘ë¯¸ì‚¬ í˜•íƒœ ë¶ˆìš©ì–´ (ë‹¨ì–´ ëì— ë¶™ëŠ” ê²ƒë“¤)
        suffix_stopwords = [
            "í•˜ë ¤ë©´", "í•´ì•¼", "ê°€ì•¼", "ê°€ë ¤ë©´", "í•´ì•¼í•´", "ê°€ì•¼í•´", "í•˜ë©´", "ë˜ë©´",
            "í•˜ê³ ", "ì‹¶ì–´", "ì‹¶ë‹¤", "ì‹¶ìœ¼ë©´", "ë˜ê³ ", "ì›í•´", "ì›í•˜ë‹¤",
            "í•˜ëŠ”", "ìˆëŠ”", "ì—†ëŠ”", "ê°€ëŠ”", "ì˜¤ëŠ”", "ë‚˜ì˜¤ëŠ”",
            "ì–´ìš”", "ì•„ìš”", "í•´ìš”", "ìŠµë‹ˆë‹¤", "ã…‚ë‹ˆë‹¤",
            "ë‚˜ìš”", "í•˜ë‚˜ìš”", "í• ê¹Œìš”", "ìˆë‚˜ìš”", "ì—†ë‚˜ìš”"
        ]
        
        # ì™„ì „ ì¼ì¹˜ ë¶ˆìš©ì–´
        stopwords = {
            # ì˜ë¬¸ì‚¬
            "ì–´ë””", "ì–´ë””ì„œ", "ì–´ë””ë¡œ", "ì–´ë””ì—", "ì–´ë–»ê²Œ", "ì–´ë–¤", "ë­", "ë¬´ì—‡", "ëˆ„êµ¬",
            "ì–¸ì œ", "ì™œ", "ì–¼ë§ˆ", "ëª‡", "ë¬´ìŠ¨",
            # ë™ì‚¬/í˜•ìš©ì‚¬ (ë‹¨ë…)
            "í•˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "ê°€ë‹¤", "ì˜¤ë‹¤", "ë˜ë‹¤", "ë³´ë‹¤", "ì£¼ë‹¤", "ë°›ë‹¤", "ì¡ë‹¤",
            "ê°€ì•¼", "ì™€ì•¼", "í•´ì•¼", "ìˆì–´ì•¼",  # ì¶”ê°€!
            # ì–´ë¯¸/ì¡°ì‚¬
            "í•´", "ìš”", "ë‚˜", "ë‚˜ìš”", "í•˜ë‚˜ìš”",
            # ì¼ë°˜ì–´
            "ê²ƒ", "ìˆ˜", "ë•Œ", "ê³³", "ì¤‘", "ë“±", "ì´ëŸ°", "ì €ëŸ°", "ê·¸ëŸ°", "ì¢€", "ë”"
        }
        
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        tokens = re.findall(r'[ê°€-í£A-Za-z0-9]+', query)
        
        # 1. ì¡°ì‚¬ ì œê±°
        cleaned_tokens = []
        for token in tokens:
            # 3ê¸€ì ì´ìƒì´ê³  ë§ˆì§€ë§‰ ê¸€ìê°€ ì¡°ì‚¬ë©´ ì œê±°
            if len(token) >= 3 and token[-1] in particles:
                cleaned_tokens.append(token[:-1])  # ë§ˆì§€ë§‰ ê¸€ì ì œê±°
            else:
                cleaned_tokens.append(token)
        
        # 2. ë¶ˆìš©ì–´ ì œê±° (ì™„ì „ ì¼ì¹˜ + ì ‘ë¯¸ì‚¬)
        keywords = []
        for token in cleaned_tokens:
            # ì™„ì „ ì¼ì¹˜ ì²´í¬
            if token in stopwords:
                continue
            
            # ì ‘ë¯¸ì‚¬ ì²´í¬ (ì˜ˆ: "ì „ì§í•˜ë ¤ë©´" â†’ "í•˜ë ¤ë©´" ì ‘ë¯¸ì‚¬ ì œê±°)
            is_stopword = False
            for suffix in suffix_stopwords:
                if len(token) > len(suffix) and token.endswith(suffix):
                    # ì ‘ë¯¸ì‚¬ ì œê±° í›„ ë‚¨ì€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    core_word = token[:-len(suffix)]
                    if len(core_word) >= 2:  # í•µì‹¬ ë‹¨ì–´ê°€ 2ê¸€ì ì´ìƒ
                        keywords.append(core_word)
                    is_stopword = True
                    break
            
            # ë¶ˆìš©ì–´ ì•„ë‹ˆê³  2ê¸€ì ì´ìƒì´ë©´ ì¶”ê°€
            if not is_stopword and len(token) >= 2:
                keywords.append(token)
        
        # 3. ìˆ«ìë§Œ ìˆëŠ” í† í° ì œì™¸
        keywords = [k for k in keywords if not k.isdigit()]
        
        # 4. ì¤‘ë³µ ì œê±°
        keywords = list(dict.fromkeys(keywords))
        
        # 5. ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        if not keywords:
            keywords = [query]
        
        return keywords
    
    def _reconstruct_ngrams(
        self,
        raw_keywords: List[str],
        original_query: str
    ) -> Dict[str, List[str]]:
        """
        âœ… N-gram ì¬êµ¬ì„±: Entity vs Sentence ë¶„ë¥˜
        
        ì „ëµ:
        1. ë‹¨ì¼ ëª…ì‚¬ (NNG, NNP) â†’ Entity (PostgreSQL ê²€ìƒ‰ìš©)
        2. ì—°ì†ëœ ë‹¨ì–´ (ë™ì‚¬ í¬í•¨) â†’ Sentence (Milvus ì˜ë¯¸ ê²€ìƒ‰ìš©)
        
        ì˜ˆì‹œ:
        - Input: ['ë¦¬ìŠ¤í•­êµ¬', 'ë¬¼ì•½', 'íŒŒëŠ”', 'ì‚¬ëŒ']
        - Output: {
            "entities": ['ë¦¬ìŠ¤í•­êµ¬'],           # PGìš©
            "sentences": ['ë¬¼ì•½ íŒŒëŠ” ì‚¬ëŒ']     # Milvusìš©
          }
        
        Args:
            raw_keywords: LLM/Kiwiê°€ ì¶”ì¶œí•œ ì›ë³¸ í‚¤ì›Œë“œ
            original_query: ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            Entityì™€ Sentenceë¡œ ë¶„ë¥˜ëœ ë”•ì…”ë„ˆë¦¬
        """
        entities = []
        sentences = []
        
        # âœ… íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë¶„ë¥˜ (Kiwi ì—†ì–´ë„ ì‘ë™)
        # ë™ì‚¬ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸ (ê°„ì ‘ í‘œí˜„)
        verb_patterns = [
            'íŒŒëŠ”', 'ì‚¬ëŠ”', 'íŒ”', 'ì‚´', 'íŒŒ', 'ì‚¬',
            'ì£¼ëŠ”', 'ë“œëŠ”', 'ë–¨ì–´', 'ë‚˜ì˜¤',
            'ìˆëŠ”', 'ê°€ëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”',
            'í• ', 'ë ', 'ê°ˆ', 'ì˜¬'
        ]
        
        # ì—°ì†ëœ í‚¤ì›Œë“œë¡œ ì›ë¬¸ì— ìˆëŠ” êµ¬ë¬¸ ì°¾ê¸° (N-gram)
        i = 0
        used_indices = set()  # ì´ë¯¸ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¸ë±ìŠ¤
        
        while i < len(raw_keywords):
            if i in used_indices:
                i += 1
                continue
            
            # ë™ì‚¬ íŒ¨í„´ì´ ìˆìœ¼ë©´ Sentence í›„ë³´
            current_has_verb = any(verb in raw_keywords[i] for verb in verb_patterns)
            
            if current_has_verb or (i > 0 and any(verb in raw_keywords[i-1] for verb in verb_patterns)):
                # 2~4ê°œ ë‹¨ì–´ ì¡°í•©í•´ì„œ ì›ë¬¸ì— ìˆëŠ”ì§€ í™•ì¸
                found_sentence = False
                for n in range(min(4, len(raw_keywords) - i), 1, -1):  # 4, 3, 2 ìˆœì„œ
                    phrase = ' '.join(raw_keywords[i:i+n])
                    
                    # ì›ë¬¸ì— ì´ êµ¬ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
                    if phrase in original_query and n >= 2:  # 2ê°œ ì´ìƒë§Œ Sentence
                        sentences.append(phrase)
                        for j in range(i, i+n):
                            used_indices.add(j)
                        i += n
                        found_sentence = True
                        break
                
                if not found_sentence:
                    # Sentence ëª» ë§Œë“¤ë©´ Entityë¡œ
                    if not current_has_verb:  # ë™ì‚¬ ë‹¨ë…ì€ ë²„ë¦¼
                        entities.append(raw_keywords[i])
                    used_indices.add(i)
                    i += 1
            else:
                # ë™ì‚¬ ì—†ìœ¼ë©´ Entity
                entities.append(raw_keywords[i])
                used_indices.add(i)
                i += 1
        
        # Kiwi ìˆìœ¼ë©´ ì¶”ê°€ ì •ì œ (ì˜µì…˜)
        if self.keyword_extractor and hasattr(self.keyword_extractor, 'kiwi'):
            entities, sentences = self._refine_with_kiwi(
                entities, sentences, raw_keywords, original_query
            )
        
        # ì¤‘ë³µ ì œê±°
        entities = list(dict.fromkeys(entities))
        sentences = list(dict.fromkeys(sentences))
        
        if self.verbose:
            print(f"         ğŸ”„ N-gram ì¬êµ¬ì„±:")
            print(f"            Entities (PG): {entities}")
            print(f"            Sentences (Milvus): {sentences}")
        
        return {
            "entities": entities,
            "sentences": sentences
        }
    
    def _refine_with_kiwi(
        self,
        entities: List[str],
        sentences: List[str],
        raw_keywords: List[str],
        original_query: str
    ) -> tuple:
        """âœ… NEW: Kiwië¡œ Entity/Sentence ë¶„ë¥˜ ì •ì œ (ì˜µì…˜)"""
        try:
            kiwi = self.keyword_extractor.kiwi
            
            # ì›ë³¸ ì§ˆë¬¸ í˜•íƒœì†Œ ë¶„ì„
            tokens = kiwi.tokenize(original_query)
            token_dict = {token.form: token.tag for token in tokens[0][0]}  # {ë‹¨ì–´: í’ˆì‚¬}
            
            # Entity ì •ì œ: ëª…ì‚¬ë§Œ ë‚¨ê¸°ê¸°
            refined_entities = []
            for entity in entities:
                pos_tag = token_dict.get(entity, 'UNKNOWN')
                if pos_tag in ['NNG', 'NNP', 'SL', 'SN']:  # ëª…ì‚¬ë§Œ
                    refined_entities.append(entity)
            
            # SentenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            return refined_entities, sentences
            
        except Exception as e:
            if self.verbose:
                print(f"         âš ï¸ Kiwi ì •ì œ ì‹¤íŒ¨, íœ´ë¦¬ìŠ¤í‹± ê²°ê³¼ ì‚¬ìš©: {e}")
            return entities, sentences
    
    async def _find_synonyms(self, entity: str) -> List[str]:
        """
        âœ… PostgreSQL synonym í…Œì´ë¸”ì—ì„œ canonical_name ì°¾ê¸°
        
        ì˜ˆì‹œ:
        - Input: "ë¬¼ì•½"
        - Output: ["ë¹¨ê°„ í¬ì…˜", "íŒŒë€ í¬ì…˜", "í•˜ì–€ í¬ì…˜"]
        
        Args:
            entity: ê²€ìƒ‰í•  ì—”í‹°í‹° ì´ë¦„
            
        Returns:
            ë™ì˜ì–´ë¡œ ì—°ê²°ëœ canonical_name ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë°©ë²• 1: descriptionì—ì„œ í•´ë‹¹ ë‹¨ì–´ í¬í•¨í•˜ëŠ” ì—”í‹°í‹° ì°¾ê¸°
            query = select(MapleDictionary).where(
                MapleDictionary.description.ilike(f"%{entity}%")
            ).limit(5)
            
            result = await self.db.execute(query)
            rows = result.scalars().all()
            
            canonical_names = []
            for row in rows:
                if row.canonical_name and row.canonical_name != entity:
                    canonical_names.append(row.canonical_name)
            
            return canonical_names
            
        except Exception as e:
            logger.warning(f"Synonym ê²€ìƒ‰ ì‹¤íŒ¨ ({entity}): {e}")
            return []
    
    async def _search_postgres_with_synonym(
        self,
        entities: List[str],
        limit_per_entity: int = 3
    ) -> List[Dict[str, Any]]:
        """
        âœ… Entity â†’ PostgreSQL ê²€ìƒ‰ (canonical_name + synonym)
        
        ì „ëµ:
        1. canonical_name ì§ì ‘ ë§¤ì¹­ ì‹œë„
        2. ê²°ê³¼ ì—†ìœ¼ë©´ synonym í…Œì´ë¸” ê²€ìƒ‰
        3. synonymìœ¼ë¡œ ì°¾ì€ canonical_nameìœ¼ë¡œ ì¬ê²€ìƒ‰
        
        Args:
            entities: ê²€ìƒ‰í•  Entity ë¦¬ìŠ¤íŠ¸
            limit_per_entity: Entityë‹¹ ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            PostgreSQL ê²€ìƒ‰ ê²°ê³¼
        """
        results = []
        
        for entity in entities:
            # 1ì°¨: canonical_name ì§ì ‘ ê²€ìƒ‰
            direct_results = await self.pg_searcher.search(
                entity,
                category=None,
                limit=limit_per_entity
            )
            
            # sources í•„ë“œ ì¶”ê°€
            for result in direct_results:
                if "sources" not in result:
                    result["sources"] = ["PostgreSQL"]
                result["match_type"] = "direct"  # ì§ì ‘ ë§¤ì¹­
            
            if len(direct_results) > 0:
                results.extend(direct_results)
                
                if self.verbose:
                    print(f"         ğŸ“Œ Entity '{entity}': {len(direct_results)}ê°œ (ì§ì ‘)")
            else:
                # 2ì°¨: synonym ê²€ìƒ‰ (ì§ì ‘ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ)
                if self.verbose:
                    print(f"         ğŸ” Entity '{entity}': ì§ì ‘ ë§¤ì¹­ ì‹¤íŒ¨, synonym ê²€ìƒ‰...")
                
                synonyms = await self._find_synonyms(entity)
                
                for canonical in synonyms[:2]:  # ìƒìœ„ 2ê°œë§Œ
                    synonym_results = await self.pg_searcher.search(
                        canonical,
                        category=None,
                        limit=2
                    )
                    
                    # sources í•„ë“œ ì¶”ê°€
                    for result in synonym_results:
                        if "sources" not in result:
                            result["sources"] = ["PostgreSQL"]
                        result["match_type"] = "synonym"  # synonym ë§¤ì¹­
                        result["original_query"] = entity  # ì›ë³¸ ê²€ìƒ‰ì–´ ë³´ì¡´
                    
                    results.extend(synonym_results)
                
                if self.verbose and len(synonyms) > 0:
                    print(f"         ğŸ“Œ Entity '{entity}': {len(synonyms)}ê°œ synonym â†’ {len(results)}ê°œ ê²°ê³¼")
        
        return results
    
    def _apply_rrf(
        self,
        results_by_source: Dict[str, List[Dict[str, Any]]],
        k: int = 60
    ) -> List[Dict[str, Any]]:
        """
        RRF (Reciprocal Rank Fusion) ì ìš©
        
        ì—¬ëŸ¬ ê²€ìƒ‰ ì†ŒìŠ¤ì˜ ê²°ê³¼ë¥¼ ë­í¬ ê¸°ë°˜ìœ¼ë¡œ ìœµí•©
        
        ê³µì‹: RRF_score(d) = Î£ 1 / (k + rank_i(d))
        
        Args:
            results_by_source: ì†ŒìŠ¤ë³„ ê²°ê³¼
                {
                    "PostgreSQL": [...],
                    "Neo4j": [...],
                    "Milvus": [...]
                }
            k: RRF ìƒìˆ˜ (ê¸°ë³¸ 60)
            
        Returns:
            RRF ì ìˆ˜ë¡œ ì •ë ¬ëœ ê²°ê³¼
        """
        rrf_scores = {}  # entity_id -> RRF score
        entity_data = {}  # entity_id -> ì‹¤ì œ ë°ì´í„°
        
        # ê° ì†ŒìŠ¤ë³„ë¡œ ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        for source, results in results_by_source.items():
            if not results:
                continue
            
            # ê²°ê³¼ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ê° ì†ŒìŠ¤ ë‚´ì—ì„œ)
            sorted_results = sorted(
                results,
                key=lambda x: x.get("score", 0),
                reverse=True
            )
            
            # ìˆœìœ„ ê¸°ë°˜ RRF ì ìˆ˜ ê³„ì‚°
            for rank, result in enumerate(sorted_results):
                data = result.get("data", {})
                entity_id = str(data.get("id", ""))
                
                if not entity_id:
                    continue
                
                # RRF ì ìˆ˜: 1 / (k + rank)
                rrf_score = 1.0 / (k + rank)
                
                # ëˆ„ì 
                if entity_id in rrf_scores:
                    rrf_scores[entity_id] += rrf_score
                else:
                    rrf_scores[entity_id] = rrf_score
                    entity_data[entity_id] = result
        
        # RRF ì ìˆ˜ë¡œ ì •ë ¬
        sorted_entities = sorted(
            rrf_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # ìµœì¢… ê²°ê³¼ ìƒì„±
        final_results = []
        for entity_id, rrf_score in sorted_entities:
            result = entity_data[entity_id]
            
            # RRF ì ìˆ˜ë¥¼ 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            # (ìµœëŒ€ RRF ì ìˆ˜ë¥¼ 100ìœ¼ë¡œ ì •ê·œí™”)
            max_rrf = sorted_entities[0][1] if sorted_entities else 1.0
            normalized_score = (rrf_score / max_rrf) * 100
            
            final_results.append({
                "score": normalized_score,
                "match_type": result.get("match_type", "rrf"),
                "data": result.get("data"),
                "sources": result.get("sources", []),
                "rrf_score": rrf_score  # ì›ë³¸ RRF ì ìˆ˜ë„ ë³´ì¡´
            })
        
        return final_results
    
    async def _rerank_with_jina(
        self,
        query: str,
        results: List[Dict[str, Any]],
        top_n: int = 5
    ) -> List[Dict[str, Any]]:
        """
        âœ… Jina Rerankerë¡œ ê²°ê³¼ ì¬ì •ë ¬
        
        RRF í›„ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•´ LLM ê¸°ë°˜ reranker ì ìš©
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            results: RRFë¡œ ë³‘í•©ëœ ê²°ê³¼
            top_n: ë°˜í™˜í•  ìƒìœ„ ê°œìˆ˜
            
        Returns:
            Reranker ì ìˆ˜ë¡œ ì •ë ¬ëœ ìƒìœ„ ê²°ê³¼
        """
        if not results:
            return results
        
        try:
            # Reranker API URL (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            reranker_url = getattr(settings, 'RERANKER_API_URL', None) or \
                          os.getenv('RERANKER_API_URL', 'http://localhost:8001/rerank')
            
            # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            texts = []
            for result in results:
                data = result.get("data", {})
                # print(f"ì—¬ê¸°ì„œ ì˜¤ë¥˜ í„°ì§ get data / results : {results}")
                text = f"{data.get('canonical_name', '')} - {data.get('description', '')}"
                texts.append(text)
            
            # Reranker API í˜¸ì¶œ
            payload = {
                "query": query,
                "texts": texts,
                "top_n": min(top_n, len(texts))
            }
            
            response = requests.post(reranker_url, json=payload, timeout=3)
            
            if response.status_code != 200:
                logger.warning(f"Reranker API ì‹¤íŒ¨ (status {response.status_code}), RRF ê²°ê³¼ ì‚¬ìš©")
                return results
            
            reranked_data = response.json()
            
            # Reranker ê²°ê³¼ì— ë”°ë¼ ì¬ì •ë ¬
            reranked_results = []
            for item in reranked_data.get("results", []):
                index = item.get("index")
                # print(f"ì—¬ê¸°ì„œ ì˜¤ë¥˜ í„°ì§ get index / results : {results}")
                score = item.get("score", 0)
                
                if index < len(results):
                    result = results[index].copy()
                    result["rerank_score"] = score
                    result["score"] = score * 100  # 0-100 ìŠ¤ì¼€ì¼
                    reranked_results.append(result)
            
            return reranked_results
            
        except requests.exceptions.Timeout:
            logger.warning("Reranker API íƒ€ì„ì•„ì›ƒ, RRF ê²°ê³¼ ì‚¬ìš©")
            return results
        except Exception as e:
            logger.warning(f"Reranker ì‹¤íŒ¨: {e}, RRF ê²°ê³¼ ì‚¬ìš©")
            return results
    
    async def _search_milvus_sentences(
        self,
        sentences: List[str]
    ) -> List[Dict[str, Any]]:
        """
        âœ… Sentence â†’ Milvus ì˜ë¯¸ ê²€ìƒ‰
        
        Args:
            sentences: ë™ì‚¬êµ¬ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["ë¬¼ì•½ íŒŒëŠ” ì‚¬ëŒ"])
            
        Returns:
            Milvus ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.milvus_searcher:
            return []
        
        all_results = []
        for sentence in sentences:
            try:
                results = await self.milvus_searcher.search(sentence, top_k=5)
                
                # ê²°ê³¼ í¬ë§·íŒ…
                for result in results:
                    all_results.append({
                        "score": result.get("score", 0) * 100,
                        "match_type": "vector_semantic",
                        "sources": ["Milvus"],
                        "data": result,
                        "search_query": sentence
                    })
                    
            except Exception as e:
                logger.warning(f"Milvus ê²€ìƒ‰ ì‹¤íŒ¨ ({sentence}): {e}")
        
        return all_results
    
    async def _search_neo4j_relations(
        self,
        query: str,
        entities: List[str],
        router_result: Optional[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        âœ… Neo4j ê´€ê³„ ê²€ìƒ‰ (hop >= 2)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            entities: Entity ë¦¬ìŠ¤íŠ¸
            router_result: Router ê²°ê³¼ (relation ì •ë³´)
            
        Returns:
            Neo4j ê²€ìƒ‰ ê²°ê³¼
        """
        if not self.neo4j_searcher:
            return []
        
        results = []
        relation = router_result.get("relation", "") if router_result else ""
        
        # Entity ê¸°ë°˜ìœ¼ë¡œ ê´€ê³„ ê²€ìƒ‰
        for entity in entities:
            try:
                # ê´€ê³„ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ Neo4j ë©”ì„œë“œ í˜¸ì¶œ
                if "MONSTER-MAP" in relation or "ì–´ë””" in query:
                    # ëª¬ìŠ¤í„° ìœ„ì¹˜
                    monster_results = await self.neo4j_searcher.find_monster_locations(entity)
                    results.extend(self._format_graph_results(monster_results, "graph_monster_location"))
                
                elif "NPC-MAP" in relation or "QUEST-NPC-MAP" in relation:
                    # NPC ìœ„ì¹˜
                    npc_results = await self.neo4j_searcher.find_npc_location(entity)
                    results.extend(self._format_graph_results(npc_results, "graph_npc_location"))
                
                elif "ITEM-MONSTER" in relation or "ë“œë" in query or "ì–»" in query:
                    # ì•„ì´í…œ ë“œë ëª¬ìŠ¤í„°
                    dropper_results = await self.neo4j_searcher.find_item_droppers(entity)
                    results.extend(self._format_graph_results(dropper_results, "graph_item_dropper"))
                
                elif "ITEM-NPC" in relation or "íŒŒëŠ”" in query or "êµ¬ë§¤" in query:
                    # ì•„ì´í…œ íŒë§¤ NPC
                    seller_results = await self.neo4j_searcher.find_item_sellers(entity)
                    results.extend(self._format_graph_results(seller_results, "graph_item_seller"))
                
                elif "MAP-MAP" in relation or "ê°€ëŠ”" in query:
                    # ë§µ ì—°ê²°
                    map_results = await self.neo4j_searcher.find_map_connections(entity)
                    results.extend(self._format_graph_results(map_results, "graph_map_connection"))
                    
            except Exception as e:
                logger.warning(f"Neo4j ê´€ê³„ ê²€ìƒ‰ ì‹¤íŒ¨ ({entity}): {e}")
        
        # PostgreSQLë¡œ ë³´ê°•
        enriched_results = await self._enrich_graph_results(results)
        
        return enriched_results
    
    async def _postgres_search(
        self,
        query: str,
        category: Optional[str],
        limit: int
    ) -> List[Dict[str, Any]]:
        """PostgreSQL ê²€ìƒ‰ (ë¹ ë¥¸ ì •í™• ë§¤ì¹­, async)"""
        try:
            results = await self.pg_searcher.search(query, category=category, limit=limit)
            return results
        except Exception as e:
            logger.error(f"PostgreSQL ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _milvus_expansion_search(
        self,
        pg_results: List[Dict[str, Any]],
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        Milvus ì—°ê´€ í™•ì¥ ê²€ìƒ‰ (async)
        PostgreSQLì—ì„œ ì°¾ì€ ì—”í‹°í‹°ë“¤ì˜ ì—°ê´€ í•­ëª© ê²€ìƒ‰
        """
        if not self.milvus_searcher:
            return []
        
        milvus_results = []
        seen_ids = set()
        
        # PostgreSQLì—ì„œ ì°¾ì€ TOP 3 ì—”í‹°í‹°ë¡œ í™•ì¥
        for pg_item in pg_results[:3]:
            data = pg_item.get("data", {})
            canonical_name = data.get("canonical_name", "")
            item_id = data.get("id")
            
            if item_id:
                seen_ids.add(str(item_id))
            
            if not canonical_name:
                continue
            
            try:
                # canonical_nameìœ¼ë¡œ Milvus ê²€ìƒ‰
                results = await self.milvus_searcher.search(canonical_name, top_k=5)
                
                # ê²°ê³¼ ì¶”ê°€
                for result in results:
                    result_id = result.get("id")
                    
                    if result_id and result_id not in seen_ids:
                        milvus_results.append({
                            "score": result.get("score", 0) * 50,  # ì ìˆ˜ ì¡°ì •
                            "match_type": "milvus_expansion",
                            "data": result,
                            "source_entity": canonical_name
                        })
                        seen_ids.add(result_id)
                        
                        if len(milvus_results) >= limit:
                            break
                
            except Exception as e:
                logger.warning(f"Milvus í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨ ({canonical_name}): {e}")
                continue
            
            if len(milvus_results) >= limit:
                break
        
        return milvus_results
    
    async def _milvus_semantic_search(
        self,
        query: str,
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        Milvus ì˜ë¯¸ ê²€ìƒ‰ (í´ë°±, async)
        ì§ˆë¬¸ ì „ì²´ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ê²€ìƒ‰
        """
        if not self.milvus_searcher:
            return []
        
        try:
            # Milvus Q&A ê²€ìƒ‰
            results = await self.milvus_searcher.search(query, top_k=limit)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for result in results:
                formatted_results.append({
                    "score": result.get("score", 0) * 100,  # ì ìˆ˜ ì¡°ì •
                    "match_type": "milvus_semantic",
                    "data": result
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Milvus ì˜ë¯¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _merge_results(
        self,
        pg_results: List[Dict[str, Any]],
        milvus_results: List[Dict[str, Any]],
        mode: str = "expansion"
    ) -> List[Dict[str, Any]]:
        """
        PostgreSQL + Milvus ê²°ê³¼ ë³‘í•© (RRF ì ìš©)
        
        Args:
            mode: "expansion" (í™•ì¥) ë˜ëŠ” "fallback" (í´ë°±)
        """
        # RRF ì ìš©
        results_by_source = {
            "PostgreSQL": pg_results,
            "Milvus": milvus_results,
            "Neo4j": []  # ì´ ë©”ì„œë“œì—ì„œëŠ” Neo4j ì—†ìŒ
        }
        
        return self._apply_rrf(results_by_source)


# í¸ì˜ í•¨ìˆ˜
async def hybrid_search(
    db: AsyncSession,
    query: str,
    category: Optional[str] = None,
    limit: int = 10,
    use_milvus: bool = True
) -> List[Dict[str, Any]]:
    """
    ê°„ë‹¨í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜ (async)
    
    Usage:
        results = await hybrid_search(db, "ì•„ì´ìŠ¤ì§„ ì–´ë””ì„œ ì‚¬ë‚˜ìš”?")
    """
    searcher = HybridSearcher(db, use_milvus=use_milvus)
    return await searcher.search(query, category=category, limit=limit)
