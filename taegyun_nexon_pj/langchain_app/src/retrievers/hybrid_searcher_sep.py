"""
Hybrid Search with Intent-based Routing (Async) - IMPROVED
Router Agent â†’ Category ìš°ì„ ìˆœìœ„ ê²°ì • â†’ PostgreSQL/Milvus ê²€ìƒ‰
+ Plan Execution: Multi-step ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
+ Kiwi í˜•íƒœì†Œ ë¶„ì„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
+ N-gram Reconstruction: Entity vs Sentence ë¶„ë¥˜
+ Synonym Resolution: PostgreSQL synonym í…Œì´ë¸” í™œìš©
"""
from typing import List, Dict, Any, Optional
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
import asyncio
import logging
import re
import requests
import os

from database.models.maple_dictionary import MapleDictionary
from src.retrievers.db_searcher import MapleDBSearcher
from src.retrievers.milvus_retriever import MilvusRetriever
from src.retrievers.neo4j_searcher import Neo4jSearcher
from src.agents.router_agent_sep import RouterAgent
from src.utils.keyword_extractor import MapleKeywordExtractor
from config.settings import settings

logger = logging.getLogger(__name__)


class HybridSearcher:
    """
    âœ… IMPROVED: Intent ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Plan Execution
    
    ê°œì„ ì‚¬í•­:
    1. N-gram Reconstruction (Entity vs Sentence ë¶„ë¥˜)
       - LLM í‚¤ì›Œë“œ ì¶”ì¶œ â†’ Entity/Sentence ì¬êµ¬ì„±
       - Entity: ë‹¨ì¼ ëª…ì‚¬ â†’ PostgreSQL
       - Sentence: ë™ì‚¬êµ¬ â†’ Milvus (ì˜ë¯¸ ê²€ìƒ‰)
    
    2. Synonym Resolution (PostgreSQL)
       - canonical_name ì§ì ‘ ë§¤ì¹­
       - ì‹¤íŒ¨ ì‹œ synonym í…Œì´ë¸” ê²€ìƒ‰
       - description ê¸°ë°˜ ê°„ì ‘ ë§¤ì¹­
    
    3. Selective DB Usage (LLM Plan ê¸°ë°˜)
       - ê°„ë‹¨í•œ ì§ˆë¬¸: SQL_DBë§Œ
       - ë³µì¡í•œ ì§ˆë¬¸: SQL_DB + VECTOR_DB
       - ê´€ê³„ ì§ˆë¬¸: GRAPH_DB ì¶”ê°€
    
    ì „ëµ:
    1. Router Agentë¡œ Intent ë¶„ì„ & Plan ìˆ˜ë¦½
       - Queryì˜ ì˜ë„ íŒŒì•…
       - Multi-step ê²€ìƒ‰ ì „ëµ ìƒì„±
       - ì–´ëŠ DBë¥¼ ì“¸ì§€ ì„ íƒì  ê²°ì • âœ…
    
    2. Plan ì‹¤í–‰ (Entity/Sentence ë¶„ë¦¬ ì²˜ë¦¬)
       - SQL_DB â†’ Entityë§Œ (canonical_name + synonym) âœ…
       - VECTOR_DB â†’ Sentenceë§Œ (ì˜ë¯¸ ê²€ìƒ‰) âœ…
       - GRAPH_DB â†’ Neo4j (ê´€ê³„ ì¶”ì )
    
    3. ê²°ê³¼ ë³‘í•© & ë­í‚¹
       - RRF (Reciprocal Rank Fusion)
       - ì—¬ëŸ¬ Stepì˜ ê²°ê³¼ë¥¼ í†µí•©
       - ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
    
    ì˜ˆì‹œ:
    - ì§ˆë¬¸: "ë¦¬ìŠ¤í•­êµ¬ì—ì„œ ë¬¼ì•½ íŒŒëŠ” ì‚¬ëŒ ëˆ„êµ¬ì•¼?"
    - í‚¤ì›Œë“œ: ['ë¦¬ìŠ¤í•­êµ¬', 'ë¬¼ì•½', 'íŒŒëŠ”', 'ì‚¬ëŒ']
    - ì¬êµ¬ì„±:
      * Entity: ['ë¦¬ìŠ¤í•­êµ¬'] â†’ PostgreSQL (canonical_name)
      * Sentence: ['ë¬¼ì•½ íŒŒëŠ” ì‚¬ëŒ'] â†’ Milvus (ì˜ë¯¸ ê²€ìƒ‰)
    - ê²°ê³¼: "ë¦¬ìŠ¤í•­êµ¬" (PG), "ë¯¸ë‚˜" (Milvus - í¬ì…˜ ìƒì¸)
    """
    
    def __init__(
        self, 
        db: AsyncSession,
        use_milvus: bool = True,
        use_neo4j: bool = True,
        use_router: bool = True,
        verbose: bool = False
    ):
        self.db = db
        self.use_milvus = use_milvus
        self.use_neo4j = use_neo4j
        self.use_router = use_router
        self.verbose = verbose
        
        # PostgreSQL Searcher
        self.pg_searcher = MapleDBSearcher(db)
        
        # Milvus Searcher (ì˜µì…˜)
        self.milvus_searcher = None
        if use_milvus:
            try:
                self.milvus_searcher = MilvusRetriever()
                logger.info("âœ… Milvus ê²€ìƒ‰ í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Milvus ì—°ê²° ì‹¤íŒ¨, PostgreSQLë§Œ ì‚¬ìš©: {e}")
                self.use_milvus = False
        
        # Neo4j Searcher (ì˜µì…˜)
        self.neo4j_searcher = None
        if use_neo4j:
            try:
                self.neo4j_searcher = Neo4jSearcher()
                logger.info("âœ… Neo4j ê²€ìƒ‰ í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Neo4j ì—°ê²° ì‹¤íŒ¨: {e}")
                self.use_neo4j = False
        
        # Router Agent (ì˜µì…˜)
        self.router = None
        if use_router:
            try:
                self.router = RouterAgent(verbose=False)
                logger.info("âœ… Router Agent í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Router Agent ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_router = False
        
        # Keyword Extractor (Kiwi í˜•íƒœì†Œ ë¶„ì„ + ë™ì˜ì–´ ì¹˜í™˜)
        try:
            self.keyword_extractor = MapleKeywordExtractor(db)
            logger.info("âœ… Kiwi Keyword Extractor í™œì„±í™”")
        except Exception as e:
            logger.warning(f"âš ï¸ Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©: {e}")
            self.keyword_extractor = None
    
    async def search(
        self,
        query: str,
        category: Optional[str] = None,
        limit: int = 10,
        pg_threshold: int = 3,
        use_plan_execution: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Intent ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Plan Execution
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            category: ì¹´í…Œê³ ë¦¬ í•„í„° (ì˜µì…˜, Routerê°€ ìë™ ê²°ì •)
            limit: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜
            pg_threshold: PostgreSQL ê²°ê³¼ê°€ ì´ ê°œìˆ˜ ì´ìƒì´ë©´ í™•ì¥, ë¯¸ë§Œì´ë©´ í´ë°±
            use_plan_execution: Plan ì‹¤í–‰ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (ì ìˆ˜ ìˆœ ì •ë ¬)
        """
        if self.verbose:
            print(f"\nğŸ” Hybrid Search: '{query}'")
        
        # Step 0: Router Agentë¡œ Intent ë¶„ì„ & Plan ìˆ˜ë¦½
        router_result = None
        if self.use_router and self.router and not category:
            try:
                router_result = self.router.route(query)
                
                # Planì˜ query í•„ë“œì—ì„œ ì¹´í…Œê³ ë¦¬ ì ‘ë‘ì‚¬ ì œê±° (í›„ì²˜ë¦¬)
                if "plan" in router_result and router_result["plan"]:
                    for step in router_result["plan"]:
                        if "query" in step:
                            original_query = step["query"]
                            # ì¹´í…Œê³ ë¦¬ ì ‘ë‘ì‚¬ ì œê±°
                            for prefix in ["MAP ", "MONSTER ", "NPC ", "ITEM "]:
                                step["query"] = step["query"].replace(prefix, "")
                
                if self.verbose:
                    print(f"   ğŸ§­ Intent: {router_result['intent']}")
                    print(f"   ğŸ“ Categories: {router_result['categories']}")
                
                # Planì´ ìˆê³  Plan ì‹¤í–‰ ëª¨ë“œë©´ Plan ì‹¤í–‰
                if use_plan_execution and "plan" in router_result and router_result["plan"]:
                    if self.verbose:
                        print(f"   ğŸš€ Plan ì‹¤í–‰ ëª¨ë“œ ({len(router_result['plan'])} steps)")
                    return await self.execute_plan(query, router_result, limit)
                
                # Routerê°€ ì œì•ˆí•œ ì²« ë²ˆì§¸ category ì‚¬ìš©
                if router_result['categories']:
                    category = router_result['categories'][0]
                    if self.verbose:
                        print(f"   âœ… Category ì„ íƒ: {category}")
            except Exception as e:
                logger.warning(f"Router ì‹¤íŒ¨, category ì—†ì´ ì§„í–‰: {e}")
        
        # Step 1: PostgreSQL ê²€ìƒ‰ (ê¸°ì¡´ ë¡œì§)
        pg_results = await self._postgres_search(query, category, limit)
        
        if self.verbose:
            print(f"   PostgreSQL: {len(pg_results)}ê°œ ê²°ê³¼")
        
        # Milvus ì‚¬ìš© ì•ˆí•˜ë©´ PostgreSQL ê²°ê³¼ë§Œ ë°˜í™˜
        if not self.use_milvus or not self.milvus_searcher:
            return pg_results[:limit]
        
        # Step 2: ê²°ê³¼ ë¶„ê¸°
        if len(pg_results) >= pg_threshold:
            # âœ… ì¶©ë¶„íˆ ì°¾ìŒ â†’ Milvusë¡œ ì—°ê´€ í™•ì¥
            if self.verbose:
                print(f"   âœ… PostgreSQL ì„±ê³µ â†’ Milvus ì—°ê´€ ê²€ìƒ‰")
            
            milvus_results = await self._milvus_expansion_search(pg_results, limit)
            
            if self.verbose:
                print(f"   Milvus í™•ì¥: {len(milvus_results)}ê°œ ì¶”ê°€")
            
            # ë³‘í•© & ë­í‚¹
            merged = self._merge_results(pg_results, milvus_results, mode="expansion")
            
        else:
            # âš ï¸ ë¶€ì¡±í•¨ â†’ Milvusë¡œ ì˜ë¯¸ ê²€ìƒ‰ (í´ë°±)
            if self.verbose:
                print(f"   âš ï¸ PostgreSQL ë¶€ì¡± ({len(pg_results)}/{pg_threshold}) â†’ Milvus ì˜ë¯¸ ê²€ìƒ‰")
            
            milvus_results = await self._milvus_semantic_search(query, limit)
            
            if self.verbose:
                print(f"   Milvus ì˜ë¯¸: {len(milvus_results)}ê°œ ê²°ê³¼")
            
            # ë³‘í•© & ë­í‚¹
            merged = self._merge_results(pg_results, milvus_results, mode="fallback")
        
        # ìµœì¢… ê²°ê³¼
        final_results = merged[:limit]
        
        if self.verbose:
            print(f"   ğŸ“Š ìµœì¢…: {len(final_results)}ê°œ\n")
        
        return final_results
    
    async def execute_plan(
        self,
        original_query: str,
        router_result: Dict[str, Any],
        limit: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Routerì˜ Planì„ ì‹¤ì œë¡œ ì‹¤í–‰ (async/await ë³‘ë ¬ + ìˆœì°¨ í•˜ì´ë¸Œë¦¬ë“œ)
        + RRF (Reciprocal Rank Fusion) ì ìš©
        
        ì „ëµ:
        1. Planì„ ë°°ì¹˜ë¡œ ê·¸ë£¹í™”
           - SQL_DB, VECTOR_DBëŠ” ë…ë¦½ì  â†’ ë³‘ë ¬ ì‹¤í–‰
           - GRAPH_DBëŠ” ì´ì „ ê²°ê³¼ í•„ìš” â†’ ìƒˆ ë°°ì¹˜ ì‹œì‘
        2. ê° ë°°ì¹˜ë¥¼ asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰
        3. ë°°ì¹˜ ê°„ì—ëŠ” ìˆœì°¨ ì‹¤í–‰ (ì˜ì¡´ì„± ë³´ì¥)
        4. RRFë¡œ ë‹¤ì¤‘ ì†ŒìŠ¤ ê²°ê³¼ ìœµí•©
        
        Args:
            original_query: ì›ë³¸ ì§ˆë¬¸
            router_result: Routerê°€ ìƒì„±í•œ Plan
            limit: ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ (RRF ì ìˆ˜ ìˆœ)
        """
        plan = router_result.get("plan", [])
        
        if not plan:
            logger.warning("Planì´ ë¹„ì–´ìˆìŒ, ê¸°ë³¸ ê²€ìƒ‰ìœ¼ë¡œ í´ë°±")
            return await self._postgres_search(original_query, None, limit)
        
        if self.verbose:
            print(f"\n   ğŸ“‹ Plan ì‹¤í–‰ (ë³‘ë ¬ ìµœì í™” + RRF):")
        
        # Planì„ ë°°ì¹˜ë¡œ ê·¸ë£¹í™”
        batches = self._group_plan_into_batches(plan)
        
        if self.verbose:
            print(f"      ë°°ì¹˜: {len(batches)}ê°œ (ë³‘ë ¬ ê°€ëŠ¥í•œ Stepë¼ë¦¬ ê·¸ë£¹í™”)")
        
        # ì†ŒìŠ¤ë³„ ê²°ê³¼ ìˆ˜ì§‘ (RRFìš©)
        results_by_source = {
            "PostgreSQL": [],
            "Neo4j": [],
            "Milvus": []
        }
        previous_batch_results = []
        
        # ê° ë°°ì¹˜ ì‹¤í–‰
        for batch_idx, batch in enumerate(batches):
            if self.verbose:
                print(f"\n      === ë°°ì¹˜ {batch_idx + 1}/{len(batches)} ({'ë³‘ë ¬' if len(batch) > 1 else 'ìˆœì°¨'}) ===")
            
            # ë°°ì¹˜ ë‚´ Stepë“¤ì„ ë³‘ë ¬ ì‹¤í–‰
            batch_results = await self._execute_batch_parallel(
                batch, 
                original_query, 
                router_result, 
                previous_batch_results
            )
            
            # ì†ŒìŠ¤ë³„ë¡œ ë¶„ë¥˜
            for result in batch_results:
                sources = result.get("sources", [])
                for source in sources:
                    if source in results_by_source:
                        results_by_source[source].append(result)
            
            # ì´ ë°°ì¹˜ ê²°ê³¼ë¥¼ ë‹¤ìŒ ë°°ì¹˜ì— ì „ë‹¬
            previous_batch_results = batch_results
        
        # RRF ì ìš©
        rrf_results = self._apply_rrf(results_by_source)
        
        if self.verbose:
            print(f"\n   âœ… RRF ì™„ë£Œ: {len(rrf_results)}ê°œ ê²°ê³¼")
        
        # âœ… Reranker ì ìš© (RRF í›„ ë…¸ì´ì¦ˆ ì œê±°)
        if len(rrf_results) > limit:
            rrf_results = await self._rerank_with_jina(original_query, rrf_results, top_n=limit)
            
            if self.verbose:
                print(f"   âœ… Reranker ì™„ë£Œ: {len(rrf_results)}ê°œ ê²°ê³¼")
        
        if self.verbose:
            print(f"\n   âœ… Plan ì‹¤í–‰ ì™„ë£Œ: ì´ {len(rrf_results)}ê°œ ê²°ê³¼")
        
        return rrf_results[:limit]
    
    def _group_plan_into_batches(self, plan: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
        """
        Planì„ ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ë°°ì¹˜ë¡œ ê·¸ë£¹í™”
        
        ê·œì¹™:
        1. SQL_DB, VECTOR_DBëŠ” ë…ë¦½ì  â†’ ê°™ì€ ë°°ì¹˜ì— í¬í•¨ ê°€ëŠ¥
        2. GRAPH_DBëŠ” ì´ì „ ê²°ê³¼ í•„ìš” â†’ ìƒˆ ë°°ì¹˜ ì‹œì‘
        
        Args:
            plan: Step ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ë°°ì¹˜ ë¦¬ìŠ¤íŠ¸ (ê° ë°°ì¹˜ëŠ” ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ Stepë“¤)
        """
        if not plan:
            return []
        
        batches = []
        current_batch = []
        
        for step in plan:
            tool = step.get("tool", "")
            
            # GRAPH_DBëŠ” ì´ì „ ê²°ê³¼ì— ì˜ì¡´ â†’ ë°°ì¹˜ ë¶„ë¦¬
            if tool == "GRAPH_DB":
                # í˜„ì¬ ë°°ì¹˜ê°€ ìˆìœ¼ë©´ ë¨¼ì € ì¶”ê°€
                if current_batch:
                    batches.append(current_batch)
                    current_batch = []
                # GRAPH_DBëŠ” ë³„ë„ ë°°ì¹˜
                batches.append([step])
            else:
                # SQL_DB, VECTOR_DBëŠ” ê°™ì€ ë°°ì¹˜ì— ì¶”ê°€
                current_batch.append(step)
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì¶”ê°€
        if current_batch:
            batches.append(current_batch)
        
        return batches
    
    async def _execute_batch_parallel(
        self,
        batch: List[Dict[str, Any]],
        original_query: str,
        router_result: Dict[str, Any],
        previous_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ë°°ì¹˜ ë‚´ Stepë“¤ì„ ë³‘ë ¬ ì‹¤í–‰ (async/await)
        
        Args:
            batch: ë³‘ë ¬ ì‹¤í–‰í•  Step ë¦¬ìŠ¤íŠ¸
            original_query: ì›ë³¸ ì§ˆë¬¸
            router_result: Router ê²°ê³¼
            previous_results: ì´ì „ ë°°ì¹˜ ê²°ê³¼
            
        Returns:
            ë°°ì¹˜ ì‹¤í–‰ ê²°ê³¼
        """
        batch_results = []
        
        # ë‹¨ì¼ Step â†’ ê·¸ëƒ¥ ì‹¤í–‰
        if len(batch) == 1:
            step = batch[0]
            step_num = step.get("step", 0)
            tool = step.get("tool", "")
            reason = step.get("reason", "")
            
            if self.verbose:
                print(f"      [{step_num}] {tool}: {reason}")
            
            results = await self._execute_single_step(step, original_query, router_result, previous_results)
            
            if self.verbose:
                print(f"         â†’ {len(results)}ê°œ ë°œê²¬")
            
            return results
        
        # ë‹¤ì¤‘ Step â†’ async ë³‘ë ¬ ì‹¤í–‰
        if self.verbose:
            for step in batch:
                print(f"      [{step.get('step', 0)}] {step.get('tool', '')}: {step.get('reason', '')}")
        
        # asyncio.gatherë¡œ ë³‘ë ¬ ì‹¤í–‰
        tasks = [
            self._execute_single_step(step, original_query, router_result, previous_results)
            for step in batch
        ]
        
        try:
            # ëª¨ë“  íƒœìŠ¤í¬ ë³‘ë ¬ ì‹¤í–‰
            results_list = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ìˆ˜ì§‘
            for idx, (step, results) in enumerate(zip(batch, results_list)):
                if isinstance(results, Exception):
                    logger.error(f"Step {step.get('step', 0)} ì‹¤í–‰ ì‹¤íŒ¨: {results}")
                else:
                    batch_results.extend(results)
                    
                    if self.verbose:
                        print(f"         [{step.get('step', 0)}] â†’ {len(results)}ê°œ ë°œê²¬")
        
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë³‘ë ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        
        return batch_results
    
    async def _execute_single_step(
        self,
        step: Dict[str, Any],
        original_query: str,
        router_result: Dict[str, Any],
        previous_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ë‹¨ì¼ Step ì‹¤í–‰ (async)
        
        Args:
            step: ì‹¤í–‰í•  Step
            original_query: ì›ë³¸ ì§ˆë¬¸
            router_result: Router ê²°ê³¼
            previous_results: ì´ì „ ê²°ê³¼
            
        Returns:
            Step ì‹¤í–‰ ê²°ê³¼
        """
        tool = step.get("tool", "")
        query = step.get("query", "")
        
        try:
            if tool == "SQL_DB":
                return await self._execute_sql_db_step(original_query, query, router_result, step)
                
            elif tool == "GRAPH_DB":
                # ì´ì „ ê²°ê³¼ë¡œ ì¿¼ë¦¬ ì¡°ì •
                adjusted_query = self._adjust_graph_query(query, previous_results)
                results = await self._execute_graph_db_step(original_query, adjusted_query, router_result)
                # PostgreSQLë¡œ ë³´ì¶©
                return await self._enrich_graph_results(results)
                
            elif tool == "VECTOR_DB":
                return await self._execute_vector_db_step(original_query, query, router_result, step)
            else:
                logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” Tool: {tool}")
                return []
                
        except Exception as e:
            logger.error(f"Step ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _adjust_graph_query(
        self,
        query: str,
        previous_results: List[Dict[str, Any]]
    ) -> str:
        """
        GRAPH_DB ì¿¼ë¦¬ë¥¼ ì´ì „ Step ê²°ê³¼ë¡œ ì¡°ì •
        
        Args:
            query: ì›ë³¸ ì¿¼ë¦¬ (ì˜ˆ: "ë‹¤í¬ë¡œë“œ â†’ ìœ„ì¹˜ â†’ MAP")
            previous_results: ì´ì „ Stepì˜ ê²€ìƒ‰ ê²°ê³¼
            
        Returns:
            ì¡°ì •ëœ ì¿¼ë¦¬
        """
        if not previous_results:
            return query
        
        try:
            # ì´ì „ Stepì—ì„œ ì°¾ì€ ì²« ë²ˆì§¸ ì—”í‹°í‹° ì´ë¦„ ì¶”ì¶œ
            first_result = previous_results[0]
            data = first_result.get("data", {})
            canonical_name = data.get("canonical_name")
            
            if not canonical_name:
                return query
            
            # ì¿¼ë¦¬ì—ì„œ ì²« ë²ˆì§¸ ë‹¨ì–´(ì—”í‹°í‹° ì´ë¦„)ë¥¼ ì‹¤ì œ ì°¾ì€ ì´ë¦„ìœ¼ë¡œ ì¹˜í™˜
            # ì˜ˆ: "ë‹¤í¬ë¡œë“œ â†’ ìœ„ì¹˜ â†’ MAP"
            parts = query.split("â†’")
            if len(parts) >= 2:
                # ì²« ë²ˆì§¸ ë¶€ë¶„ì„ ì‹¤ì œ ì°¾ì€ ì—”í‹°í‹°ë¡œ êµì²´
                parts[0] = canonical_name
                adjusted_query = " â†’ ".join(parts)
                
                if self.verbose and adjusted_query != query:
                    print(f"         ì¿¼ë¦¬ ì¡°ì •: {query} â†’ {adjusted_query}")
                
                return adjusted_query
            
            return query
            
        except Exception as e:
            logger.warning(f"ì¿¼ë¦¬ ì¡°ì • ì‹¤íŒ¨: {e}")
            return query
    
    async def _execute_sql_db_step(
        self,
        original_query: str,
        step_query: str,
        router_result: Dict[str, Any],
        step: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        âœ… IMPROVED SQL_DB Step ì‹¤í–‰ (PostgreSQL ê²€ìƒ‰ with Synonym)
        
        ê°œì„ ì‚¬í•­:
        1. Routerê°€ ë¶„ë¦¬í•œ entities ì‚¬ìš© (ìš°ì„ )
        2. Router ì—†ìœ¼ë©´ ìì²´ í‚¤ì›Œë“œ ì¶”ì¶œ + N-gram ì¬êµ¬ì„± (fallback)
        3. canonical_name ì§ì ‘ ë§¤ì¹­ + synonym ê²€ìƒ‰
        
        ì „ëµ:
        - Entity (ëª…ì‚¬) â†’ PG canonical_name + synonym
        - Sentence (ë™ì‚¬êµ¬) â†’ ë¬´ì‹œ (VECTOR_DB Stepì—ì„œ ì²˜ë¦¬)
        """
        entities = []
        
        # 1. âœ… Routerê°€ ì œê³µí•œ entities ìš°ì„  ì‚¬ìš©
        if step and "entities" in step:
            entities = step.get("entities", [])
            
            if self.verbose:
                print(f"         Router entities: {entities}")
        
        # 2. Fallback: Routerê°€ ì•ˆ ì¤¬ìœ¼ë©´ ìì²´ ì¶”ì¶œ
        if not entities:
            raw_keywords = await self._extract_keywords(original_query)
            
            if self.verbose:
                print(f"         ìì²´ ì¶”ì¶œ í‚¤ì›Œë“œ: {raw_keywords}")
            
            # N-gram ì¬êµ¬ì„± (Entity vs Sentence ë¶„ë¥˜)
            structured = self._reconstruct_ngrams(raw_keywords, original_query)
            entities = structured["entities"]
            
            if self.verbose:
                print(f"         Fallback entities: {entities}")
        
        if not entities:
            # Entityê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (SentenceëŠ” VECTOR_DBì—ì„œ ì²˜ë¦¬)
            if self.verbose:
                print(f"         âš ï¸ Entity ì—†ìŒ, PostgreSQL ê²€ìƒ‰ ìƒëµ")
            return []
        
        # 3. âœ… Entity â†’ PostgreSQL ê²€ìƒ‰ (canonical_name + synonym)
        results = await self._search_postgres_with_synonym(
            entities,
            limit_per_entity=5
        )
        
        if self.verbose:
            print(f"         PostgreSQL: {len(results)}ê°œ ê²°ê³¼")
        
        return results
    
    async def _execute_graph_db_step(
        self,
        original_query: str,
        step_query: str,
        router_result: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        GRAPH_DB Step ì‹¤í–‰ (Neo4j ê´€ê³„ ê²€ìƒ‰, async)
        
        step_query ë¶„ì„:
        - "NPC â†’ MAP" â†’ NPC ìœ„ì¹˜ ì°¾ê¸°
        - "MONSTER â†’ MAP" â†’ ëª¬ìŠ¤í„° ì¶œí˜„ ìœ„ì¹˜
        - "ITEM â†’ NPC" â†’ ì•„ì´í…œ íŒë§¤ NPC
        - "ITEM â†’ MONSTER" â†’ ì•„ì´í…œ ë“œë ëª¬ìŠ¤í„°
        - "MAP â†’ MAP" â†’ ë§µ ì—°ê²°
        """
        if not self.use_neo4j or not self.neo4j_searcher:
            logger.info("Neo4j ê²€ìƒ‰ ë¹„í™œì„±í™”")
            return []
        
        # step_queryì—ì„œ ê´€ê³„ ìœ í˜• ì¶”ì¶œ
        step_query_lower = step_query.lower()
        
        # step_queryì—ì„œ ì—”í‹°í‹° ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: "ë‹¤í¬ë¡œë“œ â†’ ìœ„ì¹˜ â†’ MAP")
        entity_name = None
        if "â†’" in step_query:
            parts = step_query.split("â†’")
            entity_name = parts[0].strip()
            
            # ì¹´í…Œê³ ë¦¬ ì ‘ë‘ì‚¬ ì œê±° (MAP, MONSTER, NPC, ITEM ë“±)
            category_prefixes = ["MAP ", "MONSTER ", "NPC ", "ITEM "]
            for prefix in category_prefixes:
                if entity_name.startswith(prefix):
                    entity_name = entity_name[len(prefix):].strip()
                    break
        
        # í‚¤ì›Œë“œë¡œ ì—”í‹°í‹° ì´ë¦„ ì¶”ì¶œ (fallback)
        keywords = await self._extract_keywords(original_query)
        
        # step_queryì—ì„œ ì¶”ì¶œí•œ ì—”í‹°í‹°ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
        if entity_name:
            if entity_name not in keywords:
                keywords = [entity_name] + keywords
            else:
                # ì´ë¯¸ ìˆìœ¼ë©´ ì•ìœ¼ë¡œ ì´ë™
                keywords.remove(entity_name)
                keywords = [entity_name] + keywords
        
        if self.verbose:
            print(f"         GRAPH_DB step_query: {step_query}")
            print(f"         Keywords: {keywords}")
        
        results = []
        
        # NPC ê´€ë ¨ ê²€ìƒ‰
        if "npc" in step_query_lower and "map" in step_query_lower:
            # Case 1: NPC ìœ„ì¹˜ ê²€ìƒ‰ ("NPCê°€ ì–´ë””ì— ìˆëŠ”ì§€")
            if any(word in step_query_lower for word in ["ìœ„ì¹˜", "ì–´ë””", "ìˆëŠ”ì§€"]) and \
               any(word in original_query for word in ["ì–´ë””", "ìœ„ì¹˜"]):
                for keyword in keywords:
                    npc_results = await self.neo4j_searcher.find_npc_location(keyword)
                    results.extend(self._format_graph_results(npc_results, "graph_npc_location"))
            
            # Case 2: MAP â†’ NPC ("ë§µì— ì–´ë–¤ NPCê°€ ìˆëŠ”ì§€")
            else:
                # PostgreSQLì—ì„œ MAP ê²€ìƒ‰ í›„ resident_npcs í™œìš©
                for keyword in keywords:
                    if keyword not in ["MAP", "NPC", "MONSTER", "ITEM"] and len(keyword) >= 2:
                        try:
                            pg_results = await self.pg_searcher.search(keyword, category="MAP", limit=3)
                            # sources í•„ë“œ ì¶”ê°€
                            for result in pg_results:
                                if "sources" not in result:
                                    result["sources"] = ["PostgreSQL"]
                            results.extend(pg_results)
                        except Exception as e:
                            logger.warning(f"MAP NPC ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
        
        # ëª¬ìŠ¤í„° ìœ„ì¹˜ ê²€ìƒ‰
        elif "monster" in step_query_lower and "map" in step_query_lower:
            for keyword in keywords:
                monster_results = await self.neo4j_searcher.find_monster_locations(keyword)
                results.extend(self._format_graph_results(monster_results, "graph_monster_location"))
        
        # ì•„ì´í…œ íŒë§¤ NPC ê²€ìƒ‰
        elif "item" in step_query_lower and "npc" in step_query_lower:
            # "íŒë§¤" ë˜ëŠ” "sell" í‚¤ì›Œë“œ
            if any(word in step_query_lower for word in ["íŒë§¤", "sell", "êµ¬ë§¤", "buy", "ì‚¬"]):
                for keyword in keywords:
                    seller_results = await self.neo4j_searcher.find_item_sellers(keyword)
                    results.extend(self._format_graph_results(seller_results, "graph_item_seller"))
        
        # ì•„ì´í…œ ë“œë ëª¬ìŠ¤í„° ê²€ìƒ‰
        elif any(word in step_query_lower for word in ["ë“œë", "drop", "ë–¨ì–´", "ë‚˜ì™€", "ë‚˜ì˜¤"]):
            # "ë“œë", "ëª¬ìŠ¤í„°" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë“œë ê²€ìƒ‰
            if "ëª¬ìŠ¤í„°" in step_query_lower or "monster" in step_query_lower:
                for keyword in keywords:
                    dropper_results = await self.neo4j_searcher.find_item_droppers(keyword)
                    results.extend(self._format_graph_results(dropper_results, "graph_item_dropper"))
        
        # ë§µ ì—°ê²° ê²€ìƒ‰
        elif "map" in step_query_lower and any(word in step_query_lower for word in ["ê²½ë¡œ", "connect", "ì´ë™", "ê°€ëŠ”"]):
            for keyword in keywords:
                map_results = await self.neo4j_searcher.find_map_connections(keyword)
                results.extend(self._format_graph_results(map_results, "graph_map_connection"))
        
        # âœ… MAP ê²€ìƒ‰ (resident_npcs, resident_monstersëŠ” PostgreSQLì— ìˆìŒ)
        # "MAP â†’ ì»¤ë‹ì‹œí‹°", "ì»¤ë‹ì‹œí‹°ì— ì–´ë–¤ NPC" ë“±
        else:
            # keywordsì— ë§µ ì´ë¦„ì´ ìˆìœ¼ë©´ PostgreSQL ì§ì ‘ ê²€ìƒ‰
            for keyword in keywords:
                if keyword not in ["MAP", "NPC", "MONSTER", "ITEM"] and len(keyword) >= 2:
                    try:
                        # MAP ìš°ì„  ê²€ìƒ‰
                        if "map" in step_query_lower or "ë§µ" in step_query_lower or \
                           any(word in original_query for word in ["ì–´ë–¤", "ìˆì–´", "ì£¼ë¯¼"]):
                            pg_results = await self.pg_searcher.search(keyword, category="MAP", limit=3)
                            # sources í•„ë“œ ì¶”ê°€
                            for result in pg_results:
                                if "sources" not in result:
                                    result["sources"] = ["PostgreSQL"]
                            results.extend(pg_results)
                        
                        # ê²°ê³¼ ì—†ìœ¼ë©´ ì „ì²´ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
                        if not results:
                            pg_results = await self.pg_searcher.search(keyword, category=None, limit=3)
                            # sources í•„ë“œ ì¶”ê°€
                            for result in pg_results:
                                if "sources" not in result:
                                    result["sources"] = ["PostgreSQL"]
                            results.extend(pg_results)
                            
                    except Exception as e:
                        logger.warning(f"ê²€ìƒ‰ ì‹¤íŒ¨ ({keyword}): {e}")
        
        return results
    
    def _format_graph_results(
        self,
        graph_results: List[Dict[str, Any]],
        match_type: str
    ) -> List[Dict[str, Any]]:
        """
        Neo4j ê²°ê³¼ë¥¼ í†µí•© í¬ë§·ìœ¼ë¡œ ë³€í™˜
        """
        formatted_results = []
        
        for result in graph_results:
            # Neo4j ê²°ê³¼ì—ì„œ ì´ë¦„ ì¶”ì¶œ (ë‹¤ì–‘í•œ í•„ë“œëª… ì§€ì›)
            name = (
                result.get("npc_name") or 
                result.get("map_name") or 
                result.get("monster_name") or 
                result.get("item_name") or 
                result.get("name", "Unknown")
            )
            
            # ID ì¶”ì¶œ
            entity_id = (
                result.get("npc_id") or 
                result.get("map_id") or 
                result.get("monster_id") or 
                result.get("item_id") or 
                result.get("id", "")
            )
            
            formatted_results.append({
                "score": 85.0,  # Graph ê´€ê³„ëŠ” ë†’ì€ ì‹ ë¢°ë„
                "match_type": match_type,
                "sources": ["Neo4j"],
                "data": {
                    "id": entity_id,
                    "canonical_name": name,
                    "category": "RELATION",  # ê´€ê³„ ê²€ìƒ‰ ê²°ê³¼
                    "description": f"{result.get('relation_type', '')} ê´€ê³„",
                    "relation_info": result  # ì›ë³¸ ê´€ê³„ ì •ë³´ ë³´ì¡´
                }
            })
        
        return formatted_results
    
    async def _enrich_graph_results(self, graph_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Neo4j ê²€ìƒ‰ ê²°ê³¼ë¥¼ PostgreSQL detail_dataë¡œ ë³´ì¶© (async)
        
        Neo4jëŠ” ê´€ê³„ë§Œ ì•Œë ¤ì£¼ê³ , ì‹¤ì œ ìƒì„¸ ì •ë³´ëŠ” PostgreSQLì— ìˆìŒ
        ì˜ˆ: Neo4j â†’ "ìŠ¤í¬ì•„"ë¼ëŠ” ì´ë¦„ë§Œ
            PostgreSQL â†’ ìŠ¤í¬ì•„ì˜ level, spawn_maps, drops ë“± ìƒì„¸ ì •ë³´
        
        Args:
            graph_results: Neo4j ê²€ìƒ‰ ê²°ê³¼
            
        Returns:
            detail_dataê°€ ì¶”ê°€ëœ ê²°ê³¼
        """
        enriched = []
        
        for result in graph_results:
            data = result.get("data", {})
            canonical_name = data.get("canonical_name")
            
            if not canonical_name or canonical_name == "Unknown":
                enriched.append(result)
                continue
            
            try:
                # PostgreSQLì—ì„œ ì „ì²´ ì •ë³´ ì¡°íšŒ (async)
                stmt = select(MapleDictionary).where(
                    MapleDictionary.canonical_name == canonical_name
                )
                db_result = await self.db.execute(stmt)
                pg_entity = db_result.scalar_one_or_none()
                
                if pg_entity:
                    # PostgreSQL ë°ì´í„°ë¡œ êµì²´ (ê´€ê³„ ì •ë³´ëŠ” ìœ ì§€)
                    relation_info = data.get("relation_info")
                    result["data"] = pg_entity.to_dict()
                    
                    # ê´€ê³„ ì •ë³´ ì¶”ê°€
                    if relation_info:
                        result["data"]["relation_info"] = relation_info
                    
                    # Category ì—…ë°ì´íŠ¸
                    result["data"]["category"] = str(pg_entity.category).split('.')[-1]
                    
                enriched.append(result)
                
            except Exception as e:
                logger.warning(f"Enrichment ì‹¤íŒ¨ ({canonical_name}): {e}")
                enriched.append(result)
        
        return enriched
    
    async def _execute_vector_db_step(
        self,
        original_query: str,
        step_query: str,
        router_result: Dict[str, Any],
        step: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        âœ… IMPROVED VECTOR_DB Step ì‹¤í–‰ (Milvus ì˜ë¯¸ ê²€ìƒ‰)
        
        ê°œì„ ì‚¬í•­:
        1. Routerê°€ ë¶„ë¦¬í•œ sentences ì‚¬ìš© (ìš°ì„ )
        2. Router ì—†ìœ¼ë©´ ìì²´ í‚¤ì›Œë“œ ì¶”ì¶œ + N-gram ì¬êµ¬ì„± (fallback)
        3. ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ìœ¼ë¡œ ê°„ì ‘ í‘œí˜„ ì²˜ë¦¬
        
        ì „ëµ:
        - Sentence (ë™ì‚¬êµ¬) â†’ Milvus ì˜ë¯¸ ê²€ìƒ‰ âœ…
        - Entity (ëª…ì‚¬) â†’ ë¬´ì‹œ (SQL_DB Stepì—ì„œ ì²˜ë¦¬)
        
        ì˜ˆì‹œ:
        - "ë¬¼ì•½ íŒŒëŠ” ì‚¬ëŒ" â†’ Milvus â†’ "ë¯¸ë‚˜" (í¬ì…˜ ìƒì¸)
        """
        if not self.use_milvus or not self.milvus_searcher:
            return []
        
        try:
            sentences = []
            
            # 1. âœ… Routerê°€ ì œê³µí•œ sentences ìš°ì„  ì‚¬ìš©
            if step and "sentences" in step:
                sentences = step.get("sentences", [])
                
                if self.verbose:
                    print(f"         Router sentences: {sentences}")
            
            # 2. Fallback: Routerê°€ ì•ˆ ì¤¬ìœ¼ë©´ ìì²´ ì¶”ì¶œ
            if not sentences:
                raw_keywords = await self._extract_keywords(original_query)
                
                if self.verbose:
                    print(f"         ìì²´ ì¶”ì¶œ í‚¤ì›Œë“œ: {raw_keywords}")
                
                # N-gram ì¬êµ¬ì„± (Entity vs Sentence ë¶„ë¥˜)
                structured = self._reconstruct_ngrams(raw_keywords, original_query)
                sentences = structured["sentences"]
                
                if self.verbose:
                    print(f"         Fallback sentences: {sentences}")
            
            # 3. Sentenceê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰ (Fallback)
            search_queries = sentences if sentences else [original_query]
            
            if self.verbose:
                print(f"         Milvus ê²€ìƒ‰ ì¿¼ë¦¬: {search_queries}")
            
            # 4. âœ… Sentence â†’ Milvus ì˜ë¯¸ ê²€ìƒ‰
            all_results = []
            for query in search_queries:
                results = await self.milvus_searcher.search(query, top_k=5)
                
                # ê²°ê³¼ í¬ë§·íŒ…
                for result in results:
                    all_results.append({
                        "score": result.get("score", 0) * 100,
                        "match_type": "vector_semantic",
                        "sources": ["Milvus"],
                        "data": result,
                        "search_query": query  # ì–´ë–¤ ì¿¼ë¦¬ë¡œ ì°¾ì•˜ëŠ”ì§€ ë³´ì¡´
                    })
            
            if self.verbose:
                print(f"         Milvus: {len(all_results)}ê°œ ê²°ê³¼")
            
            return all_results
            
        except Exception as e:
            logger.error(f"VECTOR_DB ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_keywords(self, query: str) -> List[str]:
        """
        ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (Async)
        
        ì „ëµ:
        1. Kiwi í˜•íƒœì†Œ ë¶„ì„ (ìš°ì„ , ì •í™•ë„ ë†’ìŒ)
        2. Fallback: ì •ê·œì‹ + ë¶ˆìš©ì–´ (Kiwi ì‹¤íŒ¨ ì‹œ)
        """
        # Kiwi ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ìš°ì„  ì‚¬ìš©
        if self.keyword_extractor:
            try:
                return await self.keyword_extractor.extract(query)
            except Exception as e:
                logger.warning(f"Kiwi í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨, Fallback ì‚¬ìš©: {e}")
        
        # Fallback: ê¸°ì¡´ ì •ê·œì‹ ë°©ì‹
        # ì¡°ì‚¬ (í† í° ëì—ì„œ ì œê±°)
        particles = ["ì´", "ê°€", "ì„", "ë¥¼", "ì€", "ëŠ”", "ë„", "ë§Œ", "ì˜", "ì—", "ì„œ", "ë¡œ", "ì™€", "ê³¼"]
        
        # ì ‘ë¯¸ì‚¬ í˜•íƒœ ë¶ˆìš©ì–´ (ë‹¨ì–´ ëì— ë¶™ëŠ” ê²ƒë“¤)
        suffix_stopwords = [
            "í•˜ë ¤ë©´", "í•´ì•¼", "ê°€ì•¼", "ê°€ë ¤ë©´", "í•´ì•¼í•´", "ê°€ì•¼í•´", "í•˜ë©´", "ë˜ë©´",
            "í•˜ê³ ", "ì‹¶ì–´", "ì‹¶ë‹¤", "ì‹¶ìœ¼ë©´", "ë˜ê³ ", "ì›í•´", "ì›í•˜ë‹¤",
            "í•˜ëŠ”", "ìˆëŠ”", "ì—†ëŠ”", "ê°€ëŠ”", "ì˜¤ëŠ”", "ë‚˜ì˜¤ëŠ”",
            "ì–´ìš”", "ì•„ìš”", "í•´ìš”", "ìŠµë‹ˆë‹¤", "ã…‚ë‹ˆë‹¤",
            "ë‚˜ìš”", "í•˜ë‚˜ìš”", "í• ê¹Œìš”", "ìˆë‚˜ìš”", "ì—†ë‚˜ìš”"
        ]
        
        # ì™„ì „ ì¼ì¹˜ ë¶ˆìš©ì–´
        stopwords = {
            # ì˜ë¬¸ì‚¬
            "ì–´ë””", "ì–´ë””ì„œ", "ì–´ë””ë¡œ", "ì–´ë””ì—", "ì–´ë–»ê²Œ", "ì–´ë–¤", "ë­", "ë¬´ì—‡", "ëˆ„êµ¬",
            "ì–¸ì œ", "ì™œ", "ì–¼ë§ˆ", "ëª‡", "ë¬´ìŠ¨",
            # ë™ì‚¬/í˜•ìš©ì‚¬ (ë‹¨ë…)
            "í•˜ë‹¤", "ìˆë‹¤", "ì—†ë‹¤", "ê°€ë‹¤", "ì˜¤ë‹¤", "ë˜ë‹¤", "ë³´ë‹¤", "ì£¼ë‹¤", "ë°›ë‹¤", "ì¡ë‹¤",
            "ê°€ì•¼", "ì™€ì•¼", "í•´ì•¼", "ìˆì–´ì•¼",  # ì¶”ê°€!
            # ì–´ë¯¸/ì¡°ì‚¬
            "í•´", "ìš”", "ë‚˜", "ë‚˜ìš”", "í•˜ë‚˜ìš”",
            # ì¼ë°˜ì–´
            "ê²ƒ", "ìˆ˜", "ë•Œ", "ê³³", "ì¤‘", "ë“±", "ì´ëŸ°", "ì €ëŸ°", "ê·¸ëŸ°", "ì¢€", "ë”"
        }
        
        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œ
        tokens = re.findall(r'[ê°€-í£A-Za-z0-9]+', query)
        
        # 1. ì¡°ì‚¬ ì œê±°
        cleaned_tokens = []
        for token in tokens:
            # 3ê¸€ì ì´ìƒì´ê³  ë§ˆì§€ë§‰ ê¸€ìê°€ ì¡°ì‚¬ë©´ ì œê±°
            if len(token) >= 3 and token[-1] in particles:
                cleaned_tokens.append(token[:-1])  # ë§ˆì§€ë§‰ ê¸€ì ì œê±°
            else:
                cleaned_tokens.append(token)
        
        # 2. ë¶ˆìš©ì–´ ì œê±° (ì™„ì „ ì¼ì¹˜ + ì ‘ë¯¸ì‚¬)
        keywords = []
        for token in cleaned_tokens:
            # ì™„ì „ ì¼ì¹˜ ì²´í¬
            if token in stopwords:
                continue
            
            # ì ‘ë¯¸ì‚¬ ì²´í¬ (ì˜ˆ: "ì „ì§í•˜ë ¤ë©´" â†’ "í•˜ë ¤ë©´" ì ‘ë¯¸ì‚¬ ì œê±°)
            is_stopword = False
            for suffix in suffix_stopwords:
                if len(token) > len(suffix) and token.endswith(suffix):
                    # ì ‘ë¯¸ì‚¬ ì œê±° í›„ ë‚¨ì€ ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    core_word = token[:-len(suffix)]
                    if len(core_word) >= 2:  # í•µì‹¬ ë‹¨ì–´ê°€ 2ê¸€ì ì´ìƒ
                        keywords.append(core_word)
                    is_stopword = True
                    break
            
            # ë¶ˆìš©ì–´ ì•„ë‹ˆê³  2ê¸€ì ì´ìƒì´ë©´ ì¶”ê°€
            if not is_stopword and len(token) >= 2:
                keywords.append(token)
        
        # 3. ìˆ«ìë§Œ ìˆëŠ” í† í° ì œì™¸
        keywords = [k for k in keywords if not k.isdigit()]
        
        # 4. ì¤‘ë³µ ì œê±°
        keywords = list(dict.fromkeys(keywords))
        
        # 5. ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
        if not keywords:
            keywords = [query]
        
        return keywords
    
    def _reconstruct_ngrams(
        self,
        raw_keywords: List[str],
        original_query: str
    ) -> Dict[str, List[str]]:
        """
        âœ… N-gram ì¬êµ¬ì„±: Entity vs Sentence ë¶„ë¥˜
        
        ì „ëµ:
        1. ë‹¨ì¼ ëª…ì‚¬ (NNG, NNP) â†’ Entity (PostgreSQL ê²€ìƒ‰ìš©)
        2. ì—°ì†ëœ ë‹¨ì–´ (ë™ì‚¬ í¬í•¨) â†’ Sentence (Milvus ì˜ë¯¸ ê²€ìƒ‰ìš©)
        
        ì˜ˆì‹œ:
        - Input: ['ë¦¬ìŠ¤í•­êµ¬', 'ë¬¼ì•½', 'íŒŒëŠ”', 'ì‚¬ëŒ']
        - Output: {
            "entities": ['ë¦¬ìŠ¤í•­êµ¬'],           # PGìš©
            "sentences": ['ë¬¼ì•½ íŒŒëŠ” ì‚¬ëŒ']     # Milvusìš©
          }
        
        Args:
            raw_keywords: LLM/Kiwiê°€ ì¶”ì¶œí•œ ì›ë³¸ í‚¤ì›Œë“œ
            original_query: ì‚¬ìš©ì ì›ë³¸ ì§ˆë¬¸
            
        Returns:
            Entityì™€ Sentenceë¡œ ë¶„ë¥˜ëœ ë”•ì…”ë„ˆë¦¬
        """
        entities = []
        sentences = []
        
        # âœ… íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë¶„ë¥˜ (Kiwi ì—†ì–´ë„ ì‘ë™)
        # ë™ì‚¬ íŒ¨í„´ ë¦¬ìŠ¤íŠ¸ (ê°„ì ‘ í‘œí˜„)
        verb_patterns = [
            'íŒŒëŠ”', 'ì‚¬ëŠ”', 'íŒ”', 'ì‚´', 'íŒŒ', 'ì‚¬',
            'ì£¼ëŠ”', 'ë“œëŠ”', 'ë–¨ì–´', 'ë‚˜ì˜¤',
            'ìˆëŠ”', 'ê°€ëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”',
            'í• ', 'ë ', 'ê°ˆ', 'ì˜¬'
        ]
        
        # ì—°ì†ëœ í‚¤ì›Œë“œë¡œ ì›ë¬¸ì— ìˆëŠ” êµ¬ë¬¸ ì°¾ê¸° (N-gram)
        i = 0
        used_indices = set()  # ì´ë¯¸ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¸ë±ìŠ¤
        
        while i < len(raw_keywords):
            if i in used_indices:
                i += 1
                continue
            
            # ë™ì‚¬ íŒ¨í„´ì´ ìˆìœ¼ë©´ Sentence í›„ë³´
            current_has_verb = any(verb in raw_keywords[i] for verb in verb_patterns)
            
            if current_has_verb or (i > 0 and any(verb in raw_keywords[i-1] for verb in verb_patterns)):
                # 2~4ê°œ ë‹¨ì–´ ì¡°í•©í•´ì„œ ì›ë¬¸ì— ìˆëŠ”ì§€ í™•ì¸
                found_sentence = False
                for n in range(min(4, len(raw_keywords) - i), 1, -1):  # 4, 3, 2 ìˆœì„œ
                    phrase = ' '.join(raw_keywords[i:i+n])
                    
                    # ì›ë¬¸ì— ì´ êµ¬ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸
                    if phrase in original_query and n >= 2:  # 2ê°œ ì´ìƒë§Œ Sentence
                        sentences.append(phrase)
                        for j in range(i, i+n):
                            used_indices.add(j)
                        i += n
                        found_sentence = True
                        break
                
                if not found_sentence:
                    # Sentence ëª» ë§Œë“¤ë©´ Entityë¡œ
                    if not current_has_verb:  # ë™ì‚¬ ë‹¨ë…ì€ ë²„ë¦¼
                        entities.append(raw_keywords[i])
                    used_indices.add(i)
                    i += 1
            else:
                # ë™ì‚¬ ì—†ìœ¼ë©´ Entity
                entities.append(raw_keywords[i])
                used_indices.add(i)
                i += 1
        
        # Kiwi ìˆìœ¼ë©´ ì¶”ê°€ ì •ì œ (ì˜µì…˜)
        if self.keyword_extractor and hasattr(self.keyword_extractor, 'kiwi'):
            entities, sentences = self._refine_with_kiwi(
                entities, sentences, raw_keywords, original_query
            )
        
        # ì¤‘ë³µ ì œê±°
        entities = list(dict.fromkeys(entities))
        sentences = list(dict.fromkeys(sentences))
        
        if self.verbose:
            print(f"         ğŸ”„ N-gram ì¬êµ¬ì„±:")
            print(f"            Entities (PG): {entities}")
            print(f"            Sentences (Milvus): {sentences}")
        
        return {
            "entities": entities,
            "sentences": sentences
        }
    
    def _refine_with_kiwi(
        self,
        entities: List[str],
        sentences: List[str],
        raw_keywords: List[str],
        original_query: str
    ) -> tuple:
        """âœ… NEW: Kiwië¡œ Entity/Sentence ë¶„ë¥˜ ì •ì œ (ì˜µì…˜)"""
        try:
            kiwi = self.keyword_extractor.kiwi
            
            # ì›ë³¸ ì§ˆë¬¸ í˜•íƒœì†Œ ë¶„ì„
            tokens = kiwi.tokenize(original_query)
            token_dict = {token.form: token.tag for token in tokens[0][0]}  # {ë‹¨ì–´: í’ˆì‚¬}
            
            # Entity ì •ì œ: ëª…ì‚¬ë§Œ ë‚¨ê¸°ê¸°
            refined_entities = []
            for entity in entities:
                pos_tag = token_dict.get(entity, 'UNKNOWN')
                if pos_tag in ['NNG', 'NNP', 'SL', 'SN']:  # ëª…ì‚¬ë§Œ
                    refined_entities.append(entity)
            
            # SentenceëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            return refined_entities, sentences
            
        except Exception as e:
            if self.verbose:
                print(f"         âš ï¸ Kiwi ì •ì œ ì‹¤íŒ¨, íœ´ë¦¬ìŠ¤í‹± ê²°ê³¼ ì‚¬ìš©: {e}")
            return entities, sentences
    
    async def _find_synonyms(self, entity: str) -> List[str]:
        """
        âœ… PostgreSQL synonym í…Œì´ë¸”ì—ì„œ canonical_name ì°¾ê¸°
        
        ì˜ˆì‹œ:
        - Input: "ë¬¼ì•½"
        - Output: ["ë¹¨ê°„ í¬ì…˜", "íŒŒë€ í¬ì…˜", "í•˜ì–€ í¬ì…˜"]
        
        Args:
            entity: ê²€ìƒ‰í•  ì—”í‹°í‹° ì´ë¦„
            
        Returns:
            ë™ì˜ì–´ë¡œ ì—°ê²°ëœ canonical_name ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ë°©ë²• 1: descriptionì—ì„œ í•´ë‹¹ ë‹¨ì–´ í¬í•¨í•˜ëŠ” ì—”í‹°í‹° ì°¾ê¸°
            query = select(MapleDictionary).where(
                MapleDictionary.description.ilike(f"%{entity}%")
            ).limit(5)
            
            result = await self.db.execute(query)
            rows = result.scalars().all()
            
            canonical_names = []
            for row in rows:
                if row.canonical_name and row.canonical_name != entity:
                    canonical_names.append(row.canonical_name)
            
            return canonical_names
            
        except Exception as e:
            logger.warning(f"Synonym ê²€ìƒ‰ ì‹¤íŒ¨ ({entity}): {e}")
            return []
    
    async def _search_postgres_with_synonym(
        self,
        entities: List[str],
        limit_per_entity: int = 3
    ) -> List[Dict[str, Any]]:
        """
        âœ… Entity â†’ PostgreSQL ê²€ìƒ‰ (canonical_name + synonym)
        
        ì „ëµ:
        1. canonical_name ì§ì ‘ ë§¤ì¹­ ì‹œë„
        2. ê²°ê³¼ ì—†ìœ¼ë©´ synonym í…Œì´ë¸” ê²€ìƒ‰
        3. synonymìœ¼ë¡œ ì°¾ì€ canonical_nameìœ¼ë¡œ ì¬ê²€ìƒ‰
        
        Args:
            entities: ê²€ìƒ‰í•  Entity ë¦¬ìŠ¤íŠ¸
            limit_per_entity: Entityë‹¹ ìµœëŒ€ ê²°ê³¼ ê°œìˆ˜
            
        Returns:
            PostgreSQL ê²€ìƒ‰ ê²°ê³¼
        """
        results = []
        
        for entity in entities:
            # 1ì°¨: canonical_name ì§ì ‘ ê²€ìƒ‰
            direct_results = await self.pg_searcher.search(
                entity,
                category=None,
                limit=limit_per_entity
            )
            
            # sources í•„ë“œ ì¶”ê°€
            for result in direct_results:
                if "sources" not in result:
                    result["sources"] = ["PostgreSQL"]
                result["match_type"] = "direct"  # ì§ì ‘ ë§¤ì¹­
            
            if len(direct_results) > 0:
                results.extend(direct_results)
                
                if self.verbose:
                    print(f"         ğŸ“Œ Entity '{entity}': {len(direct_results)}ê°œ (ì§ì ‘)")
            else:
                # 2ì°¨: synonym ê²€ìƒ‰ (ì§ì ‘ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ)
                if self.verbose:
                    print(f"         ğŸ” Entity '{entity}': ì§ì ‘ ë§¤ì¹­ ì‹¤íŒ¨, synonym ê²€ìƒ‰...")
                
                synonyms = await self._find_synonyms(entity)
                
                for canonical in synonyms[:2]:  # ìƒìœ„ 2ê°œë§Œ
                    synonym_results = await self.pg_searcher.search(
                        canonical,
                        category=None,
                        limit=2
                    )
                    
                    # sources í•„ë“œ ì¶”ê°€
                    for result in synonym_results:
                        if "sources" not in result:
                            result["sources"] = ["PostgreSQL"]
                        result["match_type"] = "synonym"  # synonym ë§¤ì¹­
                        result["original_query"] = entity  # ì›ë³¸ ê²€ìƒ‰ì–´ ë³´ì¡´
                    
                    results.extend(synonym_results)
                
                if self.verbose and len(synonyms) > 0:
                    print(f"         ğŸ“Œ Entity '{entity}': {len(synonyms)}ê°œ synonym â†’ {len(results)}ê°œ ê²°ê³¼")
        
        return results
    
    def _apply_rrf(
        self,
        results_by_source: Dict[str, List[Dict[str, Any]]],
        k: int = 60
    ) -> List[Dict[str, Any]]:
        """
        RRF (Reciprocal Rank Fusion) ì ìš©
        
        ì—¬ëŸ¬ ê²€ìƒ‰ ì†ŒìŠ¤ì˜ ê²°ê³¼ë¥¼ ë­í¬ ê¸°ë°˜ìœ¼ë¡œ ìœµí•©
        
        ê³µì‹: RRF_score(d) = Î£ 1 / (k + rank_i(d))
        
        Args:
            results_by_source: ì†ŒìŠ¤ë³„ ê²°ê³¼
                {
                    "PostgreSQL": [...],
                    "Neo4j": [...],
                    "Milvus": [...]
                }
            k: RRF ìƒìˆ˜ (ê¸°ë³¸ 60)
            
        Returns:
            RRF ì ìˆ˜ë¡œ ì •ë ¬ëœ ê²°ê³¼
        """
        rrf_scores = {}  # entity_id -> RRF score
        entity_data = {}  # entity_id -> ì‹¤ì œ ë°ì´í„°
        
        # ê° ì†ŒìŠ¤ë³„ë¡œ ìˆœìœ„ ê¸°ë°˜ ì ìˆ˜ ê³„ì‚°
        for source, results in results_by_source.items():
            if not results:
                continue
            
            # ê²°ê³¼ë¥¼ ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ê° ì†ŒìŠ¤ ë‚´ì—ì„œ)
            sorted_results = sorted(
                results,
                key=lambda x: x.get("score", 0),
                reverse=True
            )
            
            # ìˆœìœ„ ê¸°ë°˜ RRF ì ìˆ˜ ê³„ì‚°
            for rank, result in enumerate(sorted_results):
                data = result.get("data", {})
                entity_id = str(data.get("id", ""))
                
                if not entity_id:
                    continue
                
                # RRF ì ìˆ˜: 1 / (k + rank)
                rrf_score = 1.0 / (k + rank)
                
                # ëˆ„ì 
                if entity_id in rrf_scores:
                    rrf_scores[entity_id] += rrf_score
                else:
                    rrf_scores[entity_id] = rrf_score
                    entity_data[entity_id] = result
        
        # RRF ì ìˆ˜ë¡œ ì •ë ¬
        sorted_entities = sorted(
            rrf_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        
        # ìµœì¢… ê²°ê³¼ ìƒì„±
        final_results = []
        for entity_id, rrf_score in sorted_entities:
            result = entity_data[entity_id]
            
            # RRF ì ìˆ˜ë¥¼ 0-100 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            # (ìµœëŒ€ RRF ì ìˆ˜ë¥¼ 100ìœ¼ë¡œ ì •ê·œí™”)
            max_rrf = sorted_entities[0][1] if sorted_entities else 1.0
            normalized_score = (rrf_score / max_rrf) * 100
            
            final_results.append({
                "score": normalized_score,
                "match_type": result.get("match_type", "rrf"),
                "data": result.get("data"),
                "sources": result.get("sources", []),
                "rrf_score": rrf_score  # ì›ë³¸ RRF ì ìˆ˜ë„ ë³´ì¡´
            })
        
        return final_results
    
    async def _rerank_with_jina(
        self,
        query: str,
        results: List[Dict[str, Any]],
        top_n: int = 5
    ) -> List[Dict[str, Any]]:
        """
        âœ… Jina Rerankerë¡œ ê²°ê³¼ ì¬ì •ë ¬
        
        RRF í›„ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ ìœ„í•´ LLM ê¸°ë°˜ reranker ì ìš©
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            results: RRFë¡œ ë³‘í•©ëœ ê²°ê³¼
            top_n: ë°˜í™˜í•  ìƒìœ„ ê°œìˆ˜
            
        Returns:
            Reranker ì ìˆ˜ë¡œ ì •ë ¬ëœ ìƒìœ„ ê²°ê³¼
        """
        if not results:
            return results
        
        try:
            # Reranker API URL (í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
            reranker_url = getattr(settings, 'RERANKER_API_URL', None) or \
                          os.getenv('RERANKER_API_URL', 'http://localhost:8001/rerank')
            
            # ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            texts = []
            for result in results:
                data = result.get("data", {})
                # print(f"ì—¬ê¸°ì„œ ì˜¤ë¥˜ í„°ì§ get data / results : {results}")
                text = f"{data.get('canonical_name', '')} - {data.get('description', '')}"
                texts.append(text)
            
            # Reranker API í˜¸ì¶œ
            payload = {
                "query": query,
                "texts": texts,
                "top_n": min(top_n, len(texts))
            }
            
            response = requests.post(reranker_url, json=payload, timeout=3)
            
            if response.status_code != 200:
                logger.warning(f"Reranker API ì‹¤íŒ¨ (status {response.status_code}), RRF ê²°ê³¼ ì‚¬ìš©")
                return results
            
            reranked_data = response.json()
            
            # Reranker ê²°ê³¼ì— ë”°ë¼ ì¬ì •ë ¬
            reranked_results = []
            for item in reranked_data.get("results", []):
                index = item.get("index")
                # print(f"ì—¬ê¸°ì„œ ì˜¤ë¥˜ í„°ì§ get index / results : {results}")
                score = item.get("score", 0)
                
                if index < len(results):
                    result = results[index].copy()
                    result["rerank_score"] = score
                    result["score"] = score * 100  # 0-100 ìŠ¤ì¼€ì¼
                    reranked_results.append(result)
            
            return reranked_results
            
        except requests.exceptions.Timeout:
            logger.warning("Reranker API íƒ€ì„ì•„ì›ƒ, RRF ê²°ê³¼ ì‚¬ìš©")
            return results
        except Exception as e:
            logger.warning(f"Reranker ì‹¤íŒ¨: {e}, RRF ê²°ê³¼ ì‚¬ìš©")
            return results
    
    async def _postgres_search(
        self,
        query: str,
        category: Optional[str],
        limit: int
    ) -> List[Dict[str, Any]]:
        """PostgreSQL ê²€ìƒ‰ (ë¹ ë¥¸ ì •í™• ë§¤ì¹­, async)"""
        try:
            results = await self.pg_searcher.search(query, category=category, limit=limit)
            return results
        except Exception as e:
            logger.error(f"PostgreSQL ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    async def _milvus_expansion_search(
        self,
        pg_results: List[Dict[str, Any]],
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        Milvus ì—°ê´€ í™•ì¥ ê²€ìƒ‰ (async)
        PostgreSQLì—ì„œ ì°¾ì€ ì—”í‹°í‹°ë“¤ì˜ ì—°ê´€ í•­ëª© ê²€ìƒ‰
        """
        if not self.milvus_searcher:
            return []
        
        milvus_results = []
        seen_ids = set()
        
        # PostgreSQLì—ì„œ ì°¾ì€ TOP 3 ì—”í‹°í‹°ë¡œ í™•ì¥
        for pg_item in pg_results[:3]:
            data = pg_item.get("data", {})
            canonical_name = data.get("canonical_name", "")
            item_id = data.get("id")
            
            if item_id:
                seen_ids.add(str(item_id))
            
            if not canonical_name:
                continue
            
            try:
                # canonical_nameìœ¼ë¡œ Milvus ê²€ìƒ‰
                results = await self.milvus_searcher.search(canonical_name, top_k=5)
                
                # ê²°ê³¼ ì¶”ê°€
                for result in results:
                    result_id = result.get("id")
                    
                    if result_id and result_id not in seen_ids:
                        milvus_results.append({
                            "score": result.get("score", 0) * 50,  # ì ìˆ˜ ì¡°ì •
                            "match_type": "milvus_expansion",
                            "data": result,
                            "source_entity": canonical_name
                        })
                        seen_ids.add(result_id)
                        
                        if len(milvus_results) >= limit:
                            break
                
            except Exception as e:
                logger.warning(f"Milvus í™•ì¥ ê²€ìƒ‰ ì‹¤íŒ¨ ({canonical_name}): {e}")
                continue
            
            if len(milvus_results) >= limit:
                break
        
        return milvus_results
    
    async def _milvus_semantic_search(
        self,
        query: str,
        limit: int
    ) -> List[Dict[str, Any]]:
        """
        Milvus ì˜ë¯¸ ê²€ìƒ‰ (í´ë°±, async)
        ì§ˆë¬¸ ì „ì²´ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ê²€ìƒ‰
        """
        if not self.milvus_searcher:
            return []
        
        try:
            # Milvus Q&A ê²€ìƒ‰
            results = await self.milvus_searcher.search(query, top_k=limit)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            for result in results:
                formatted_results.append({
                    "score": result.get("score", 0) * 100,  # ì ìˆ˜ ì¡°ì •
                    "match_type": "milvus_semantic",
                    "data": result
                })
            
            return formatted_results
            
        except Exception as e:
            logger.error(f"Milvus ì˜ë¯¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _merge_results(
        self,
        pg_results: List[Dict[str, Any]],
        milvus_results: List[Dict[str, Any]],
        mode: str = "expansion"
    ) -> List[Dict[str, Any]]:
        """
        PostgreSQL + Milvus ê²°ê³¼ ë³‘í•© (RRF ì ìš©)
        
        Args:
            mode: "expansion" (í™•ì¥) ë˜ëŠ” "fallback" (í´ë°±)
        """
        # RRF ì ìš©
        results_by_source = {
            "PostgreSQL": pg_results,
            "Milvus": milvus_results,
            "Neo4j": []  # ì´ ë©”ì„œë“œì—ì„œëŠ” Neo4j ì—†ìŒ
        }
        
        return self._apply_rrf(results_by_source)


# í¸ì˜ í•¨ìˆ˜
async def hybrid_search(
    db: AsyncSession,
    query: str,
    category: Optional[str] = None,
    limit: int = 10,
    use_milvus: bool = True
) -> List[Dict[str, Any]]:
    """
    ê°„ë‹¨í•œ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í•¨ìˆ˜ (async)
    
    Usage:
        results = await hybrid_search(db, "ì•„ì´ìŠ¤ì§„ ì–´ë””ì„œ ì‚¬ë‚˜ìš”?")
    """
    searcher = HybridSearcher(db, use_milvus=use_milvus)
    return await searcher.search(query, category=category, limit=limit)
