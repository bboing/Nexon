# ğŸ” MLX íŒŒì¸íŠœë‹ ë™ì‘ ì›ë¦¬

## ğŸ“Š ë‹¨ê³„ë³„ ìƒì„¸ ì„¤ëª…

### **Step 1: ì‹¤í–‰ ëª…ë ¹ì–´**

```bash
sh scripts/start-mlx-training.sh
```

---

### **Step 2: MLX í™˜ê²½ í™œì„±í™”**

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± (ì²« ì‹¤í–‰ ì‹œë§Œ)
python3 -m venv training/mlx-env

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install mlx mlx-lm transformers datasets

# í™œì„±í™”
source training/mlx-env/bin/activate
```

**ì†Œìš” ì‹œê°„**: ì²« ì‹¤í–‰ 2~3ë¶„, ì´í›„ ì¦‰ì‹œ

---

### **Step 3: ê¸°ë³¸ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ**

```python
# finetune_mlx.py ë‚´ë¶€
model, tokenizer = load("mlx-community/Llama-3.2-1B-Instruct-4bit")
```

**ì–´ë””ì„œ ë°›ë‚˜ìš”?**
- Hugging Face Hubì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ
- URL: https://huggingface.co/mlx-community/Llama-3.2-1B-Instruct-4bit
- ìºì‹œ ìœ„ì¹˜: `~/.cache/huggingface/hub/`
- í¬ê¸°: ì•½ 1~2GB
- ì²« ì‹¤í–‰ ì‹œê°„: 3~5ë¶„ (ë„¤íŠ¸ì›Œí¬ ì†ë„ì— ë”°ë¼)
- ì´í›„: ìºì‹œì—ì„œ ì¦‰ì‹œ ë¡œë“œ

**ì™œ Ollama ëª¨ë¸ì„ ì•ˆ ì“°ë‚˜ìš”?**
```
Ollama ëª¨ë¸:
- ìœ„ì¹˜: ~/.ollama/models/
- í˜•ì‹: GGUF (Ollama ì „ìš©)
- ìš©ë„: ì¶”ë¡  ì „ìš© (ì½ê¸° ì „ìš©)
- í•™ìŠµ: âŒ ë¶ˆê°€ëŠ¥

MLX ëª¨ë¸:
- ìœ„ì¹˜: ~/.cache/huggingface/
- í˜•ì‹: SafeTensors (í•™ìŠµ ê°€ëŠ¥)
- ìš©ë„: í•™ìŠµ + ì¶”ë¡ 
- í•™ìŠµ: âœ… ê°€ëŠ¥
```

---

### **Step 4: ë°ì´í„°ì…‹ ìƒì„±**

```python
# training/data/train.jsonl ìë™ ìƒì„±
sample_dataset = [
    {
        "instruction": "ë‹¹ì‹ ì€ NPCì…ë‹ˆë‹¤.",
        "input": "ì•ˆë…•?",
        "output": "í™˜ì˜í•©ë‹ˆë‹¤!"
    },
    # ... 100ê°œ
] * 20  # ì´ 100ê°œ ìƒ˜í”Œ

# JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
{"text": "ë‹¹ì‹ ì€ NPCì…ë‹ˆë‹¤. ì•ˆë…•? í™˜ì˜í•©ë‹ˆë‹¤!"}
{"text": "ë‹¹ì‹ ì€ ìƒì¸ì…ë‹ˆë‹¤. ê²€ íŒ”ì•„ìš”? ë„¤, ìˆìŠµë‹ˆë‹¤."}
...
```

**ì»¤ìŠ¤í…€ ë°ì´í„° ì‚¬ìš©í•˜ë ¤ë©´?**
```bash
# ì§ì ‘ ì‘ì„±
vi training/data/train.jsonl

# í˜•ì‹
{"text": "instruction + input + output"}
{"text": "instruction + input + output"}
```

---

### **Step 5: LoRA íŒŒì¸íŠœë‹ ì‹¤í–‰**

```python
# MLXê°€ ìë™ìœ¼ë¡œ:
# 1. ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ (1GB)
# 2. LoRA ë ˆì´ì–´ ì¶”ê°€ (ì‘ì€ ê°€ì¤‘ì¹˜)
# 3. ë°ì´í„°ë¡œ í•™ìŠµ (100 iterations)
# 4. ì–´ëŒ‘í„° ì €ì¥ (16MB)

cmd = [
    "python", "-m", "mlx_lm.lora",
    "--model", "mlx-community/Llama-3.2-1B-Instruct-4bit",
    "--train",
    "--data", "data/",
    "--iters", "100",
    "--lora-rank", "8",
]
```

**í•™ìŠµ ê³¼ì •**:
```
Iteration   1/100: Loss 2.456
Iteration  10/100: Loss 1.823
Iteration  20/100: Loss 1.456
...
Iteration 100/100: Loss 0.523
âœ… í•™ìŠµ ì™„ë£Œ!
```

**ì†Œìš” ì‹œê°„** (M2 Mac ê¸°ì¤€):
- 1B ëª¨ë¸, iters=100: ì•½ 5~10ë¶„
- 3B ëª¨ë¸, iters=100: ì•½ 15~20ë¶„

---

### **Step 6: ê²°ê³¼ ì €ì¥**

```
training/models/llama-game-npc-mlx/
â”œâ”€â”€ adapters.safetensors  (16MB)  â† í•™ìŠµëœ ì°¨ì´ì !
â””â”€â”€ config.json           (1KB)   â† ì„¤ì •

~/.cache/huggingface/hub/
â””â”€â”€ models--mlx-community--Llama-3.2-1B-Instruct-4bit/
    â””â”€â”€ (ì›ë³¸ ëª¨ë¸, 1GB, ë³€ê²½ ì•ˆ ë¨)
```

**ì¤‘ìš”**: ì›ë³¸ ëª¨ë¸ì€ ìˆ˜ì •ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤!

---

### **Step 7: ì¶”ë¡  í…ŒìŠ¤íŠ¸**

```python
# ì›ë³¸ + ì–´ëŒ‘í„° ìë™ ë³‘í•©
model, tokenizer = load(
    "mlx-community/Llama-3.2-1B-Instruct-4bit",
    adapter_path="models/llama-game-npc-mlx"
)

# í…ŒìŠ¤íŠ¸
prompt = "ë‹¹ì‹ ì€ ì—¬ê´€ ì£¼ì¸ì…ë‹ˆë‹¤. í”Œë ˆì´ì–´: ë°© ìˆë‚˜ìš”? NPC:"
response = generate(model, tokenizer, prompt=prompt)

print(response)
# â†’ "*ë”°ëœ»í•œ ë¯¸ì†Œë¥¼ ì§€ìœ¼ë©°* ë„¤, 2ì¸µì— ê¹¨ë—í•œ ë°©ì´ ìˆìŠµë‹ˆë‹¤!"
```

---

## ğŸ”„ LoRA ë™ì‘ ì›ë¦¬

### **ì „í†µì  íŒŒì¸íŠœë‹**

```
ì›ë³¸ ëª¨ë¸ (1GB)
    â†“ í•™ìŠµ
ìƒˆ ëª¨ë¸ (1GB)  â† ì „ì²´ë¥¼ ë‹¤ì‹œ ì €ì¥!

ë¬¸ì œì :
- ë©”ëª¨ë¦¬: 16GB+ í•„ìš”
- ì‹œê°„: ìˆ˜ ì‹œê°„
- ì €ì¥ ê³µê°„: 1GBì”© ëŠ˜ì–´ë‚¨
```

### **LoRA íŒŒì¸íŠœë‹**

```
ì›ë³¸ ëª¨ë¸ (1GB)   +   ì–´ëŒ‘í„° (16MB)
    â†“                      â†“
 ë³€ê²½ ì•ˆ ë¨           í•™ìŠµëœ ì°¨ì´ì 

ì‚¬ìš© ì‹œ:
ì›ë³¸(1GB) + ì–´ëŒ‘í„°(16MB) = í•™ìŠµëœ ëª¨ë¸!

ì¥ì :
- ë©”ëª¨ë¦¬: 4~8GB ì¶©ë¶„
- ì‹œê°„: 5~10ë¶„
- ì €ì¥ ê³µê°„: 16MBë§Œ ì¶”ê°€
```

**ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„**:

```
ì „í†µì : W' = W_original + Î”W (ì „ì²´)
LoRA:   W' = W_original + BÂ·A (ì‘ì€ í–‰ë ¬ 2ê°œ)

BÂ·A = 16MB << Î”W = 1GB
```

---

## ğŸ® Ollamaì™€ì˜ ì—°ê²°

### **í˜„ì¬ ìƒíƒœ**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama             â”‚       â”‚  MLX í•™ìŠµ ê²°ê³¼      â”‚
â”‚  (ì¶”ë¡  ì „ìš©)        â”‚       â”‚  (í•™ìŠµ ì™„ë£Œ)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  llama3.1 (8B)      â”‚       â”‚  adapters.safetensorsâ”‚
â”‚  gemma-3 (12B)      â”‚       â”‚  (16MB)             â”‚
â”‚                     â”‚       â”‚                     â”‚
â”‚  ìš©ë„: ì±„íŒ… í…ŒìŠ¤íŠ¸  â”‚       â”‚  ìš©ë„: ê²Œì„ NPC     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†‘                              â†‘
    ollama run                    python ì§ì ‘ ì‹¤í–‰
```

**ë‘˜ì€ ë³„ê°œì…ë‹ˆë‹¤!**

### **Ollamaì— ë°°í¬í•˜ë ¤ë©´? (ì„ íƒ ì‚¬í•­)**

```bash
# 1. ì–´ëŒ‘í„°ë¥¼ ì›ë³¸ê³¼ ë³‘í•©
python -m mlx_lm.convert \
  --model mlx-community/Llama-3.2-1B-Instruct-4bit \
  --adapter models/llama-game-npc-mlx \
  --output models/game-npc-merged

# 2. GGUFë¡œ ë³€í™˜
llama.cpp/convert.py models/game-npc-merged \
  --outtype q4_0 \
  --outfile models/game-npc.gguf

# 3. Modelfile ìƒì„±
cat > Modelfile << EOF
FROM ./models/game-npc.gguf
TEMPLATE """{{ .Prompt }}"""
EOF

# 4. Ollamaì— ì¶”ê°€
ollama create game-npc -f Modelfile

# 5. ì‹¤í–‰!
ollama run game-npc
```

---

## ğŸ’» ì‹¤ì œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

### **M2 Max (32GB) ê¸°ì¤€**

```
í•™ìŠµ ì¤‘:
- ì‹œìŠ¤í…œ: 8GB
- MLX ëª¨ë¸: 1GB (ê¸°ë³¸ ëª¨ë¸)
- LoRA: 1GB (í•™ìŠµ ì¤‘ ì„ì‹œ)
- ë°ì´í„°: 1GB
- ì´: ~11GB

ì¶”ë¡  ì¤‘:
- ì‹œìŠ¤í…œ: 8GB
- MLX ëª¨ë¸: 1GB
- ì–´ëŒ‘í„°: 0.1GB
- ì´: ~9GB
```

### **M1 (8GB) ì—ì„œëŠ”?**

```
ê¶Œì¥ ì„¤ì •:
CONFIG = {
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",
    "batch_size": 2,  # 4 â†’ 2ë¡œ ê°ì†Œ
    "lora_rank": 4,   # 8 â†’ 4ë¡œ ê°ì†Œ
}

ë©”ëª¨ë¦¬ ì‚¬ìš©: ì•½ 5~6GB
```

---

## ğŸ¯ ìš”ì•½

### **Q: Ollama ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë‚˜ìš”?**
**A**: âŒ ì•„ë‹ˆìš”! Hugging Faceì—ì„œ ìƒˆ ëª¨ë¸ì„ ë°›ì•„ì„œ í•™ìŠµí•©ë‹ˆë‹¤.

### **Q: í•™ìŠµ í›„ Ollamaì—ì„œ ì“¸ ìˆ˜ ìˆë‚˜ìš”?**
**A**: âœ… ê°€ëŠ¥í•˜ì§€ë§Œ ë³„ë„ ë³€í™˜ ì‘ì—… í•„ìš” (ì„ íƒ ì‚¬í•­)

### **Q: í•™ìŠµëœ ëª¨ë¸ì„ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?**
**A**: Pythonì—ì„œ ì§ì ‘ `mlx_lm.load()`ë¡œ ì‚¬ìš© (ê°€ì¥ ê°„ë‹¨)

### **Q: ì›ë³¸ ëª¨ë¸ì´ ë³€ê²½ë˜ë‚˜ìš”?**
**A**: âŒ ì•„ë‹ˆìš”! ì‘ì€ ì–´ëŒ‘í„°(16MB)ë§Œ ìƒì„±ë©ë‹ˆë‹¤.

### **Q: ì–¼ë§ˆë‚˜ ê±¸ë¦¬ë‚˜ìš”?**
**A**: ì²« ì‹¤í–‰ 10ë¶„ (ë‹¤ìš´ë¡œë“œ í¬í•¨), ì´í›„ 5ë¶„

---

## ğŸš€ ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•˜ê¸°

```bash
cd /Users/taegyunkim/bboing/ollama_model/my-ai-platform

# í•œ ì¤„ë¡œ ì‹¤í–‰!
sh scripts/start-mlx-training.sh

# ìë™ìœ¼ë¡œ:
# 1. í™˜ê²½ ì„¤ì •
# 2. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
# 3. ë°ì´í„° ìƒì„±
# 4. í•™ìŠµ ì‹¤í–‰
# 5. í…ŒìŠ¤íŠ¸
```

**ê²°ê³¼ë¬¼**: `training/models/llama-game-npc-mlx/adapters.safetensors`

ì´ì œ ì´í•´ê°€ ë˜ì…¨ë‚˜ìš”? ğŸ¯
