import subprocess
import sys
from pathlib import Path
from mlx_lm import load, generate

# 1. í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (fuse í•˜ê¸° ì „ ì–´ëŒ‘í„°ë‚˜, fuseëœ ëª¨ë¸ ê²½ë¡œ)
# ì£¼ì˜: ì•„ê¹Œ í•™ìŠµí•œ ê²°ê³¼ë¬¼ì´ ìˆëŠ” í´ë”ë¡œ ì§€ì •!
base_path = Path(__file__).resolve().parent.parent
model_path = base_path / "models" / "llama-game-npc-mlx-dequantized"
gguf_output = base_path / "models" / "gguf" / "llama-game-npc.gguf"
# ë§Œì•½ fuseëœ ê±¸ í…ŒìŠ¤íŠ¸í•˜ë ¤ë©´: "./models/llama-game-npc-merged-fp16"

print(f"â³ ëª¨ë¸ ë¡œë“œ ì¤‘: {model_path}")
model, tokenizer = load(model_path)

header = f"FROM {gguf_output.absolute()}\n"
modelfile_content = """
SYSTEM \"\"\"ë‹¹ì‹ ì€ ì´ ê²Œì„ ì„¸ê³„ì˜ ì‚´ì•„ìˆëŠ” NPCì…ë‹ˆë‹¤.
ë‹¤ìŒ ì›ì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œ ëŒ€ë‹µí•˜ì„¸ìš”:
1. ì–¸ì–´: ë¬´ì¡°ê±´ ìì—°ìŠ¤ëŸ¬ìš´ 'í•œêµ­ì–´'ë§Œ ì‚¬ìš©í•˜ì„¸ìš”. (ì˜ì–´, í•œì ê¸ˆì§€)
2. íƒœë„: AI ë¹„ì„œì²˜ëŸ¼ ë”±ë”±í•˜ê²Œ êµ´ì§€ ë§ê³ , ë§¡ì€ ì—­í• (Role)ì— ëª°ì…í•˜ì—¬ ì—°ê¸°í•˜ì„¸ìš”.
3. ì§€ì‹: ë‹¹ì‹ ì´ ì•Œê³  ìˆëŠ” ì„¤ì • ë‚´ì—ì„œë§Œ ëŒ€ë‹µí•˜ê³ , ëª¨ë¥´ëŠ” ë‚´ìš©ì€ "ê·¸ê±´ ë‚´ ì•Œ ë°” ì•„ë‹ˆë„¤" í˜¹ì€ "ëª¨ë¥´ê² ëŠ”ë°?"ì™€ ê°™ì´ NPCìŠ¤ëŸ½ê²Œ ê±°ì ˆí•˜ì„¸ìš”. ì—†ëŠ” ì‚¬ì‹¤ì„ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
4. í˜•ì‹: ë‹µë³€ì€ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ, ëŒ€í™”í•˜ë“¯ì´ 2~3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ í•˜ì„¸ìš”.\"\"\"

TEMPLATE \"\"\"{{ if .System }}<|start_header_id|>system<|end_header_id|>

{{ .System }}<|eot_id|>{{ end }}{{ if .User }}<|start_header_id|>user<|end_header_id|>

{{ .User }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>

{{ .Response }}<|eot_id|>\"\"\"

PARAMETER temperature 0.6

PARAMETER num_ctx 4096

PARAMETER stop "<|start_header_id|>"
PARAMETER stop "<|end_header_id|>"
PARAMETER stop "<|eot_id|>"
"""
prompt = header + modelfile_content

# 3. ìƒì„±
print("ğŸ’¬ ìƒì„± ì‹œì‘...")
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True, 
    max_tokens=100,
)

print("\n=== ê²°ê³¼ ===")
print(response)