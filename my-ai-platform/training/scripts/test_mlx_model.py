#!/usr/bin/env python3
"""
MLX í•™ìŠµ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•˜ì—¬ ëŒ€í™” í…ŒìŠ¤íŠ¸
"""

from mlx_lm import load, generate
from mlx_lm.sample_utils import make_sampler
import sys

def test_model(prompt: str = None):
    """í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸"""
    
    print("ðŸŽ MLX í•™ìŠµ ëª¨ë¸ ë¡œë”© ì¤‘...")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    # ëª¨ë¸ + ì–´ëŒ‘í„° ë¡œë“œ
    model_name = "mlx-community/Llama-3.2-3B-Instruct-4bit"
    adapter_path = "../models/llama-game-npc-mlx"
    
    print(f"ðŸ“¦ ê¸°ë³¸ ëª¨ë¸: {model_name}")
    print(f"ðŸŽ¨ LoRA ì–´ëŒ‘í„°: {adapter_path}\n")
    
    model, tokenizer = load(
        model_name,
        adapter_path=adapter_path
    )
    
    print("âœ… ë¡œë”© ì™„ë£Œ!\n")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
    test_prompts = [
        "í”Œë ˆì´ì–´: ì•ˆë…•í•˜ì„¸ìš”?\nNPC:",
        "í”Œë ˆì´ì–´: ë§ˆë²•ì‚¬ë¡œ ì „ì§í• ëž˜ìš”\nNPC:",
        "í”Œë ˆì´ì–´: ì „ì‚¬ë¡œ ì „ì§í•˜ê³  ì‹¶ì–´ìš”\nNPC:",
        "í”Œë ˆì´ì–´: ë„ì ìœ¼ë¡œ ì „ì§í• ëž˜ìš”\nNPC:",
        "í”Œë ˆì´ì–´: ê¶ìˆ˜ë¡œ ì „ì§í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”\nNPC:",
    ]
    
    # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸ê°€ ìžˆìœ¼ë©´ ì‚¬ìš©
    if prompt:
        test_prompts = [prompt]
    
    # ê° í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸
    for i, prompt in enumerate(test_prompts, 1):
        print(f"ðŸŽ® í…ŒìŠ¤íŠ¸ {i}/{len(test_prompts)}")
        print(f"ðŸ“ í”„ë¡¬í”„íŠ¸:\n{prompt}")
        print(f"\nðŸ¤– AI ì‘ë‹µ:")
        
        sampler = make_sampler(temp=0.7, top_p=0.9)
        response = generate(
            model,
            tokenizer,
            prompt=prompt,
            max_tokens=100,
            verbose=False,
            sampler=sampler,
        )
        
        print(response)
        print("\n" + "â”"*50 + "\n")
    
    print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nðŸ’¡ íŒ:")
    print("   - ì»¤ìŠ¤í…€ í…ŒìŠ¤íŠ¸: python test_mlx_model.py \"í”Œë ˆì´ì–´: ì•ˆë…•í•˜ì„¸ìš”?\\nNPC:\"")
    print("   - ëŒ€í™”í˜• ëª¨ë“œ: python interactive_chat.py")

def interactive_mode():
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    print("ðŸŽ MLX í•™ìŠµ ëª¨ë¸ - ëŒ€í™”í˜• ëª¨ë“œ")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    # ëª¨ë¸ ë¡œë“œ
    model_name = "mlx-community/Llama-3.2-3B-Instruct-4bit"
    adapter_path = "../models/llama-game-npc-mlx"
    
    print(f"ðŸ“¦ ë¡œë”© ì¤‘... ", end="", flush=True)
    model, tokenizer = load(model_name, adapter_path=adapter_path)
    print("ì™„ë£Œ! âœ…\n")
    
    print("ðŸ’¬ ëŒ€í™”ë¥¼ ì‹œìž‘í•©ë‹ˆë‹¤. (ì¢…ë£Œ: 'exit' ë˜ëŠ” Ctrl+C)")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    
    while True:
        try:
            # ì‚¬ìš©ìž ìž…ë ¥
            user_input = input("ðŸ‘¤ í”Œë ˆì´ì–´: ")
            
            if user_input.lower() in ['exit', 'quit', 'ì¢…ë£Œ']:
                print("\nðŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"í”Œë ˆì´ì–´: {user_input}\nNPC:"
            
            # ì‘ë‹µ ìƒì„±
            print("ðŸ¤– NPC: ", end="", flush=True)
            sampler = make_sampler(temp=0.7, top_p=0.9)
            response = generate(
                model,
                tokenizer,
                prompt=prompt,
                max_tokens=100,
                verbose=False,
                sampler=sampler,
            )
            
            # NPC ì‘ë‹µë§Œ ì¶œë ¥ (í”„ë¡¬í”„íŠ¸ ì œì™¸)
            npc_response = response.split("NPC:")[-1].strip()
            print(npc_response)
            print()
            
        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜: {e}")
            break

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--interactive" or sys.argv[1] == "-i":
            # ëŒ€í™”í˜• ëª¨ë“œ
            interactive_mode()
        else:
            # ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸
            custom_prompt = " ".join(sys.argv[1:])
            test_model(custom_prompt)
    else:
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
        test_model()
