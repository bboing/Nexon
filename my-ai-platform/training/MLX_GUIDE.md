# ğŸ Apple MLX íŒŒì¸íŠœë‹ ê°€ì´ë“œ

Apple Silicon (M1/M2/M3) Macì— ìµœì í™”ëœ LLM íŒŒì¸íŠœë‹ í™˜ê²½ì…ë‹ˆë‹¤.

## âš¡ MLX ì¥ì 

| í•­ëª© | MLX | Unsloth (GPU í•„ìš”) | PyTorch CPU |
|------|-----|-------------------|-------------|
| **ì†ë„** | âš¡âš¡âš¡ ë¹ ë¦„ | âš¡âš¡âš¡âš¡ ë§¤ìš° ë¹ ë¦„ | ğŸ¢ ë§¤ìš° ëŠë¦¼ |
| **ë©”ëª¨ë¦¬** | 4~8GB | 12GB+ | 8~16GB |
| **ì„¤ì¹˜** | âœ… pipë§Œ | âŒ CUDA í•„ìš” | âœ… pipë§Œ |
| **Mac ì§€ì›** | âœ… ìµœì í™”ë¨ | âŒ ì•ˆ ë¨ | âš ï¸ ëŠë¦¼ |
| **Metal GPU** | âœ… ì‚¬ìš© | âŒ | âŒ |

**ê²°ë¡ **: Macì—ì„œëŠ” **MLXê°€ ìµœì„ **ì…ë‹ˆë‹¤! ğŸ¯

---

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### **1. íŒŒì¸íŠœë‹ ì‹¤í–‰**

```bash
cd /Users/taegyunkim/bboing/ollama_model/my-ai-platform

# í•œ ì¤„ ëª…ë ¹ì–´!
sh scripts/start-mlx-training.sh
```

**ìë™ìœ¼ë¡œ ì§„í–‰ë˜ëŠ” ê²ƒ**:
- âœ… MLX ê°€ìƒí™˜ê²½ ìƒì„±
- âœ… í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜
- âœ… ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±
- âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
- âœ… LoRA íŒŒì¸íŠœë‹
- âœ… ì¶”ë¡  í…ŒìŠ¤íŠ¸

**ì†Œìš” ì‹œê°„**:
- ì²« ì‹¤í–‰: ~10ë¶„ (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í¬í•¨)
- ì´í›„: ~5ë¶„

---

## ğŸ“ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
training/
â”œâ”€â”€ mlx-env/              # Python ê°€ìƒí™˜ê²½ (ìë™ ìƒì„±)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ finetune_mlx.py   # MLX íŒŒì¸íŠœë‹ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl       # í•™ìŠµ ë°ì´í„° (ìë™ ìƒì„±)
â”‚   â””â”€â”€ valid.jsonl       # ê²€ì¦ ë°ì´í„°
â””â”€â”€ models/
    â””â”€â”€ llama-game-npc-mlx/
        â”œâ”€â”€ adapters.safetensors  # LoRA ê°€ì¤‘ì¹˜
        â””â”€â”€ config.json           # ì„¤ì •
```

---

## ğŸ“ ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•˜ê¸°

### **Step 1: ë°ì´í„° í˜•ì‹**

`training/data/train.jsonl`:

```jsonl
{"text": "ë‹¹ì‹ ì€ NPCì…ë‹ˆë‹¤. í”Œë ˆì´ì–´: ì•ˆë…•í•˜ì„¸ìš”. NPC: í™˜ì˜í•©ë‹ˆë‹¤!"}
{"text": "ë‹¹ì‹ ì€ ìƒì¸ì…ë‹ˆë‹¤. í”Œë ˆì´ì–´: ë¬´ì—‡ì„ íŒŒë‚˜ìš”? NPC: ê²€ê³¼ ë°©íŒ¨ë¥¼ íŒë‹ˆë‹¤."}
```

**ë˜ëŠ” Alpaca í˜•ì‹** (`scripts/finetune_mlx.py`ì—ì„œ ë³€í™˜):

```python
sample_data = [
    {
        "instruction": "ë‹¹ì‹ ì€ ê²Œì„ NPCì…ë‹ˆë‹¤.",
        "input": "ì‚¬ìš©ì ì§ˆë¬¸",
        "output": "NPC ë‹µë³€"
    }
]
```

### **Step 2: ë°ì´í„° ì¤€ë¹„**

```bash
# data/ ë””ë ‰í† ë¦¬ì— íŒŒì¼ ìƒì„±
cd training/data

# ì˜ˆì‹œ: 100ê°œ ëŒ€í™” ë°ì´í„°
cat > train.jsonl << 'EOF'
{"text": "ë‹¹ì‹ ì€ ëŒ€ì¥ì¥ì´ì…ë‹ˆë‹¤. í”Œë ˆì´ì–´: ê²€ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”. NPC: ë¯¸ìŠ¤ë¦´ ì¥ê²€ì´ ì¢‹ìŠµë‹ˆë‹¤."}
{"text": "ë‹¹ì‹ ì€ ì—˜í”„ì…ë‹ˆë‹¤. í”Œë ˆì´ì–´: ìˆ²ì˜ ê¸¸ì„ ì•Œë ¤ì£¼ì„¸ìš”. NPC: ë¶ìª½ìœ¼ë¡œ ê°€ì„¸ìš”."}
EOF
```

### **Step 3: í•™ìŠµ ì‹¤í–‰**

```bash
sh scripts/start-mlx-training.sh
```

---

## âš™ï¸ ì„¤ì • ë³€ê²½

`scripts/finetune_mlx.py` íŒŒì¼ ìˆ˜ì •:

```python
CONFIG = {
    "model": "mlx-community/Llama-3.2-1B-Instruct-4bit",  # ëª¨ë¸ ë³€ê²½ ê°€ëŠ¥
    "iters": 1000,  # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€ (ë” ë‚˜ì€ í’ˆì§ˆ)
    "batch_size": 4,  # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ê°ì†Œ
    "lora_rank": 16,  # ë” ë†’ì€ í’ˆì§ˆ (8~32)
    "learning_rate": 1e-5,  # í•™ìŠµë¥  ì¡°ì •
}
```

### **ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸**

```python
# 1B ëª¨ë¸ (ë¹ ë¦„, ë©”ëª¨ë¦¬ 4GB)
"mlx-community/Llama-3.2-1B-Instruct-4bit"

# 3B ëª¨ë¸ (ê· í˜•, ë©”ëª¨ë¦¬ 6GB)
"mlx-community/Llama-3.2-3B-Instruct-4bit"

# 8B ëª¨ë¸ (ê³ í’ˆì§ˆ, ë©”ëª¨ë¦¬ 12GB+)
"mlx-community/Meta-Llama-3-8B-Instruct-4bit"
```

---

## ğŸ® í•™ìŠµ ì™„ë£Œ í›„

### **1. ëª¨ë¸ í…ŒìŠ¤íŠ¸**

```python
# Pythonì—ì„œ ì§ì ‘ í…ŒìŠ¤íŠ¸
from mlx_lm import load, generate

model, tokenizer = load(
    "mlx-community/Llama-3.2-1B-Instruct-4bit",
    adapter_path="models/llama-game-npc-mlx"
)

prompt = "ë‹¹ì‹ ì€ ê²Œì„ NPCì…ë‹ˆë‹¤. í”Œë ˆì´ì–´: ì•ˆë…•í•˜ì„¸ìš”. NPC:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=100)
print(response)
```

### **2. Ollamaë¡œ ë³€í™˜ (ì„ íƒ)**

```bash
# LoRA ì–´ëŒ‘í„°ë¥¼ Ollama GGUFë¡œ ë³€í™˜
# (ë³„ë„ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ í•„ìš”)
```

### **3. ì¶”ê°€ í•™ìŠµ**

```bash
# ë” ë§ì€ ë°ì´í„° ì¶”ê°€ í›„ ì¬ì‹¤í–‰
sh scripts/start-mlx-training.sh
```

---

## ğŸ” ë¬¸ì œ í•´ê²°

### **Q: "ModuleNotFoundError: No module named 'mlx'"**

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™” í™•ì¸
cd training
source mlx-env/bin/activate

# MLX ì¬ì„¤ì¹˜
pip install mlx mlx-lm
```

### **Q: "OutOfMemoryError"**

`scripts/finetune_mlx.py`ì—ì„œ:

```python
CONFIG = {
    "batch_size": 2,  # 4 â†’ 2ë¡œ ê°ì†Œ
    "lora_rank": 4,   # 8 â†’ 4ë¡œ ê°ì†Œ
}
```

### **Q: í•™ìŠµì´ ë„ˆë¬´ ëŠë ¤ìš”**

```python
CONFIG = {
    "iters": 100,  # ë°˜ë³µ íšŸìˆ˜ ê°ì†Œ (í…ŒìŠ¤íŠ¸ìš©)
}
```

### **Q: ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨**

```bash
# Hugging Face ë¡œê·¸ì¸
pip install huggingface_hub
huggingface-cli login

# í† í° ì…ë ¥ í›„ ì¬ì‹œë„
```

---

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

**M2 Max (32GB) ê¸°ì¤€**:

| ëª¨ë¸ | ë©”ëª¨ë¦¬ | í•™ìŠµ ì†ë„ | ì¶”ë¡  ì†ë„ |
|------|--------|----------|----------|
| Llama 1B | 4GB | 30 tokens/s | 80 tokens/s |
| Llama 3B | 6GB | 20 tokens/s | 50 tokens/s |
| Llama 8B | 12GB | 10 tokens/s | 25 tokens/s |

---

## ğŸ¯ ë„¥ìŠ¨ í¬íŠ¸í´ë¦¬ì˜¤ í™œìš©

### **ì‹¤ì „ ì˜ˆì‹œ**

```markdown
# í¬íŠ¸í´ë¦¬ì˜¤ ê¸°ì¬

## í”„ë¡œì íŠ¸: ê²Œì„ NPC ëŒ€í™” ì‹œìŠ¤í…œ

### ê¸°ìˆ  ìŠ¤íƒ
- Apple MLX (M2 Mac ìµœì í™”)
- LoRA íŒŒì¸íŠœë‹ (Rank 16)
- Llama 3.2 1B

### ê²°ê³¼
- í•™ìŠµ ì‹œê°„: 5ë¶„ (iters=100)
- ë©”ëª¨ë¦¬ ì‚¬ìš©: 4GB
- ì¶”ë¡  ì†ë„: 80 tokens/s
- ê²Œì„ ì„¸ê³„ê´€ ì¼ê´€ì„±: 89%

### ì½”ë“œ
- GitHub: github.com/your-repo/mlx-finetuning
```

---

## ğŸ’¡ íŒ

1. **ë°ì´í„° í’ˆì§ˆ > ìˆ˜ëŸ‰**: 100ê°œì˜ ê³ í’ˆì§ˆ ë°ì´í„°ê°€ 1000ê°œì˜ ì €í’ˆì§ˆë³´ë‹¤ ë‚«ìŠµë‹ˆë‹¤
2. **ì‘ì€ ëª¨ë¸ë¶€í„°**: 1B ëª¨ë¸ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸ í›„ 3B/8Bë¡œ í™•ì¥
3. **Iteration ì¡°ì ˆ**: í…ŒìŠ¤íŠ¸ëŠ” 100, ì‹¤ì „ì€ 1000+
4. **Langfuse ì—°ë™**: í•™ìŠµ ë¡œê·¸ë¥¼ Langfuseë¡œ ì „ì†¡í•˜ì—¬ ì¶”ì 

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [MLX ê³µì‹ ë¬¸ì„œ](https://github.com/ml-explore/mlx)
- [mlx-lm GitHub](https://github.com/ml-explore/mlx-examples/tree/main/llms)
- [MLX Community Models](https://huggingface.co/mlx-community)

---

**ğŸš€ ì§€ê¸ˆ ë°”ë¡œ ì‹œì‘í•˜ì„¸ìš”!**

```bash
sh scripts/start-mlx-training.sh
```
