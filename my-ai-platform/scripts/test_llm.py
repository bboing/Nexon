#!/usr/bin/env python3
"""
LLM ì—°ê²° í…ŒìŠ¤íŠ¸
"""
import sys
from pathlib import Path

SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
LANGCHAIN_APP_DIR = PROJECT_ROOT / "langchain_app"
sys.path.insert(0, str(LANGCHAIN_APP_DIR))

from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / ".env")

print("âœ… 1. í™˜ê²½ ì„¤ì • ì™„ë£Œ")

from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage

print("âœ… 2. LangChain import ì™„ë£Œ")

# LLM ì´ˆê¸°í™”
model_name = "llama3.1:latest"  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
base_url = "http://localhost:11434"

print(f"\nğŸ¤– LLM ì´ˆê¸°í™” ì¤‘...")
print(f"   ëª¨ë¸: {model_name}")
print(f"   Base URL: {base_url}")

try:
    llm = ChatOllama(
        base_url=base_url,
        model=model_name,
        temperature=0.1,
        timeout=30  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    )
    print("âœ… 3. LLM ê°ì²´ ìƒì„± ì™„ë£Œ")
    
    # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    print("\n" + "="*80)
    print("ğŸ’¬ LLM í…ŒìŠ¤íŠ¸: 'ì•ˆë…•í•˜ì„¸ìš”'")
    print("="*80)
    print("\nê¸°ë‹¤ë¦¬ëŠ” ì¤‘... (ìµœëŒ€ 30ì´ˆ)")
    
    response = llm.invoke([HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”.")])
    
    print(f"\nğŸ¤– LLM ì‘ë‹µ:")
    print("-" * 80)
    print(response.content)
    print("-" * 80)
    
    print("\nâœ… 4. LLM í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("\nì´ì œ Agentë¥¼ ì‹¤í–‰í•´ë³¼ ìˆ˜ ìˆì–´ìš”!")
    
except Exception as e:
    print(f"\nâŒ LLM í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    print("\nê°€ëŠ¥í•œ ì›ì¸:")
    print("1. Ollamaê°€ ì‹¤í–‰ ì¤‘ì´ ì•„ë‹˜ â†’ ollama serve ì‹¤í–‰")
    print("2. ëª¨ë¸ì´ ì—†ìŒ â†’ ollama pull llama3.1")
    print("3. í¬íŠ¸ê°€ ë‹¤ë¦„ â†’ .env íŒŒì¼ í™•ì¸")
    import traceback
    traceback.print_exc()
