#!/usr/bin/env python3
"""
Router ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸ (llama3.1 vs gemma3-12b)
"""
import sys
from pathlib import Path
import time

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
LANGCHAIN_APP_DIR = PROJECT_ROOT / "langchain_app"
sys.path.insert(0, str(LANGCHAIN_APP_DIR))

# .env ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / ".env")

from langchain_community.chat_models import ChatOllama
from src.agents.router_agent import RouterAgent, QueryType


# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
TEST_CASES = [
    ("ì•„ì´ìŠ¤ì§„ ê°€ê²©ì€?", QueryType.SIMPLE_LOOKUP, "ê°€ê²©_ì¡°íšŒ"),
    ("í˜ì´ìŠ¨ì€ ëˆ„êµ¬ì•¼?", QueryType.SIMPLE_LOOKUP, "ì—”í‹°í‹°_ì •ë³´"),
    ("ì•„ì´ìŠ¤ì§„ ì–´ë””ì„œ ë‚˜ì™€?", QueryType.RELATIONSHIP, "ë“œë¡­_ì •ë³´_í™•ì¸"),
    ("í—¤ë„¤ì‹œìŠ¤ì—ì„œ ì»¤ë‹ì‹œí‹° ê°€ëŠ” ë²•?", QueryType.RELATIONSHIP, "ê¸¸ì°¾ê¸°"),
    ("ì´ˆë³´ì ì¶”ì²œ ì¥ë¹„", QueryType.SEMANTIC, "ì¶”ì²œ_ìš”ì²­"),
    ("ë„ì ì—ê²Œ ì¢‹ì€ ì‚¬ëƒ¥í„°", QueryType.SEMANTIC, "ì¶”ì²œ_ìš”ì²­"),
    ("ì•„ì´ìŠ¤ì§„ ì‚¬ê³  ë‹¤ìŒì— ë­ ì‚¬ì•¼ í•´?", QueryType.COMPLEX, "ì¶”ì²œ_ìš”ì²­"),
]


def test_model(model_name: str, test_cases: list):
    """íŠ¹ì • ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸"""
    print(f"\n{'='*80}")
    print(f"ğŸ¤– ëª¨ë¸: {model_name}")
    print(f"{'='*80}")
    
    # LLM ì´ˆê¸°í™”
    llm = ChatOllama(
        base_url="http://localhost:11434",
        model=model_name,
        temperature=0.1
    )
    
    router = RouterAgent(llm, verbose=False)
    
    results = []
    total_time = 0
    
    for idx, (question, expected_type, expected_intent_keyword) in enumerate(test_cases, 1):
        print(f"\n[{idx}/{len(test_cases)}] {question}")
        
        # ì‹œê°„ ì¸¡ì •
        start = time.time()
        result = router.classify(question)
        elapsed = time.time() - start
        total_time += elapsed
        
        # ê²°ê³¼ í‰ê°€
        type_correct = result['type'] == expected_type
        intent_match = expected_intent_keyword in result.get('intent', '').lower()
        
        # ì¶œë ¥
        print(f"   íƒ€ì…: {result['type']} {'âœ…' if type_correct else 'âŒ'}")
        print(f"   ì˜ë„: {result.get('intent', 'N/A')} {'âœ…' if intent_match else 'âš ï¸'}")
        print(f"   ì‹ ë¢°ë„: {result['confidence']:.2f}")
        print(f"   ì‹œê°„: {elapsed:.2f}ì´ˆ")
        
        results.append({
            "question": question,
            "expected_type": expected_type,
            "actual_type": result['type'],
            "type_correct": type_correct,
            "intent": result.get('intent', 'N/A'),
            "intent_match": intent_match,
            "confidence": result['confidence'],
            "time": elapsed
        })
    
    # í†µê³„
    type_accuracy = sum(1 for r in results if r['type_correct']) / len(results) * 100
    intent_accuracy = sum(1 for r in results if r['intent_match']) / len(results) * 100
    avg_time = total_time / len(results)
    avg_confidence = sum(r['confidence'] for r in results) / len(results)
    
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {model_name} í†µê³„")
    print(f"{'='*80}")
    print(f"íƒ€ì… ì •í™•ë„: {type_accuracy:.1f}%")
    print(f"ì˜ë„ ë§¤ì¹­: {intent_accuracy:.1f}%")
    print(f"í‰ê·  ì‹ ë¢°ë„: {avg_confidence:.2f}")
    print(f"í‰ê·  ì‹œê°„: {avg_time:.2f}ì´ˆ")
    print(f"ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
    
    return {
        "model": model_name,
        "type_accuracy": type_accuracy,
        "intent_accuracy": intent_accuracy,
        "avg_confidence": avg_confidence,
        "avg_time": avg_time,
        "total_time": total_time,
        "results": results
    }


def compare_models():
    """ë‘ ëª¨ë¸ ë¹„êµ"""
    print("\n" + "ğŸ”¬"*40)
    print("Router ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("ğŸ”¬"*40)
    
    # ëª¨ë¸ 1: llama3.1
    llama_stats = test_model("llama3.1:latest", TEST_CASES)
    
    # ëª¨ë¸ 2: gemma3-12b
    gemma_stats = test_model(
        "hf.co/bartowski/google_gemma-3-12b-it-GGUF:Q4_K_M", 
        TEST_CASES
    )
    
    # ë¹„êµ ê²°ê³¼
    print(f"\n{'='*80}")
    print("ğŸ† ìµœì¢… ë¹„êµ")
    print(f"{'='*80}")
    
    print(f"\n{'í•­ëª©':<20} {'llama3.1':<15} {'gemma3-12b':<15} {'ìŠ¹ì':<10}")
    print("-" * 80)
    
    # íƒ€ì… ì •í™•ë„
    print(f"{'íƒ€ì… ì •í™•ë„':<20} "
          f"{llama_stats['type_accuracy']:.1f}%{'':<10} "
          f"{gemma_stats['type_accuracy']:.1f}%{'':<10} "
          f"{'ğŸ† Gemma' if gemma_stats['type_accuracy'] > llama_stats['type_accuracy'] else 'ğŸ† Llama' if llama_stats['type_accuracy'] > gemma_stats['type_accuracy'] else 'ğŸ¤ ë™ì '}")
    
    # ì˜ë„ ë§¤ì¹­
    print(f"{'ì˜ë„ ë§¤ì¹­':<20} "
          f"{llama_stats['intent_accuracy']:.1f}%{'':<10} "
          f"{gemma_stats['intent_accuracy']:.1f}%{'':<10} "
          f"{'ğŸ† Gemma' if gemma_stats['intent_accuracy'] > llama_stats['intent_accuracy'] else 'ğŸ† Llama' if llama_stats['intent_accuracy'] > gemma_stats['intent_accuracy'] else 'ğŸ¤ ë™ì '}")
    
    # í‰ê·  ì‹ ë¢°ë„
    print(f"{'í‰ê·  ì‹ ë¢°ë„':<20} "
          f"{llama_stats['avg_confidence']:.2f}{'':<12} "
          f"{gemma_stats['avg_confidence']:.2f}{'':<12} "
          f"{'ğŸ† Gemma' if gemma_stats['avg_confidence'] > llama_stats['avg_confidence'] else 'ğŸ† Llama' if llama_stats['avg_confidence'] > gemma_stats['avg_confidence'] else 'ğŸ¤ ë™ì '}")
    
    # í‰ê·  ì‹œê°„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
    print(f"{'í‰ê·  ì‹œê°„':<20} "
          f"{llama_stats['avg_time']:.2f}ì´ˆ{'':<10} "
          f"{gemma_stats['avg_time']:.2f}ì´ˆ{'':<10} "
          f"{'ğŸ† Llama' if llama_stats['avg_time'] < gemma_stats['avg_time'] else 'ğŸ† Gemma' if gemma_stats['avg_time'] < llama_stats['avg_time'] else 'ğŸ¤ ë™ì '}")
    
    print("\n" + "="*80)
    
    # ì¶”ì²œ
    llama_score = (
        llama_stats['type_accuracy'] * 0.4 +
        llama_stats['intent_accuracy'] * 0.3 +
        llama_stats['avg_confidence'] * 100 * 0.2 -
        llama_stats['avg_time'] * 10 * 0.1
    )
    
    gemma_score = (
        gemma_stats['type_accuracy'] * 0.4 +
        gemma_stats['intent_accuracy'] * 0.3 +
        gemma_stats['avg_confidence'] * 100 * 0.2 -
        gemma_stats['avg_time'] * 10 * 0.1
    )
    
    print(f"\nğŸ’¡ ì¶”ì²œ:")
    if gemma_score > llama_score + 5:
        print(f"   ğŸ† gemma3-12b ì¶”ì²œ!")
        print(f"   - ë” ì •í™•í•¨ (íƒ€ì…: {gemma_stats['type_accuracy']:.1f}%, ì˜ë„: {gemma_stats['intent_accuracy']:.1f}%)")
        print(f"   - ì•½ê°„ ëŠë¦¬ì§€ë§Œ ({gemma_stats['avg_time']:.2f}ì´ˆ vs {llama_stats['avg_time']:.2f}ì´ˆ)")
        print(f"   - ì •í™•ë„ê°€ ì†ë„ë³´ë‹¤ ì¤‘ìš”!")
    elif llama_score > gemma_score + 5:
        print(f"   ğŸ† llama3.1 ì¶”ì²œ!")
        print(f"   - ì¶©ë¶„íˆ ì •í™•í•¨ (íƒ€ì…: {llama_stats['type_accuracy']:.1f}%)")
        print(f"   - í›¨ì”¬ ë¹ ë¦„ ({llama_stats['avg_time']:.2f}ì´ˆ)")
        print(f"   - RouterëŠ” ì†ë„ê°€ ì¤‘ìš”!")
    else:
        print(f"   ğŸ¤ ë¹„ìŠ·í•œ ì„±ëŠ¥!")
        print(f"   - ì†ë„ ìš°ì„ ì´ë©´ llama3.1")
        print(f"   - ì •í™•ë„ ìš°ì„ ì´ë©´ gemma3-12b")


if __name__ == "__main__":
    compare_models()
