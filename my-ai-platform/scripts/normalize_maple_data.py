#!/usr/bin/env python3
"""
maple_data.json ì •ê·œí™”
- Categoryë³„ë¡œ ë°ì´í„° êµ¬ì¡° í†µì¼
- í‘œì¤€ ìŠ¤í‚¤ë§ˆì— ë§ì¶°ì„œ ëª¨ë“  í•„ë“œ ì •ë¦¬
- ê°’ì´ ì—†ìœ¼ë©´ null ì²˜ë¦¬
"""
import json
from pathlib import Path
from typing import Dict, Any, List, Set
from collections import defaultdict

# ê²½ë¡œ ì„¤ì •
SCRIPT_DIR = Path(__file__).resolve().parent
PROJECT_ROOT = SCRIPT_DIR.parent
INPUT_FILE = PROJECT_ROOT / "training/data/input_data/maple_data.json"
OUTPUT_FILE = PROJECT_ROOT / "training/data/input_data/maple_data_normalized.json"
BACKUP_FILE = PROJECT_ROOT / "training/data/input_data/maple_data.backup.json"


# Categoryë³„ í‘œì¤€ ìŠ¤í‚¤ë§ˆ ì •ì˜
STANDARD_SCHEMAS = {
    "MAP": {
        # ê¸°ë³¸ í•„ë“œ
        "category": "MAP",
        "map_type": None,
        "region": None,
        "bgm": None,
        "description": None,
        
        # ì—°ê²° ì •ë³´
        "adjacent_maps": [],
        "special_portals": [],
        
        # ê±°ì£¼ì
        "resident_npcs": [],
        "resident_monsters": [],
        
        # ê¸°ëŠ¥/íŠ¹ì§•
        "features": [],
        
        # ë ˆë²¨ ì •ë³´
        "min_level": 0,  # ê¸°ë³¸ê°’: 0
        "recommended_level_range": None,
        
        # ì•ˆì „ ì§€ëŒ€
        "is_safe_zone": True,  # ê¸°ë³¸ê°’: True
        
        # ê¸°íƒ€
        "required_quest": None,
        "star_force_limit": 0,  # ê¸°ë³¸ê°’: 0
        "arcane_power_limit": 0,  # ê¸°ë³¸ê°’: 0
    },
    
    "ITEM": {
        # ê¸°ë³¸ í•„ë“œ
        "category": "ITEM",
        "item_type": None,
        "description": None,
        
        # ê°€ê²©/êµ¬ë§¤
        "price": None,
        "obtainable_from": [],
        
        # ìš”êµ¬ ì‚¬í•­
        "required_level": 0,  # ê¸°ë³¸ê°’: 0
        "required_job": [],  # ê¸°ë³¸ê°’: ë¹ˆ ë¦¬ìŠ¤íŠ¸
        "required_stats": None,
        
        # ëŠ¥ë ¥ì¹˜
        "stats": None,
        "effects": [],
        
        # íŠ¹ì„±
        "tradable": None,
        "stackable": None,
        "max_stack": None,
        
        # ê¸°íƒ€
        "quest_item": None,
        "consumable": None,
    },
    
    "NPC": {
        # ê¸°ë³¸ í•„ë“œ
        "category": "NPC",
        "npc_type": None,
        "description": None,
        
        # ìœ„ì¹˜
        "location": None,
        "region": None,
        
        # ì„œë¹„ìŠ¤
        "services": [],
        "sells_items": [],
        
        # í€˜ìŠ¤íŠ¸
        "related_quests": [],
        
        # ëŒ€í™”
        "dialogue": None,
        
        # ê¸°íƒ€
        "is_merchant": None,
        "is_quest_giver": None,
    },
    
    "MONSTER": {
        # ê¸°ë³¸ í•„ë“œ
        "category": "MONSTER",
        "monster_type": "NORMAL",  # ê¸°ë³¸ê°’: NORMAL
        "description": None,
        
        # ìŠ¤íƒ¯
        "level": None,
        "hp": None,
        "mp": None,
        "exp": None,
        "attack": None,
        "defense": None,
        
        # ìœ„ì¹˜
        "spawn_maps": [],
        "region": None,
        
        # ë“œë
        "drops": [],
        "meso_drop_range": None,
        
        # ì†ì„±
        "element": None,
        "boss": None,
        
        # íŠ¹ì§•
        "abilities": [],
        "weaknesses": [],
        
        # ê¸°íƒ€
        "respawn_time": None,
    }
}


def collect_all_fields_by_category(data: List[Dict[str, Any]]) -> Dict[str, Set[str]]:
    """ê° categoryë³„ë¡œ ì‚¬ìš©ëœ ëª¨ë“  í•„ë“œ ìˆ˜ì§‘"""
    fields_by_category = defaultdict(set)
    
    for item in data:
        category = item.get('category')
        if category:
            detail_data = item.get('detail_data', {})
            if isinstance(detail_data, dict):
                for key in detail_data.keys():
                    fields_by_category[category].add(key)
    
    return fields_by_category


def normalize_entity(entity: Dict[str, Any], schema: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì—”í‹°í‹°ë¥¼ í‘œì¤€ ìŠ¤í‚¤ë§ˆì— ë§ì¶°ì„œ ì •ê·œí™”
    
    Args:
        entity: ì›ë³¸ ì—”í‹°í‹°
        schema: í‘œì¤€ ìŠ¤í‚¤ë§ˆ
        
    Returns:
        ì •ê·œí™”ëœ detail_data
    """
    detail_data = entity.get('detail_data', {})
    if not isinstance(detail_data, dict):
        detail_data = {}
    
    # í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ìƒˆë¡œìš´ ê°ì²´ ìƒì„±
    normalized = {}
    
    for key, default_value in schema.items():
        # ê¸°ì¡´ ê°’ì´ ìˆê³  Noneì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‚¬ìš©
        if key in detail_data and detail_data[key] is not None:
            normalized[key] = detail_data[key]
        else:
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            normalized[key] = default_value
    
    return normalized


def normalize_data(input_file: Path, output_file: Path) -> None:
    """ë°ì´í„° ì •ê·œí™” ë©”ì¸ í•¨ìˆ˜"""
    
    print("=" * 80)
    print("maple_data.json ì •ê·œí™” ì‹œì‘")
    print("=" * 80)
    print()
    
    # 1. ì›ë³¸ íŒŒì¼ ì½ê¸°
    print(f"ğŸ“– ì›ë³¸ íŒŒì¼ ì½ê¸°: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"   ì´ {len(data)}ê°œ ì—”í‹°í‹°")
    
    # 2. Categoryë³„ í†µê³„
    category_counts = defaultdict(int)
    for item in data:
        category = item.get('category')
        category_counts[category] += 1
    
    print(f"\nğŸ“Š Categoryë³„ ì—”í‹°í‹° ìˆ˜:")
    for category, count in sorted(category_counts.items()):
        print(f"   {category}: {count}ê°œ")
    
    # 3. í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í•„ë“œ ìˆ˜ì§‘
    print(f"\nğŸ” í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ í•„ë“œ ë¶„ì„...")
    fields_by_category = collect_all_fields_by_category(data)
    
    for category, fields in sorted(fields_by_category.items()):
        print(f"\n   [{category}] ì‚¬ìš© ì¤‘ì¸ í•„ë“œ ({len(fields)}ê°œ):")
        for field in sorted(fields):
            print(f"      - {field}")
    
    # 4. ì •ê·œí™” ìˆ˜í–‰
    print(f"\nğŸ”¨ ì •ê·œí™” ìˆ˜í–‰ ì¤‘...")
    normalized_data = []
    
    for entity in data:
        category = entity.get('category')
        
        if category in STANDARD_SCHEMAS:
            # í‘œì¤€ ìŠ¤í‚¤ë§ˆ ì ìš©
            schema = STANDARD_SCHEMAS[category]
            normalized_detail = normalize_entity(entity, schema)
            
            # ìƒˆë¡œìš´ ì—”í‹°í‹° ìƒì„±
            normalized_entity = {
                "canonical_name": entity.get('canonical_name'),
                "category": category,
                "synonyms": entity.get('synonyms', []),
                "description": entity.get('description', ''),
                "detail_data": normalized_detail
            }
            
            normalized_data.append(normalized_entity)
        else:
            # ìŠ¤í‚¤ë§ˆê°€ ì—†ëŠ” categoryëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            print(f"   âš ï¸  ìŠ¤í‚¤ë§ˆ ì—†ìŒ: {category}")
            normalized_data.append(entity)
    
    print(f"   âœ… {len(normalized_data)}ê°œ ì—”í‹°í‹° ì •ê·œí™” ì™„ë£Œ")
    
    # 5. ë°±ì—… ìƒì„±
    print(f"\nğŸ’¾ ì›ë³¸ ë°±ì—…: {BACKUP_FILE}")
    with open(BACKUP_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # 6. ì •ê·œí™”ëœ ë°ì´í„° ì €ì¥
    print(f"\nğŸ’¾ ì •ê·œí™”ëœ ë°ì´í„° ì €ì¥: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(normalized_data, f, ensure_ascii=False, indent=2)
    
    # 7. ê²€ì¦
    print(f"\nâœ… ì •ê·œí™” ì™„ë£Œ!")
    print(f"\nğŸ“Š ê²°ê³¼:")
    print(f"   ì›ë³¸: {len(data)}ê°œ")
    print(f"   ì •ê·œí™”: {len(normalized_data)}ê°œ")
    print(f"   ë°±ì—…: {BACKUP_FILE}")
    print(f"   ì¶œë ¥: {output_file}")
    
    print("\n" + "=" * 80)
    print("ì •ê·œí™” ì™„ë£Œ! ğŸ‰")
    print("=" * 80)
    print()
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("1. maple_data_normalized.json í™•ì¸")
    print("2. ë¬¸ì œ ì—†ìœ¼ë©´ ì›ë³¸ êµì²´:")
    print(f"   mv {output_file} {input_file}")
    print("3. PostgreSQL ì¬ìƒì„±:")
    print("   docker exec -it ai-langchain-api python /app/scripts/import_data.py --drop")
    print("4. Milvus ì¬ìƒì„±:")
    print("   echo 'y' | docker exec -i ai-langchain-api python /app/scripts/sync_to_milvus.py --drop")
    print()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="maple_data.json ì •ê·œí™”")
    parser.add_argument("--input", type=str, default=str(INPUT_FILE), help="ì…ë ¥ íŒŒì¼")
    parser.add_argument("--output", type=str, default=str(OUTPUT_FILE), help="ì¶œë ¥ íŒŒì¼")
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    output_path = Path(args.output)
    
    if not input_path.exists():
        print(f"âŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        exit(1)
    
    try:
        normalize_data(input_path, output_path)
    except Exception as e:
        print(f"\nâŒ ì •ê·œí™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        exit(1)
